---
title: "Exploratory Data Analysis"
---

Usually once you get the data for your analysis the first thing you want to do is get to know it and briefly browse through it to know e.g. what variables are in the dataset, how many observations you have etc. You might feel the urge to jump right into answering your key research questions. After all, usually data collection is a laborious and tiring process and you are so curious to check what the results are! But it's best to set aside the main analyses for a moment and first take a closer look at the data. Why? It is a necessary steps because there is a near-infinite number of things that can go wrong when preparing the data from errors in how data was coded to errors in preprocessing or loading the files. Lots of things can also happen that can make analysis or drawing conclusions difficult. These can range from issues with your measures (e.g. reliability) through unexpected behaviour of participants during the study (e.g. non-compliance, reactance, purposufely giving ridiculous answers) all the way to errors in the data (e.g. items not recoded or errors in coding, values out of range etc.).

## getting the basic information about a dataset

How to best get to know a new dataset? You can work with datasets from various sources. If it's your study that you're analyzing, then you probably know everything about the variables that should be in it, how many particiapnts took part in the study, how variables are coded etc. However, you might also be working with datasets provided by other people (e.g. by a colleague who ran the study, by a company, or you got the data from an open repository). In these situation it is crucial that you can get to know your data. Many of such datasets will have a codebook available which should describe it in detail. Unfortunately that is not always the case (codebook is like documentation - super important but nobody wants to do it). If a codebook is available, read it (this still does not allow you to skip checking for potential errors/problems with the data). If you don't have a codebook you need to check everything yourself. Fortunately there is a lot of functions in R that make it a lot easier.

Lets starts with just general first look at the data. We might want to see how many observations we have, how many columns there are, and the names and types of columns. `dplyr` has a very nice function for it called `glimpse()`. It's more concise than `str()` but gives you more information than e.g. `head()`. Lets load our dataset and have a firsty look at it!

-   

    ```{r}
    library(tidyverse)
    #what dataset do I want for this? Some dataset that I tinkered with?
    #' Should I load a dataset I have?
    #' Can I easily do it? Which dataset could I use for that purpose?
    ```

    The `psych` package also has a pretty useful function that can give you multiple summaries very quickly. It's called `describe()`. It takes a data frame or a matrix as an argument and can provide a lot of summary statistics for numeric variables such as the mean, median, standard deviation, standard error, skew, kurtosis and quantiles. You can specify which of these you want by setting additional arguments.d

    ```{r}
    library(psych)
    ```

    Sometimes you want summary statistics grouped by some categorical variable (e.g. experimental condition). You can do it with `describe.by()` from `psych` package. It works the same way as `describe()` you just need to provide the `group` argument which tells R what variable to split the dataset by.

    ```{r}


    ```

You might also want to get some summary statistics of categorical variables. The simplest way to do it is with `table()` which allows you to get counts or cross tabulations of categorical variables.

```{r}

```

## Exploring via plots

Why plotting data is always important

### A cautionary tale the boring way

One of the reasons why plotting is very important and useful is that all sorts of data can provide you the same point estimate (like a mean or a regression slope). The most common example is the Anscombe quartet. It's a set of 4 datasets with x and y variable each. Each of these have exactly the same correlation coefficient between x and y so you might be tempted to say they are pretty much the same, right? But if you plot them you'll see this:

![](images/paste-E216A64B.png){fig-align="center"}

Even though the regression line is the same on each plot you can immediately see that it makes sense only in the first one (top left). The top right one shows a quadratic and not a linear relation. The bottom left one clearly has an outlier that drives the regression line to be steeper. And the bottom right one shows pretty much no relation because entire variation in x is driven by a single data point. If we didn't plot the data we wouldn't realize just how different these datasets are. You could discover this in some numerical way (e.g. looking at regression diagnostics) but plotting makes it much faster and allows you to immediately spot the problem.

### A cautionary tale the fun way

There is a more fun way to see the same point that was made by Anscombe. We'll look at an experiment that was conducted to test how displaying information in the media affects attitude towards migrants. The researchers showed participants either a negative article or a neutral one and measured attitudes towards migrants. They also measured general exposure to the media (ie how much media does one consume) because they were interested in the relation between media consumption and attitudes towards migrants. They also recorded gender and age of participants. Lets load the data:

```{r echo=FALSE, message=FALSE}
experiment <- read_csv("data/att_experiment.csv")
glimpse(experiment)
```

Lets look at some summary statistics first:

```{r}
experiment %>%
  group_by(condition) %>%
  summarise(mean_att = mean(att_migrants),
            se_att = sd(att_migrants)/sqrt(n())) %>%
  knitr::kable()
```

Well, it does not look like there are any meaningful differences between the conditions. We can also plot these differences:

```{r}
experiment %>%
  ggplot(aes(x = condition, y = att_migrants)) +
  stat_summary(fun.data = "mean_se", geom = "pointrange") +
  coord_cartesian(ylim = c(0,5))
```

This is a bit disappointing. We could say that we did not reject the null hypothesis and there is no support in the data that reading a negative article affect attitudes towards migrants relative to reading a neutral article. Lets at least look at the correlation between media exposure and attitude towards migrants. Maybe there is something interesting there? We can use the `cor()` function for it:

```{r}
cor(experiment$media_exposure, experiment$att_migrants)
```

Well the correlation seems negative and it definitely is not small. Maybe we are finally on to something! So lets finally plot the data to see how it looks like:

```{r}
experiment %>%
  ggplot(aes(x = media_exposure, y = att_migrants)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Whoops! This obviously makes no sense. We get a plot of a gorilla. This dataset is fabricated! This example is taken fully from a paper: Yanai, I, Lercher, M. (2020). A Hypothesis is a liability. *Genome Biology*, 21, 231. What they did was provide students with a very similar dataset as above and randomly assign their students into one of two condition: half the students were asked to test a specific hypothesis and half to just analyze the dataset. The researchers were interested how many students would find the gorilla. They found (although the sample was small) that students who were asked to test specific hypothesis were less likely to find the gorilla in the data than the students that were not assigned a specific hypothesis to test. This of course dos not mean we should throw hypotheses away. However it shows that being too focused on testing very specific things with our data we can miss some really big errors and unless we explore the data properly we might make terrible mistakes.

The key takeaway is that a lot of things that are wrong or at least problematic can be immediately spotted when you plot the data. e.g. in when comparing two conditions of an experiment you might spot that the whole effect is driven but just a few outliers. Or that some values are out of range. There are of course limits to what a plot can provide (if you want to quantify then you need more than a plot) but they often work very well for initial screening.

Another important takeaway is that good exploratory data analysis makes detecting both errors and fraud much easier.

## reliabilities

-getting reliability analysis with alpha()
