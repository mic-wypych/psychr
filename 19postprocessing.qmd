---
title: "Postprocessing results"
author: "Micha≈Ç Wypych"
---

Once you finally ran a model it finally is time to look at what it says and interpret the results. A common way of interpreting statistical models is to look directly at the coefficients In older tutorials or textbooks you could usually read to make models directly interpretable. That meant for example such coding of categorical variables so that the regression coefficient were exactly the contrast you wanted.

With newer and more versatile software you don't really need to do it this way. You can run a model and then process the results to get exactly what matches your hypothesis e.g. if you want to test a specific contrast from an experiment. Or if you want to compare whether the effect of one variable is significantly stronger than the effect of another variable. This gets especially important once you start working with more complex models (like spline, logistic or ordinal regression) where coefficients themselves aren't easily interpretable.

## General logic

The packages we will be working with is `broom`, `performance` and (amazing!) `marginaleffects`. The last package also has absolutely incredible documentation so I highly recommend checking out its [website](https://marginaleffects.com/). It also has a wonderful paper on interpreting models [here](https://www.jstatsoft.org/article/view/v111i09). A very gentle and nice introduction by Julia Rohrer is [here](https://www.the100.ci/2022/05/27/%e2%9c%a8-unleash-your-inner-stats-sparkle-%e2%9c%a8-with-this-very-non-technical-introduction-to-marginal-effects/).

The dataset we will be working with is:

```{r}
library(tidyverse)
library(broom)
library(performance)
library(marginaleffects)
library(openintro)



```

## Tidying the model

You may have noticed that results of the `lm()` function is a rather complicated list. It is not easy to work with. Fortunately there is the `broom` package that has functions extremely useful for tidying up the results of such models. We can extract infromation from the model on 3 levels: individual observations, model coefficients or the entire model.\
The first one can be achieved with the `augment()` function which will give us predicted values as well as plenty of diagnostics.

```{r}
#augment()
```

We can tidy the coefficients using `tidy()`. It will give us a nice data frame with all model coefficients:

```{r}
#tidy()
```

Finally we can tidy the model-level information using `glance()`:

```{r}
#glance()
```

## Assessing general model performance

The `performance` package from the `easystats` suite of packages has a host of functions for assessing the performance of models.

We can extract such values like the R squared, rmse, AIC etc. This can also provide a basis for comparing models.

```{r}
#model_performance()
```

What you might also want to do is comparison of model performence. Remember that what you are comparing though is usually predictive performence and it won't tell you which model is correctly specified

```{r}
#compare_performance()
```

## Making predictions

Sometimes you might want to assess the predictions of your model - how well does it predict? Do predictions for various combinations of independent variables differ from each other?

```{r}
#predictions()
```

```{r}
#avg_predictions()
```

We can also make predictions for specific values of our predictors. For example we might be interested in **what**

```{r}

```

-   splitting the sample

-   common metrics like rmse, r2

## Working with predictions

In simple models you might get away with directly interpreting the coefficients but for anything even slightly more complicated things get messy really quickly. It is often much easier to work on predictions from the model rather than directly on the coefficients. It is a good practice to get accustomed to working with predictions even for simple models especially that ultimately they allow you to do more.

You can still get everything you would get from looking at coefficients this way. For example lets say you have a simple linear model with a binary predictor (so basically a t-test). If you run it as a regression model you will get an intercept that tells you the predicted value for reference category and the coefficient that informs you about the size of the difference between reference and the other category. You could just as well get it by making predictions from the model (its going to predict one value for reference category and another value for the other category) and then calculating the mean difference. I promise the result is going to be the same. It might seem like more work than just reading the coefficients from the table but you will see that pretty quickly coefficients get very confusing and can't be easily interpreted.

-   make an exmaple with a squared term?

## Marginal effects

An extremely good and clear introduction to marginal effects with explanation of differences between 2 main packages used for them in R can be found on [Andrew Heiss' blog](https://www.andrewheiss.com/blog/2022/05/20/marginalia/) (although this blog works a lot with logistic regression). If you want to dive deeper into this topic I highly recommend you read that blog post (as well as documentation of the `marginaleffects` package linked above).

In simplest terms a marginal effect is the effect of some X variable on some Y variable when we increase X by a tiny amount (for ease of interpretation we often say about increasing X by 1).

How can we get a marginal effect? One way would be to take every observation in our dataset, make a prediction, then increment the variable of interest by 1, make a prediction, then take a mean of differences in predictions. This will give us what is called an average marginal effect.

-   marginal means?

## Comparisons, contrasts 

In many situations when you work linear model you are interested in testing specific hypotheses. In the examples below we will look at some of the most common scenarios like comparing predictions for various levels of categorical predictors, comparing effects of one variable across levels of a moderator

**For example**

-   A very common mistake is to look at coefficients for two predictors from a model one of which is statistically significant while the other is not and declaring that these coefficients differ significantly. [These are two completely different things](https://stat.columbia.edu/~gelman/research/published/signif4.pdf)! You can make a more principled comparison using `marginaleffects`.
-   when you have a categorical predictor with multiple levels a common thing advised in older software was to recode that predictor in some way. This is not really necessary if you take a postprocessing approach.
-   For more complex models (e.g. with interactions, polynomial terms) the coefficients will also be misleading or will tell you very little.

## Effect sizes?
