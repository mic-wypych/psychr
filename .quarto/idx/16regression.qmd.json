{"title":"Regression","markdown":{"yaml":{"title":"Regression"},"headingText":"Regression analysis","containsRefs":false,"markdown":"\n\n\nWe will finally move to actually running statistical models with general linear models. We'll see that a simple t-test and between-subjects anova are just linear models.\n\n## Prediction and inference\n\nRegression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest. When doing inference you are interested in learning how the data was generated as a function of your variables of interest. In prediction you just want to be as accurate as possible. This is of course a bit simplistic and as you'll see looking at predicitons can be very useful for actual inference but this explanation will do for now.\n\nAs an example imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on (e.g. what to invest in).\n\n## Regression engine\n\nThe very idea of making a regression analysis is to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones. How can we do that? We want a way that will approximate our variable of interest so that each time we are to say what value this variable will have we will make the best guess possible. But what does it mean 'the best guess possible'? We need some rule on what it would mean to make a best guess. Since we are predicting a continuous variable we can calculate how much we miss for every prediction (subtract the actual value from the predicted value) and then choose the one that has the smallest error. This is already a start but we can miss in two ways: we can predict too little or too much. The first one is going to produce a negative error and the other one a positive one. Negative numbers are smaller than positive ones so our rule so far will always favor predicting too little! Fortunately there is a very easy way to deal with this - we can square the errors (there are reasons why squaring is preferred to taking absolute values but they are beyond the scope of this course). Now, we can sum all the squared errors for each observation in our dataset. The prediction that gets the smallest sum of squared errors wins! This is pretty much how the Ordinary Least Squares (OLS) regression works.\n\nLets start with the simplest example possible. Lets say we have no other information except our variable that we want to predict. Which value will minimize the sum of squared errors? There's actually a fancy formula for this because OLS regression has a closed form but perhaps a better way to learn this is to actually make a bunch of guesses and see what happens. This is what simulation is perfect for.\n\nLets first simulate a bunch of observations from a normal distribution: our dependent variable. We will keep mean = 0 and standard deviation = 1.\n\n```{r}\nlibrary(tidyverse)\ny <- rnorm(1000)\n```\n\nNext, we'll make a bunch of guesses in the range from -1 to 1 and see how each of them performs (what is their sum of squared errors).\n\n```{r}\nsse <- c() #initialize vector to store sum of squared errors\npred <- seq(-1,1,length.out = 10) #we'll make 10 guesses from -1 to 1, equally spaced\nfor (i in pred) {\n  res <- sum((i - y)^2) # for each guess calculate sum of squared errors\n  sse <- c(sse, res) #append the sum to the vector\n}\npred_sse <-data.frame(pred,sse) #put these \n```\n\nYou can go ahead and inspect the `sse` and `pred`. Can you see for which guess we get the smallest error? You can also look at the animation below:\n\n![predictions and sums of squared errors](images/sse_example.gif){fig-align=\"center\"}\n\nYou can see that the points fall on a really nice parabola. For a guess of mean - 1 standard deviation we get a really big sum of squared errors, they gradually get smaller and smaller and then start to get bigger up to mean + 1 standard deviation. The lowest point of the parabola is at the mean and that is in fact our best guess. When making regression analysis we will be working with means all the time. We'll just be adding more information to the model (e.g. if we add belonging to experimental vs control group into the model, then our best guess will be the mean in each of those groups and we get a two sample t-test). Linear models are all about conditional means i.e. means of one variable given a value of another variable.\n\nA more general way which we can use to think about linear models is that we are modelling Y as following a normal distribution with mean $\\mu$ and variance $\\sigma$. The mean is then determined by the variables we put into the model (those are the conditional means from above). If we don't add anything $\\mu$ is going to be the actual mean in our sample. If we start adding variables into the model then $\\mu_i$ will be determined by them (that's why we have the i).\n\n$$\nY_i \\sim Normal(\\mu_i, \\sigma)\n$$\n\n$$\n\\mu_i =\\alpha\n$$\n\nWhen we add predictors into the model we will be able to assess how much we can improve over just using the mean of the dependent variable (in fact this is what one of the most commons metrics for assessing linear models is - R squared).\n\n## Making a simple regression\n\nBefore we move on to running our first model we have to understand one more thing - formulas in R. They allow us to write models in a very succint form. A formula generally takes the form of `y ~ x1 + x2 + ...`. On the left hand side is always the dependent variable and on the right hand side you put all of your predictors.\n\nWe'll start with a simple simulated example to get an intuition on what exactly a simple linear model does. We will simulate some data, run a model and then plot the predictions against the data.\n\n```{r}\nN <- 1e4\nx1 <- rnorm(N)\ny <- rnorm(N, 2 + .5*x1)\ndf1 <- data.frame(x1, y)\n```\n\nLets run a simple linear model:\n\n```{r}\nlm_sim <- lm(y ~ x1, df1)\n```\n\nOk, now we can get to the plotting. What we will do is:\n\n1.  Plot the original data\n2.  Plot the regression line\n3.  Plot the conditional distributions implied by the model.\n\nYou can inspect the code if you want but it's not that important right now\n\n```{r}\n#| code-fold: true\n#| code-summary: \"simulating data implied by the model\"\n#| fig-align: \"center\"\n\n\nx1 <- seq(min(df1$x1), max(df1$x1), length.out = 10)\n\n#set up a data frame to store simulated values\ndf_sim <- data.frame(x1 = numeric(), sim = numeric())\n\n#run loop for simulation\nfor(i in x1) {\n  #simulate y for given x1\n  sim <- rnorm(1e5, mean = lm_sim$coefficients[[1]] + lm_sim$coefficients[[2]]*i, sd = 1)\n  #store x1 for given simulations\n  x1_1 <- rep(i, 1e5)\n  #store sim and x1 in temp\n  temp <- data.frame(x1 = x1_1, sim = sim)\n  #append simulated data to results data frame\n  df_sim <- rbind(df_sim, temp)\n}\n\n#' the above is based on simulating data from the distribution\ndf_rib <- data.frame(x1 = numeric(), lb = numeric(), hb = numeric())\nfor(i in x1) {\n  q <- qnorm(c(.025, .975), mean = lm_sim$coefficients[[1]] + lm_sim$coefficients[[2]]*i, sd = 1)\n\n  temp <- data.frame(x1 = i, lb = q[1], hb = q[2])\n  df_rib <- rbind(df_rib, temp)\n  \n}\n\n\ndf_sim %>%\n  ggplot(aes(x = x1, y = sim)) +\n  geom_point(data = df1, aes(x = x1, y = y), alpha = .1) +\n  geom_ribbon(data = df_rib, aes(x = x1, ymin = lb, ymax = hb, y = hb), alpha = .2, fill = \"steelblue\") +\n  ggdist::stat_halfeye(alpha = .8, fill = \"navyblue\") +\n  geom_smooth(data = df1, aes(x = x1, y = y), alpha = .4, method = \"lm\", color = \"firebrick4\", fill = \"firebrick4\") +\n  labs(title = \"Conditional distribution for linear regression\",\n       subtitle = \"The plot shows raw data and simualted data from regressing y on x1.\\nRed line is line of best fit, gray area is 95% prediction interval\",\n       y = \"y\") +\n  theme_minimal()\n```\n\nThe plot above is quite packed with information so lets unpack it. First, we have the datapoints plotted as a scatterplot. Next we have the red line which indicates the regression line or in other words the conditional means of `y` given `x1`. The we have the distributions which are data simulated from the model for 10 values of `x1`. Each distribution is what we would expect y to look like given a value of `x1` based on our model. Notice that the mean of each distribution (the black dot) passes exactly through the red line. Next notice that each distribution has the same shape (pretty much a normal distribution - that is the normal distribution assumption) and more or less the same width (that is the homoscedacticity assumption. The light blue area indicates where we would expect 95% of the `y` to fall in for any given value of `x1`.\n\nNow it's time to introduce some data we can work on. We will be working with a dataset that stores information from 2010 General Social Survey. the dataset can be found in the `openintro` package (which stores data from various open source textbooks). Lets load the dataset and take a look at it.\n\n```{r message=F}\nlibrary(openintro)\ndata(\"gss2010\")\nstr(gss2010)\n```\n\nThe dataset stores information on 5 variables: number of hours for relaxing after and average work day, number of days during the last 30 days in which participants mental health was not good, number of hours worked per week, degree and opinion on legalizing marihuana. (the docs are also available [here](#0)). For now we will be mainly intersted in the `hrsrelax` variable and explaining it with `hrs1`.\n\nWe can start with the null model - a model that has no predictors, just the intercept in it. As we already know this intercept will in fact be the sample mean. Intercept can be coded with 1 in the formula. Running a regression in R is actually super simple. You can use the `lm()` function to do it.:\n\n```{r}\nnull_model <- lm(hrsrelax ~ 1, gss2010)\n\n```\n\nWe can inspect the output using `summary()`:\n\n```{r}\nsummary(null_model)\n```\n\nThis summary tells us the following:\n\n1.  the formula and data used for the model (under the \"Call\")\n2.  Rough approximation of the distribution of residuals\n3.  The coefficients: the estimated Intercept equal to `r round(null_model$coefficients[[1]],2)`, its standard error and associated t and p values. This is equal to the mean of the `hrsrelax` variable: `r round(mean(gss2010$hrsrelax, na.rm = T),2)`\n4.  The residual standard error (which in the case of the null model is just the standard deviation of the variable) and degrees of freedom\n5.  The number of deleted observations due to missing values. This one is real unfortunate as `lm()` will silently drop any missing values without so much as a warning! This can cause a lot of trouble so it's worth always inspecting how many observations were in fact dropped.\n6.  Once we add predictors we will also get an omnibus F test statistic and its p value as well as an R squared value.\n\nLets add the first predictor to our model. The model will then look like this\n\n$$\nY_i \\sim Normal(\\mu_i, \\sigma)\n$$\n\n$$\n\\mu_i =\\alpha + \\beta_1 * hrs1\n$$\n\n```{r}\nsimple_fit <- lm(hrsrelax ~ hrs1, gss2010)\n```\n\nNow that we have our model lets proceed to interpret it:\n\n```{r}\nsummary(simple_fit)\n```\n\nWhat this output tells us is:\n\n1.  for a person who works 0 hours per week we expect 5.11 hours of relax time per day on average.\n2.  With each additional hour worked per week we expect .035 hours of relax time less. This means that we expect people whose work time per week differs by 10 to have on average .35 hour difference in their time for relaxation per day. This effect is significant at conventional levels.\n3.  The model dropped 893 observations which had missing values in any of the variables used in the model. Notice that it's a lot!\n4.  The model explains about 4% of variance in `hrsrelax` variable and the F statistic is also significant. This means we were able to reduce the squared erros by 4% compared to the null model.\n\n### A note on assumptions\n\nA lot of psych statistic courses has an obsession with assumptions for statistical tests. You may have heard such terms as normality, independence or homoscedacticity. They sound very scary and often intimidate students into running all sorts of tests to check for those assumptions and state things like \"oh no that test for normality was violated I cannot use a parametric test!\". Truth is you always make assumptions of some sorts (even of the sorts like \"my measurement tools actually worked\") and there is no running away from them. Rather than blindly checking assumptions it is much better to actually understand how they impact your model - what is affected if one of the assumptions gets violated and can we do something to deal with it? [Here is a great post on precisely this topic](https://blog.msbstats.info/posts/2018-08-30-linear-regression-assumptions/).\n\n## Categorical predictors\n\nSimple OLS regression is actually quite flexible in what kinds of predictors it can use. You can easily include various categorical predictors into your linear model. The trick is to code them properly.\n\nRemember that the regression coefficient from a linear model tells us what is the predicted difference in Y for 1 unit increase in x? We can use that to code categorical predictors. For those variables that have only 2 levels (e.g. control and experimental condition) you code one as 0 (this level is called the reference category) and the other as 1. Then a 1 unit increase in x is simply the shift from the category coded as 0 to the category coded as 1. Lets look at an example. Lets see if people who want or do not want legalization of marijuana differ in the amount of time they have to relax:\n\n```{r}\nlm_mar <- lm(hrsrelax ~ grass, gss2010)\nsummary(lm_mar)\n```\n\nThis output is quite similar but the coefficients require a bit more attention now. What the output tells us is that people who want legalization of marijuana have on average 3.67 hours for relaxing per day. This is coded by the intercept. People who do not want legalization of marijuana have on average .07 hours more for relaxing per day. This difference is not statistically significant. Notice that we are silently dropping 1343 observations which is most of our sample! In this model `LEGAL` was coded as the reference category by default. This is because unless specified otherwise R will use the category that comes first alphabetically as the reference category. You can change it to set a different reference category with `relevel()`:\n\n```{r}\nlm_mar_2 <- lm(hrsrelax ~ grass, gss2010 %>%\n                 mutate(grass = relevel(grass, ref = \"NOT LEGAL\")))\nsummary(lm_mar_2)\n```\n\nThe output is pretty much the same with intercept and dummy swapped. Now the intercept codes the mean number of hours of relax time for those who do not want legalization of marijuana and\n\nIn case you were wondering, yes we just did a simple t test. We can compare the results to the output of `t.test()` function:\n\n```{r}\nt.test(hrsrelax ~ grass, gss2010)\n```\n\nSo far we had only 2 levels of our categorical predictor. However, often you have to work with variables that have multiple levels. In such case you can no longer code one level as 0 and the other as 1. There are a few ways in which you can deal with this. We'll look at two: dummy coding and effect coding.\n\nDummy coding extends the logic of coding categorical variables as 0 and 1. We still have one level as reference category coded as 0. Other levels are coded as 1 but this time we need 1 variable for each of those levels. We end up with k-1 variables that take values of 0 or 1 where k is the number of levels that a variable has (these are called dummy variables). Each dummy variable is coded 1 if it takes the value of a corresponding level and 0 otherwise. We have k-1 dummy variables because we do not need a variable for the reference level. If all the dummy variables take the value of 0 then we know it has the value of the reference level. Each of the dummy variables is going to express the difference between the reference level and a given dummy level. Note that this coding itself does not allow to compare 2 dummy levels to each other (we'll see how to do that later). Lets see this type of coding in action. We will see if number of hours for relaxing after work differs among people with different degrees:\n\nbefore we do it lets look up the levels of `degree` variable:\n\n```{r}\nlevels(gss2010$degree)\n```\n\n```{r}\nlm_deg <- lm(hrsrelax ~ degree, gss2010)\nsummary(lm_deg)\n```\n\nSince bachelor is first alphabetically it was coded as the reference category. All other dummy variables code comparisons between the reference category and a given level. We can see the for people with bachelor degree we expect 3.48 hours of relax time per day on average. All other degrees are expected to have more relax time on average but none of the comparisons is statistically significant. Again we are dropping a lot of observations.\n\nThe other way to code a categorical variable is through effect coding. This type of coding will use intercept as the grand mean and each variable will reflect the difference between the grand mean and a given level. Not that again we have k-1 variables because the last comparison is just a function of the other ones.\n\n```{r}\nlm_deg_2 <- lm(hrsrelax ~ degree, gss2010, contrasts = list(degree = \"contr.sum\"))\nsummary(lm_deg_2)\n```\n\nIn this case the intercept codes the grand mean and each dummy codes the difference for a given level from the grand mean.\n\nFinally lets compare the results of our model to simple one-way anova:\n\n```{r}\naov(hrsrelax ~ degree, gss2010) %>% summary()\n```\n\nThe F and p values are exactly the same. In fact `aov()` calls `lm()`.\n","srcMarkdownNoYaml":"\n\n# Regression analysis\n\nWe will finally move to actually running statistical models with general linear models. We'll see that a simple t-test and between-subjects anova are just linear models.\n\n## Prediction and inference\n\nRegression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest. When doing inference you are interested in learning how the data was generated as a function of your variables of interest. In prediction you just want to be as accurate as possible. This is of course a bit simplistic and as you'll see looking at predicitons can be very useful for actual inference but this explanation will do for now.\n\nAs an example imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on (e.g. what to invest in).\n\n## Regression engine\n\nThe very idea of making a regression analysis is to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones. How can we do that? We want a way that will approximate our variable of interest so that each time we are to say what value this variable will have we will make the best guess possible. But what does it mean 'the best guess possible'? We need some rule on what it would mean to make a best guess. Since we are predicting a continuous variable we can calculate how much we miss for every prediction (subtract the actual value from the predicted value) and then choose the one that has the smallest error. This is already a start but we can miss in two ways: we can predict too little or too much. The first one is going to produce a negative error and the other one a positive one. Negative numbers are smaller than positive ones so our rule so far will always favor predicting too little! Fortunately there is a very easy way to deal with this - we can square the errors (there are reasons why squaring is preferred to taking absolute values but they are beyond the scope of this course). Now, we can sum all the squared errors for each observation in our dataset. The prediction that gets the smallest sum of squared errors wins! This is pretty much how the Ordinary Least Squares (OLS) regression works.\n\nLets start with the simplest example possible. Lets say we have no other information except our variable that we want to predict. Which value will minimize the sum of squared errors? There's actually a fancy formula for this because OLS regression has a closed form but perhaps a better way to learn this is to actually make a bunch of guesses and see what happens. This is what simulation is perfect for.\n\nLets first simulate a bunch of observations from a normal distribution: our dependent variable. We will keep mean = 0 and standard deviation = 1.\n\n```{r}\nlibrary(tidyverse)\ny <- rnorm(1000)\n```\n\nNext, we'll make a bunch of guesses in the range from -1 to 1 and see how each of them performs (what is their sum of squared errors).\n\n```{r}\nsse <- c() #initialize vector to store sum of squared errors\npred <- seq(-1,1,length.out = 10) #we'll make 10 guesses from -1 to 1, equally spaced\nfor (i in pred) {\n  res <- sum((i - y)^2) # for each guess calculate sum of squared errors\n  sse <- c(sse, res) #append the sum to the vector\n}\npred_sse <-data.frame(pred,sse) #put these \n```\n\nYou can go ahead and inspect the `sse` and `pred`. Can you see for which guess we get the smallest error? You can also look at the animation below:\n\n![predictions and sums of squared errors](images/sse_example.gif){fig-align=\"center\"}\n\nYou can see that the points fall on a really nice parabola. For a guess of mean - 1 standard deviation we get a really big sum of squared errors, they gradually get smaller and smaller and then start to get bigger up to mean + 1 standard deviation. The lowest point of the parabola is at the mean and that is in fact our best guess. When making regression analysis we will be working with means all the time. We'll just be adding more information to the model (e.g. if we add belonging to experimental vs control group into the model, then our best guess will be the mean in each of those groups and we get a two sample t-test). Linear models are all about conditional means i.e. means of one variable given a value of another variable.\n\nA more general way which we can use to think about linear models is that we are modelling Y as following a normal distribution with mean $\\mu$ and variance $\\sigma$. The mean is then determined by the variables we put into the model (those are the conditional means from above). If we don't add anything $\\mu$ is going to be the actual mean in our sample. If we start adding variables into the model then $\\mu_i$ will be determined by them (that's why we have the i).\n\n$$\nY_i \\sim Normal(\\mu_i, \\sigma)\n$$\n\n$$\n\\mu_i =\\alpha\n$$\n\nWhen we add predictors into the model we will be able to assess how much we can improve over just using the mean of the dependent variable (in fact this is what one of the most commons metrics for assessing linear models is - R squared).\n\n## Making a simple regression\n\nBefore we move on to running our first model we have to understand one more thing - formulas in R. They allow us to write models in a very succint form. A formula generally takes the form of `y ~ x1 + x2 + ...`. On the left hand side is always the dependent variable and on the right hand side you put all of your predictors.\n\nWe'll start with a simple simulated example to get an intuition on what exactly a simple linear model does. We will simulate some data, run a model and then plot the predictions against the data.\n\n```{r}\nN <- 1e4\nx1 <- rnorm(N)\ny <- rnorm(N, 2 + .5*x1)\ndf1 <- data.frame(x1, y)\n```\n\nLets run a simple linear model:\n\n```{r}\nlm_sim <- lm(y ~ x1, df1)\n```\n\nOk, now we can get to the plotting. What we will do is:\n\n1.  Plot the original data\n2.  Plot the regression line\n3.  Plot the conditional distributions implied by the model.\n\nYou can inspect the code if you want but it's not that important right now\n\n```{r}\n#| code-fold: true\n#| code-summary: \"simulating data implied by the model\"\n#| fig-align: \"center\"\n\n\nx1 <- seq(min(df1$x1), max(df1$x1), length.out = 10)\n\n#set up a data frame to store simulated values\ndf_sim <- data.frame(x1 = numeric(), sim = numeric())\n\n#run loop for simulation\nfor(i in x1) {\n  #simulate y for given x1\n  sim <- rnorm(1e5, mean = lm_sim$coefficients[[1]] + lm_sim$coefficients[[2]]*i, sd = 1)\n  #store x1 for given simulations\n  x1_1 <- rep(i, 1e5)\n  #store sim and x1 in temp\n  temp <- data.frame(x1 = x1_1, sim = sim)\n  #append simulated data to results data frame\n  df_sim <- rbind(df_sim, temp)\n}\n\n#' the above is based on simulating data from the distribution\ndf_rib <- data.frame(x1 = numeric(), lb = numeric(), hb = numeric())\nfor(i in x1) {\n  q <- qnorm(c(.025, .975), mean = lm_sim$coefficients[[1]] + lm_sim$coefficients[[2]]*i, sd = 1)\n\n  temp <- data.frame(x1 = i, lb = q[1], hb = q[2])\n  df_rib <- rbind(df_rib, temp)\n  \n}\n\n\ndf_sim %>%\n  ggplot(aes(x = x1, y = sim)) +\n  geom_point(data = df1, aes(x = x1, y = y), alpha = .1) +\n  geom_ribbon(data = df_rib, aes(x = x1, ymin = lb, ymax = hb, y = hb), alpha = .2, fill = \"steelblue\") +\n  ggdist::stat_halfeye(alpha = .8, fill = \"navyblue\") +\n  geom_smooth(data = df1, aes(x = x1, y = y), alpha = .4, method = \"lm\", color = \"firebrick4\", fill = \"firebrick4\") +\n  labs(title = \"Conditional distribution for linear regression\",\n       subtitle = \"The plot shows raw data and simualted data from regressing y on x1.\\nRed line is line of best fit, gray area is 95% prediction interval\",\n       y = \"y\") +\n  theme_minimal()\n```\n\nThe plot above is quite packed with information so lets unpack it. First, we have the datapoints plotted as a scatterplot. Next we have the red line which indicates the regression line or in other words the conditional means of `y` given `x1`. The we have the distributions which are data simulated from the model for 10 values of `x1`. Each distribution is what we would expect y to look like given a value of `x1` based on our model. Notice that the mean of each distribution (the black dot) passes exactly through the red line. Next notice that each distribution has the same shape (pretty much a normal distribution - that is the normal distribution assumption) and more or less the same width (that is the homoscedacticity assumption. The light blue area indicates where we would expect 95% of the `y` to fall in for any given value of `x1`.\n\nNow it's time to introduce some data we can work on. We will be working with a dataset that stores information from 2010 General Social Survey. the dataset can be found in the `openintro` package (which stores data from various open source textbooks). Lets load the dataset and take a look at it.\n\n```{r message=F}\nlibrary(openintro)\ndata(\"gss2010\")\nstr(gss2010)\n```\n\nThe dataset stores information on 5 variables: number of hours for relaxing after and average work day, number of days during the last 30 days in which participants mental health was not good, number of hours worked per week, degree and opinion on legalizing marihuana. (the docs are also available [here](#0)). For now we will be mainly intersted in the `hrsrelax` variable and explaining it with `hrs1`.\n\nWe can start with the null model - a model that has no predictors, just the intercept in it. As we already know this intercept will in fact be the sample mean. Intercept can be coded with 1 in the formula. Running a regression in R is actually super simple. You can use the `lm()` function to do it.:\n\n```{r}\nnull_model <- lm(hrsrelax ~ 1, gss2010)\n\n```\n\nWe can inspect the output using `summary()`:\n\n```{r}\nsummary(null_model)\n```\n\nThis summary tells us the following:\n\n1.  the formula and data used for the model (under the \"Call\")\n2.  Rough approximation of the distribution of residuals\n3.  The coefficients: the estimated Intercept equal to `r round(null_model$coefficients[[1]],2)`, its standard error and associated t and p values. This is equal to the mean of the `hrsrelax` variable: `r round(mean(gss2010$hrsrelax, na.rm = T),2)`\n4.  The residual standard error (which in the case of the null model is just the standard deviation of the variable) and degrees of freedom\n5.  The number of deleted observations due to missing values. This one is real unfortunate as `lm()` will silently drop any missing values without so much as a warning! This can cause a lot of trouble so it's worth always inspecting how many observations were in fact dropped.\n6.  Once we add predictors we will also get an omnibus F test statistic and its p value as well as an R squared value.\n\nLets add the first predictor to our model. The model will then look like this\n\n$$\nY_i \\sim Normal(\\mu_i, \\sigma)\n$$\n\n$$\n\\mu_i =\\alpha + \\beta_1 * hrs1\n$$\n\n```{r}\nsimple_fit <- lm(hrsrelax ~ hrs1, gss2010)\n```\n\nNow that we have our model lets proceed to interpret it:\n\n```{r}\nsummary(simple_fit)\n```\n\nWhat this output tells us is:\n\n1.  for a person who works 0 hours per week we expect 5.11 hours of relax time per day on average.\n2.  With each additional hour worked per week we expect .035 hours of relax time less. This means that we expect people whose work time per week differs by 10 to have on average .35 hour difference in their time for relaxation per day. This effect is significant at conventional levels.\n3.  The model dropped 893 observations which had missing values in any of the variables used in the model. Notice that it's a lot!\n4.  The model explains about 4% of variance in `hrsrelax` variable and the F statistic is also significant. This means we were able to reduce the squared erros by 4% compared to the null model.\n\n### A note on assumptions\n\nA lot of psych statistic courses has an obsession with assumptions for statistical tests. You may have heard such terms as normality, independence or homoscedacticity. They sound very scary and often intimidate students into running all sorts of tests to check for those assumptions and state things like \"oh no that test for normality was violated I cannot use a parametric test!\". Truth is you always make assumptions of some sorts (even of the sorts like \"my measurement tools actually worked\") and there is no running away from them. Rather than blindly checking assumptions it is much better to actually understand how they impact your model - what is affected if one of the assumptions gets violated and can we do something to deal with it? [Here is a great post on precisely this topic](https://blog.msbstats.info/posts/2018-08-30-linear-regression-assumptions/).\n\n## Categorical predictors\n\nSimple OLS regression is actually quite flexible in what kinds of predictors it can use. You can easily include various categorical predictors into your linear model. The trick is to code them properly.\n\nRemember that the regression coefficient from a linear model tells us what is the predicted difference in Y for 1 unit increase in x? We can use that to code categorical predictors. For those variables that have only 2 levels (e.g. control and experimental condition) you code one as 0 (this level is called the reference category) and the other as 1. Then a 1 unit increase in x is simply the shift from the category coded as 0 to the category coded as 1. Lets look at an example. Lets see if people who want or do not want legalization of marijuana differ in the amount of time they have to relax:\n\n```{r}\nlm_mar <- lm(hrsrelax ~ grass, gss2010)\nsummary(lm_mar)\n```\n\nThis output is quite similar but the coefficients require a bit more attention now. What the output tells us is that people who want legalization of marijuana have on average 3.67 hours for relaxing per day. This is coded by the intercept. People who do not want legalization of marijuana have on average .07 hours more for relaxing per day. This difference is not statistically significant. Notice that we are silently dropping 1343 observations which is most of our sample! In this model `LEGAL` was coded as the reference category by default. This is because unless specified otherwise R will use the category that comes first alphabetically as the reference category. You can change it to set a different reference category with `relevel()`:\n\n```{r}\nlm_mar_2 <- lm(hrsrelax ~ grass, gss2010 %>%\n                 mutate(grass = relevel(grass, ref = \"NOT LEGAL\")))\nsummary(lm_mar_2)\n```\n\nThe output is pretty much the same with intercept and dummy swapped. Now the intercept codes the mean number of hours of relax time for those who do not want legalization of marijuana and\n\nIn case you were wondering, yes we just did a simple t test. We can compare the results to the output of `t.test()` function:\n\n```{r}\nt.test(hrsrelax ~ grass, gss2010)\n```\n\nSo far we had only 2 levels of our categorical predictor. However, often you have to work with variables that have multiple levels. In such case you can no longer code one level as 0 and the other as 1. There are a few ways in which you can deal with this. We'll look at two: dummy coding and effect coding.\n\nDummy coding extends the logic of coding categorical variables as 0 and 1. We still have one level as reference category coded as 0. Other levels are coded as 1 but this time we need 1 variable for each of those levels. We end up with k-1 variables that take values of 0 or 1 where k is the number of levels that a variable has (these are called dummy variables). Each dummy variable is coded 1 if it takes the value of a corresponding level and 0 otherwise. We have k-1 dummy variables because we do not need a variable for the reference level. If all the dummy variables take the value of 0 then we know it has the value of the reference level. Each of the dummy variables is going to express the difference between the reference level and a given dummy level. Note that this coding itself does not allow to compare 2 dummy levels to each other (we'll see how to do that later). Lets see this type of coding in action. We will see if number of hours for relaxing after work differs among people with different degrees:\n\nbefore we do it lets look up the levels of `degree` variable:\n\n```{r}\nlevels(gss2010$degree)\n```\n\n```{r}\nlm_deg <- lm(hrsrelax ~ degree, gss2010)\nsummary(lm_deg)\n```\n\nSince bachelor is first alphabetically it was coded as the reference category. All other dummy variables code comparisons between the reference category and a given level. We can see the for people with bachelor degree we expect 3.48 hours of relax time per day on average. All other degrees are expected to have more relax time on average but none of the comparisons is statistically significant. Again we are dropping a lot of observations.\n\nThe other way to code a categorical variable is through effect coding. This type of coding will use intercept as the grand mean and each variable will reflect the difference between the grand mean and a given level. Not that again we have k-1 variables because the last comparison is just a function of the other ones.\n\n```{r}\nlm_deg_2 <- lm(hrsrelax ~ degree, gss2010, contrasts = list(degree = \"contr.sum\"))\nsummary(lm_deg_2)\n```\n\nIn this case the intercept codes the grand mean and each dummy codes the difference for a given level from the grand mean.\n\nFinally lets compare the results of our model to simple one-way anova:\n\n```{r}\naov(hrsrelax ~ degree, gss2010) %>% summary()\n```\n\nThe F and p values are exactly the same. In fact `aov()` calls `lm()`.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"monokai","output-file":"16regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"visual","theme":"theme.scss","toc-location":"left","page-layout":"full","title":"Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}