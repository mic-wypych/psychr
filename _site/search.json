[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a website accompanying the Introduction to R course at the Faculty of Psychology at University of Warsaw. The course covers basics of working in R, from the basic operations to fundamental statistics. No knowledge of R is assumed but basic knowledge of statistics is required to follow the course (especially for the latter topics).\nPlease note that this website is not a substitution for the course. It supplements and extends it but cannot replace it."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "r_intro.html",
    "href": "r_intro.html",
    "title": "r_intro",
    "section": "",
    "text": "This chapter introduces R and RStudio and shows you how to perform the most basic operations in R. We’ll go through the basics of what is R, how to navigate in the most common program for working with R called RStudio and we’ll also look into where to look for help when you might need it."
  },
  {
    "objectID": "r_intro.html#what-is-r",
    "href": "r_intro.html#what-is-r",
    "title": "r_intro",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language. It was designed in the 90s mainly with working with data in mind.\n\nR is a programming language that was designed mainly for statistical analyses.\nShort history of R?\n\nSo why bother with R rather than stick to SPSS? There is a number of reasons:\n\nIs open source and free! This becomes especially important when you leave academia and very often can’t work on proprietary software like SPSS.\nHas a great community! The first point also makes it much easier for people to collaborate and create new things for R.\nIs more reproducible! Yes, some proprietary software allows you to write code (like SPSS) but what good is that if you need to buy the software to run it?\nIs more flexible than SPSS. There is generally more things you can do in R and you can customize the code so that it does exactly what you need. Don’t like the defaultsettings? You can change them! If you need to you can just write your own functions to calculate just what you need.\nMore things are being developed in R. Generally more newer stuff is developed in R before it reaches e.g. SPSS.\nIs much faster. Once you start working with large datasets and computation heavy analyses you will need things to run fast. R is pretty good with it (although for some really large things it won’t do and you might want to switch to other programming languages like C++).\n\nYou can download R from here."
  },
  {
    "objectID": "r_intro.html#what-is-rstudio",
    "href": "r_intro.html#what-is-rstudio",
    "title": "r_intro",
    "section": "What is RStudio?",
    "text": "What is RStudio?\nWhile R is a programming language RStudio is an IDE (Integrated Development Environment). Basically, it’s purpose is to make working in R easier and more reproducible. You can download RStudio from here.\nWhen you open RStudio you might see something like this:\n\nWe’ll go through each pane one by one and see what they are used for.\n\n\n\nFirst pane: R scripts\n\n\nThe first pane is where your R scripts are displayed. This is the place where you will most commonly write your code. Scripts are files that contain your code and can be saved.\n\n\n\nSecond pane: console\n\n\nSecond pane is where the console is. That’s where the results of your analyses will appear. You can also write code in the console but you can’t easily save it. Writing code in the console is best if you need to check something quickly and you don’t need it saved. However, it’s best to write the code in the script because then you have all your steps saved and can easily retrace them.\n\n\n\nThird pane\n\n\nThird pane contains a few tabs. The most important for us is the Environment tab. This will by default show the global environment - a place where all the objects that you create in your script are stored (e.g. datasets that you load, results of analyses, plots, etc.). The other tabs show the history (so all the functions you ran), connections to databases and allow access to simple tutorials in R.\n\n\n\nFourth pane\n\n\nFourth pane again contains a few tabs. It’s mainly used for viewing various things that are results of your code. The plots that you create will be displayed here, as well as some other visualizations like tables, maps etc. (the latter two in the Viewer tab). If you look up help for a function (you’ll see in a second how to do that) the documentation for that function will also show up here. You can also view which files are available in your current directory (the place where R will try to look for or save files by default) and the list of available packages."
  },
  {
    "objectID": "r_intro.html#getting-help",
    "href": "r_intro.html#getting-help",
    "title": "r_intro",
    "section": "Getting help",
    "text": "Getting help\nWorking in R, especially in the beginning, might be quite overwhelming. Fortunately there is plenty of places where you can look for help. It’s fairly common to look for help.\nLooking up help\n\nBuilt-in help: you can check the documentation of a function in R by calling ?function_name. The documentation of the function will appear in the help tab. It should contain the basic information on the function: what it does, how it should be called, what arguments the function accepts. It can also contain more detailed information like what is the result of a given function or some examples of how to use it.\nCRAN: short for Comprehensive R Archive Network. You can download the newest version of R from CRAN. It’s also the place where many R packages are stored and can be downloaded from. Documentation of packages can be found there as well. You can visit its website here.\nBooks: there is plenty of great books on R that cover a range of various topics, many of them are available for free on the web. Books allow you to get a much more detailed account of various things you can do in R, they are also often written by people who developed specific packages for doing the things described in the books.\nThe Internet: R community is generally very welcoming and helpful and there is plenty of places where you can look for help on the web, from social media like Twitter to common forums like StackOverflow. One thing to keep in mind about answers from the web is that they do not constitute the ‘official’ solutions so they might be wrong - it’s often a good idea to double check or be sure that you understand the solution proposed by someone on a forum."
  },
  {
    "objectID": "r_intro.html#basic-operations",
    "href": "r_intro.html#basic-operations",
    "title": "r_intro",
    "section": "Basic operations",
    "text": "Basic operations\nNow that we know how the basic interface for working in R looks like and where to look for help if we need it, we can move on to making basic operations in R. As you could see, there are 2 places where we can write code: the console or in the scripts. If you type a command in the console you can run it by pressing enter. In order to run a command from the script go to the line where your command is or highlight it and press ctrl + enter.\nIf you’re feeling like using a very fancy calculator you can turn R into one. All the basic mathematical operations work in the same way. So e.g.\n\n2 + 2\n\n[1] 4\n\n\nOther operations work in a similar way. Raising to a power can be achievied with ^:\n\n2^3 + (3/2)\n\n[1] 9.5\n\n\nOk, so we know we can use R as a calculator. However, that is not very useful."
  },
  {
    "objectID": "r_intro.html#objects",
    "href": "r_intro.html#objects",
    "title": "r_intro",
    "section": "Objects",
    "text": "Objects\n\ncreating objects\nUpdating objects"
  },
  {
    "objectID": "basic_data.html",
    "href": "basic_data.html",
    "title": "Basic data",
    "section": "",
    "text": "Now we have a very fancy calculator lets see what R can actually be more useful for. One of the most basic but at the same time useful things is the ability to save objects in R. An object can be anything with something assigned to it. You can store a number, text or whole datasets in an object. Another way to think about objects is that they are like pointers. You assign a name to an object which then points to something. Whenever you call the name of the object it refers to whatever it points to. This means that you can easily create objects and then make calculations using the objects. These calculations can then be easily reused no matter what is inside the object as long as you don’t get any errors.\n\n\nCreating an object in R is actually very simple. You just assign whatever you want to store in the object to a name you want to use to refer to your object. There are a few ways in which you can make the assignment: <-, = or sometimes ->. After creating an object it should appear in your global environment. When you want to access whatever is stored in your object you can just use the name of the object.\n\n my_first_object <- 1\n my_first_object\n\n[1] 1\n\n\n\n\n\nYou can also easily update/change an object. As a general rule in R you can’t change an object once you created it. The way to actually change it is to recreate an object by assigning a new value to the same name.\n\nmy_object <- 5\nmy_object\n\n[1] 5\n\nmy_object <- 10\nmy_object\n\n[1] 10\n\n\n\n\n\n\n\nYou can perform basic mathematical operations on objects just like you would make them on values. You can also assign such result to a new object\n\nnumber_1 <- 5\nnumber_2 <- 3\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 8\n\n\nIf we now update one of the objects and repeat the same command we will get updated result.\n\nnumber_2 <- 1\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 6\n\n\n\n\n\nYou can also make a number of logical operations. These will generally either be comparisons (is something larger than something else?) or logical operation (just like logic 101 operations like and, or, not). These operations result in a boolean value.\nComparisons are fairly straightforward. You make them with >, < and =. One thing to remember is that to test equality you need double equality sign: ==. A single = is used for assigning objects:\n\nsmall_object <- 5\nlarge_object <- 10\nsmall_object <= large_object\n\n[1] TRUE\n\nsmall_object == large_object\n\n[1] FALSE\n\n\nThere are a few things to keep in mind when making comparisons, especially of other types of objects. If you compare a true/false value to a number, R will convert the boolean to 0 or 1. If you compare strings R will start with the first letter and if they are the same it will move on to the second letter and so on. Alphabetical order determines which string is ‘larger’. Note that for some special signs (like polish signs etc) this might get weird. Another thing to note is that R is case sensitive - ‘A’ is not the same as ‘a’.\n\n'hello' < 'world'\n\n[1] TRUE\n\n\nThe other type of logical operations allow you to create more complicated comparisons. Generally & evaluates as TRUE only if both parts of the expression are TRUE. | (or) evaluates to true only if at least one part of the expression is TRUE. The last operator is the not one: !. It turns TRUE into FALSE and FALSE into TRUE.\n\nsomething_true <- TRUE\nsomething_false <- FALSE\n\nsomething_true & something_false\n\n[1] FALSE\n\nsomething_true | something_false\n\n[1] TRUE\n\n!something_true\n\n[1] FALSE\n\n\nThese operation will be especially useful when we will be filtering datasets (e.g. we want only those observations that have age higher than some value and come from a given region).\n\n\n\nOne might think that perhaps you can add strings together? Ultimately maybe something will come out of it? Lets test it out:\n\n'hello' + 'world'\n\nError in \"hello\" + \"world\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nYou can’t add strings together the same way you would with numbers. WWorking with strings will require some functions. We’ll dig into more details on functions in the future but for now you can think about them as operations that take some input, do something with it and produce some output. Using function generally looks like this function_name(arguments). Arguments of the functions are the inputs.\nYou can put together two strings using paste0() and cat() functions. If you want to learn more about them you can look up their documentation with ?paste0 and ?cat. Notice that cat() inserts a space between the words and paaste0 does not.\n\ncat('Hello', 'World')\n\nHello World\n\npaste0('Hello', 'World')\n\n[1] \"HelloWorld\""
  },
  {
    "objectID": "storing_data.html",
    "href": "storing_data.html",
    "title": "Storing data",
    "section": "",
    "text": "So far we have worked only with single values. However, usually you want to work with whole sets of values like variables or whole datasets. There is a number of types of data you can encounter in R.\n\n\nThe most basic type of data is a vector. Vectors can store any number of values of the same type. You can create a vector using c() function.\n\nmy_very_first_vector <- c(1,2,3)\nmy_very_first_vector\n\n[1] 1 2 3\n\n\nVectors are ordered: they have a first, second value etc. This means that you can access part of a vector -subset them. Subsetting is accomplished with []. You can get a third element of a vector. You can also subset a range of values from a vector wirh :.\n\nlong_vector <- c(1,2,3,4,5,6,7,8,9,10)\nlong_vector[3:5]\n\n[1] 3 4 5\n\n\nIf you try to put different types of values into one vector R will convert the types to a matching one:\n\nmy_vector <- c(1, TRUE, 'some text')\nmy_vector\n\n[1] \"1\"         \"TRUE\"      \"some text\"\n\n\nOperations on vectors: you can make the same operations on vectors as on single values. One of the great features of R is that by default it will make operations element wise - if you try to add two vectors together then the first element from vector 1 will be added to first element of vector 2 and so on.\n\nnumbers <- c(1,2,3,4,5)\nnumbers2 <- c(6,7,8,9,10)\nnumbers + numbers2\n\n[1]  7  9 11 13 15\n\n\nIf the vectors have different lengths then R will start to recycle values from the shorter vectors. But it will output a warning.\n\nshort_v <- c(1,2,3)\nlong_v <- c(1,2,3,4,5)\nshort_v + long_v\n\nWarning in short_v + long_v: długość dłuszego obiektu nie jest wielokrotnością\ndługości krótszego obiektu\n\n\n[1] 2 4 6 5 7\n\n\n\n\n\nFactors are much like vectors except that they are used for storing categorical values - they have levels. You can create a factor by calling factor() and passing it a vector as an argument.\n\nmy_vector <- c('a', 'b', 'a', 'b')\nmy_factor <- factor(my_vector)\nmy_factor\n\n[1] a b a b\nLevels: a b\n\n\nYou can set levels of a factor or specify order.\n\nordered_vec <- c('low', 'high', 'high', 'low', 'low')\nordered_fac <- factor(ordered_vec, ordered = T)\nordered_fac\n\n[1] low  high high low  low \nLevels: high < low\n\n\nNotice how the output looks different now: it includes information on the order.\n\n\n\nWhat is a matrix?\nMatrices are a bit like vectors but they have two dimensions. They can store only one type of values but they have rows and columns.\nHow to create a matrix?\n\n\n\nSubsetting matrices\nSince we have two dimensions subsetting can work both on rows and matrices. The general idea is still the same but we have to specify whether we are subsetting rows or columns. Rows always come first, columns second separated by a comma.\n\n\n\nIn a day to day analysis you will most likely work with data frames most of the time. A data frame is like a matrix in that it has rows and columns but can store different types of values in each column (so that e.g. you can have some variables that are numeric and others that are text). A different way of thinking about data frames is as a list of vectors of the same length with each vector representing a different variable.\n\n\n\nsubsetting data frames\n\n\n\nLists are the final type of basic data in R. They are the most versatile ones - they can store anything inside of them: single values, vectors, matrices, dataframes or even other lists! One important feature of lists is that they are ordered: you can access their elements by position.\n\n\n\nYou might encounter lists if you need to store a number of different things together. E.g. results of many statistical analyses are stored in lists.\nSubsetting lists can be done the same way\nCreating and subsetting lists"
  },
  {
    "objectID": "r_intro.html#basic-values",
    "href": "r_intro.html#basic-values",
    "title": "r_intro",
    "section": "Basic values",
    "text": "Basic values\nWhen analyzing something or writing a program you will probably encounter a number of different types of values. Above you saw one of them: numeric values. These can store numbers with values after the decimal point.. The other types are:\n\ninteger: it stores numbers without decimal point. They can be created by putting capital l after the number e.g. 3L.\nstring: this type refers to to text values. Text is created using quotation marks e.g. ‘this is text’\nboolean/logical: this type refers to logical values: TRUE or FALSE\n\nThese types form a certain hierarchy: some of them can be converted to other but not the other way around. The reason why this is important is that if R encounters more than one type of value in one operation it will often try to convert the types so that they match. It will do so in a way to create the highest matching type. The hierarchy goes in this way: boolean -> integer -> numeric -> string. All the types can be converted to text but not the other way around. Boolean values are converted so that TRUE = 1 and FALSE = 0. o e.g. if you try to add boolean and numeric value, R will convert the boolean to numeric.\n\nTRUE + 6\n\n[1] 7\n\n\nSome operations can’t be performed on certain types of values, e.g. you can’t multiply two strings. If R encoutners two types it can’t work with it will throw an error.\n\n6 * 'this is gonna throw an error'\n\nError in 6 * \"this is gonna throw an error\": argument nieliczbowy przekazany do operatora dwuargumentowego"
  },
  {
    "objectID": "data_viz_2.html",
    "href": "data_viz_2.html",
    "title": "Data visualization part 2",
    "section": "",
    "text": "We know how to make various plots to display the data and how to control the attributes and scales. Now we’ll move on to additional layers in ggplot2\n\n\nThere are situations in which there is too much information to put it in a single plot.\n\n\n\n\nhow to represent statistics on a plot\nPre-calculating values\nstat_summary\n\n\n\n\nThis layer controls how the coordinates of the plot should be handled. Most likely you are used to plots with the cartesian space: an x and y axis that are perpendicular. However you can change that e.g. by fising the ratio of x to y axis, zooming in on a particualr part of the plot or even bending the axis altogether.\n\n\n\nThe final layer, called theme, controlls all the non-data part of the plot: setting fonts, typeface, text size, controlling the background color, plot legend and the grid.\nThere is a number of pre-specified themes that you can use but you can also control everything manually. You can check the documentation of theme() to see just how many elements you can control.\n\nsome example themes\ncontrolling theme yourself\n\n\n\nA note on working with fonts: if you want to work with more fonts than the built-in ones you will have to load them into R. This can sometimes prove quite problematic.\n\nshowtext package"
  },
  {
    "objectID": "data_viz_1.html",
    "href": "data_viz_1.html",
    "title": "Data visualization part 1",
    "section": "",
    "text": "R is absolutely great for plotting\n\n\n-Describe ggplot2 - part of tidyverse.\nOne of the great things about it is that it breaks down each plot into a number of layers that can be changed (more or less) independently.\n\n\n\n-What is grammar of graphics\n-What layers are there in ggplot2\n-ggplot() starts the plot and each layer is chained together with a +.\n\n\n\n-What are aesthetics\n-What are the basic aesthetics\n\n\n\nAfter providing a dataset and aesthetics we have the variables and their mapping to axes on the plot. However, we still don’t have any shapes to actually represent the data. Do we want a scatter plot? Or maybe a bar plot? Or a line plot? The shapes used to represent the data are defined in the geometry layer. Generally all geometries start with geom_ so for example geom_point() will make a scatter plot while geom_bar() will make a bar plot.\nGeometries differ in what aesthetics they accept. You can look up what these are by looking up help for a given geometry. They also differ in what kinds of variables they expect (any combination of categorical vs continuous variables)\n-What are some common geometries:\nNow we can make our first plot in ggplot!\n\nlibrary(ggplot2)\n\nWarning: pakiet 'ggplot2' został zbudowany w wersji R 4.2.2\n\n\nThere is one more thing about geometries and aesthetics. You can set global aesthetics inside the ggplot() function. If you do so these aesthetics will be used by default by all geometries in that plot. You can also set aesthetics inside a given geometry (in fact you can even set a different dataset for a given geometry; this is what we meant by independence of layers) but then they will be used only for this particular geometry and won’t be inherited by other ones.\nCompare the to plots below. They produce the same result:\n\n\n\nAnd the second plot:\n\n\n\n\n\n\nThere are situations in which you don’t want to set some feature to be represented by a given variable but to set them to a fixed value for the entire plot/geometry. For example you might want to set the color or size of all points in a scatter plot. That’s when you set attributes. They are declared inside geometries but outside of aesthetics.\n\n\n\nOne more thing are functions for working with scales: they allow you to have more control over how each scale is represented (e.g. what the breaks and values are, should the scale be transformed)\n\n\n\nA cautionary tale about the limits: scale functions allow you to\n\n\n\nScale functions also allow you to control the color and fill aesthetics.\n\nDiscuss brewer and viridis packages\n\nWhat to be mindful of when setting color palettes:\n\nhow people perceive color\nwhat information you want to represent (is there some order to the variable, is it divergent etc.)"
  },
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "data wrangling",
    "section": "",
    "text": "Now that we have our dataset loaded we can finally get to work with it!\n\n\nWe’ll be working within Tidyverse throughout this course.\n-Explain what tidyverse is\n-How each package functions within tidyverse\nFor now we’ll focus on dplyr.\n\n\n\n-What is the pipe, why it’s useful\n-Native pipe alternative\n\n\n\n-Filter\n-Select\n-Mutate\n-Summarise\n-Group by\n-Arrange\n\n\n\n-Chaining functions together with pipes\nThis already gives us the ability to anwser a number of questions that might be very interesting for analysis.\n\n\n\n-make this section optional: if you want to see how to achieve similar things but without using tiduverse"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Functions",
    "section": "",
    "text": "Introduce the subject: most of the time we don’t just build things from scratch\n\n\nThe insides of\n\n\n\nDefault argument\nalternative argument\n… argument\n\n\n\nWhy spend time building your own functions?\n-How to turn a chunk of code into a function\n-Documentation: why it matters, why do it\n\n\n\nSince R has a huge community people are constantly developing new things you can do in R. You don’t have to define everything from scratch."
  },
  {
    "objectID": "join_restructure.html",
    "href": "join_restructure.html",
    "title": "restructuring_and_joining_data",
    "section": "",
    "text": "-separating and uniting variables\n-separate when there is various number of values\n\n\n-Different types of joins\n-Gotchas in joins\n\n\n\n-pivot longer\n-pivot wider"
  },
  {
    "objectID": "loading_data.html",
    "href": "loading_data.html",
    "title": "loading_data",
    "section": "",
    "text": "There are many ways of loading a dataset or file into your R session so that you can use it. How to do it depends mainly on how the data is stored."
  },
  {
    "objectID": "loading_data.html#loading-flat-files",
    "href": "loading_data.html#loading-flat-files",
    "title": "loading_data",
    "section": "Loading flat files",
    "text": "Loading flat files\n\nloading csv and tsv files"
  },
  {
    "objectID": "loading_data.html#loading-data-from-spss",
    "href": "loading_data.html#loading-data-from-spss",
    "title": "loading_data",
    "section": "Loading data from SPSS",
    "text": "Loading data from SPSS\nHaven vs foreign package\nData coming from statistical software can’t be loaded in such a simple way.\nMainly because it stores more information. Here we’ll focus on .sav format which is used by SPSS. Apart from rows and columns (observations and variables) .sav format stores additional information e.g. on value labels - it is able to attach labels to numbers (like in Likert scales 1 can refer to ‘strongly disagree’). SPSS also allows to specify user-defined missing values (a common practice is to e.g. code missing values with 99).\nThis means we have to somehow deal with this additional information when loading .sav files. There are essentially two ways to go about it: reduce the amount of information stored or introduce a new type of values that can store this additional information. This first approach is taken by the foreign package. The second one is taken by haven package.\nDiscuss the consequences: you keep types of values native to R but loose information or you introduce a new format of data that keeps the information but might not work with some types of analysis\n\nloading data with foreign\nGo through loading data in foreign\nPlease notice though that the documentation for read.spss() function in foreign states that it was originally developed in 2000 and does not guarantee compatibility with newer versions of SPSS (whcih hasn’t changed much since but still).\nBy default foregin will load data into a list rather than a dataframe. You can load into a data frame by setting the argument to.data.frame to TRUE. Another useful argument is use.value.labels which, if set to TRUE will convert the numerical values stored in .sav into their corresponding labels. This is the way foreign deals with labelled values: you can use either numeric values or their labels. In the documentation of the function you can read about additional arguments that control handling of labels.\n\nlibrary(foreign)\n\nOnce we have the data loaded lets see what types of values we have.\n\n\nloading data with haven\nHaven package deals differently with loading labeled variables. It introduces a new type of variable: haven-labelled data. It is capable of storing both numeric values and labels attached to it by adding an attribute to the variable with labels.loops_conditionals\n\nlibrary(haven)\n\nNow that we have loaded the dataset, lets look at the types of variables we have"
  },
  {
    "objectID": "loading_data.html#loading-excel-files",
    "href": "loading_data.html#loading-excel-files",
    "title": "loading_data",
    "section": "Loading excel files",
    "text": "Loading excel files\nOne additional thing you have to take into account when loading data from excel is that it can store a number of sheets in a single file. This has to be taken into account when loading such file into R.\nOne of the packages available for loading excel data is readxl.\n\nlibrary(readxl)"
  },
  {
    "objectID": "loading_data.html#loading-jsons",
    "href": "loading_data.html#loading-jsons",
    "title": "loading_data",
    "section": "Loading jsons",
    "text": "Loading jsons\nThere are situations in which you might work with data that does not come from simple tables but is stored in completely different way. One example that we’ll introduce here is the json format. Json is short for Javascript Object Notation and is a common way of storing data in the web.\nJson stores data as key - value pairs. These might not approximate tabular format and can be nested and fairly complicated. This type of data is especially common when downloading data directly from the web (e.g. social media data) or from APIs. You can imagine a file that stores information on each user of a website: their username, password and all posts that they have created along with information on each post like their creation date. It might look something like this:\nPut an image of a json file here\nBecause json can be a complicated and nested structure the type of data that best approximates it in R is a list. There are ways to ask R to try and handle such list structure and try to convert it into a data frame but it does not always work. Cleaning an unevenly nested json can be a real pain sometimes!\nWe’ll look at an example of a NASA API that stores information on the number of people currently present on space stations. The package we’ll use to load the data is jsonlite.\n\n\n\nNotice how the loaded object looks like. It is a list with …"
  },
  {
    "objectID": "loops_conditionals.html",
    "href": "loops_conditionals.html",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Some introduction to what the flow is and why it’s worth knowing how to control it\n\n\nIf statements, if else statements and combinations\n\n\n\nFor loops\nWhile loops\n\n\n\nDesign some code-along example that would show why this might actually be useful. e.g.\n\na simple game?\nSome simple algorithm?\nCalculator-type of thing"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "What type of problems are we talking about?\n-What is a regression analysis?\n\n\nRegression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest.\nHousing market example: imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on.\n\n\n\nThe very idea of making a regression analysis is ultimately an optimization problem. We want to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones.\n\n\n\n-how to make a regression analysis\n-how to look at the results\n\n\n\n-how to make it\n-Why you should always think first: putting things into regression"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "Anova",
    "section": "",
    "text": "-many groups, many means\n\n\n-how to make a one-way anova.\n\n\n-What these are and how to make them\n\n\n\n-How these are different from post hocs\n-How to make them\n\n\n\n\n\n\n\n-how to interpret the results\n-plotting results\n-effect size and power"
  },
  {
    "objectID": "correlations.html",
    "href": "correlations.html",
    "title": "correlations",
    "section": "",
    "text": "-what are those"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Some introduction on moving on to section about data analysis.\nExploring and cleaning data is often a necessary and fairly long process\nWhy even bother with it?\nIt is a necessary steps because there is a near-infinite number of things that can go wrong when preparing the data from errors in how data was coded to errors in preprocessing or loading the files. Lots of things can also happen that can make analysis or drawing conclusions difficult (like how reliable are your scales, are the items recoded correctly etc.)"
  },
  {
    "objectID": "eda.html#getting-the-basic-information-about-a-dataset",
    "href": "eda.html#getting-the-basic-information-about-a-dataset",
    "title": "Exploratory Data Analysis",
    "section": "getting the basic information about a dataset",
    "text": "getting the basic information about a dataset\n\nif you have it, start with any documentation on the dataset (like codebooks etc.)\nGetting basic info on the data: glimpse etc.\nDescribe from psych package\nGetting information on categorical variables"
  },
  {
    "objectID": "eda.html#exploring-via-plots",
    "href": "eda.html#exploring-via-plots",
    "title": "Exploratory Data Analysis",
    "section": "Exploring via plots",
    "text": "Exploring via plots\nWhy plotting data is always important\nA cautionary tale the boring way: Anscombe quartet\nThe fun way: Gorilla in the data\nThe key takeaway is that a lot of things that are wrong or at least problematic can be immediately spotted when you plot the data. e.g. in when comparing two conditions of an experiment you might spot that the whole effect is driven but just a few outliers."
  },
  {
    "objectID": "eda.html#reliabilities",
    "href": "eda.html#reliabilities",
    "title": "Exploratory Data Analysis",
    "section": "reliabilities",
    "text": "reliabilities\n-getting reliability analysis with alpha()"
  },
  {
    "objectID": "t-tests.html",
    "href": "t-tests.html",
    "title": "t tests",
    "section": "",
    "text": "-The whole idea behind the t-tests.\nThe logic behind a t test."
  },
  {
    "objectID": "01r_intro.html",
    "href": "01r_intro.html",
    "title": "r_intro",
    "section": "",
    "text": "This chapter introduces R and RStudio and shows you how to perform the most basic operations in R. We’ll go through the basics of what is R, how to navigate in the most common program for working with R called RStudio and we’ll also look into where to look for help when you might need it."
  },
  {
    "objectID": "01r_intro.html#what-is-r",
    "href": "01r_intro.html#what-is-r",
    "title": "r_intro",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language. It was designed in the 90s mainly with working with data in mind.\n\nR is a programming language that was designed mainly for statistical analyses.\nShort history of R?\n\nSo why bother with R rather than stick to SPSS? There is a number of reasons:\n\nIs open source and free! This becomes especially important when you leave academia and very often can’t work on proprietary software like SPSS.\nHas a great community! The first point also makes it much easier for people to collaborate and create new things for R.\nIs more reproducible! Yes, some proprietary software allows you to write code (like SPSS) but what good is that if you need to buy the software to run it?\nIs more flexible than SPSS. There is generally more things you can do in R and you can customize the code so that it does exactly what you need. Don’t like the defaultsettings? You can change them! If you need to you can just write your own functions to calculate just what you need.\nMore things are being developed in R. Generally more newer stuff is developed in R before it reaches e.g. SPSS.\nIs much faster. Once you start working with large datasets and computation heavy analyses you will need things to run fast. R is pretty good with it (although for some really large things it won’t do and you might want to switch to other programming languages like C++).\n\nYou can download R from here."
  },
  {
    "objectID": "01r_intro.html#what-is-rstudio",
    "href": "01r_intro.html#what-is-rstudio",
    "title": "r_intro",
    "section": "What is RStudio?",
    "text": "What is RStudio?\nWhile R is a programming language RStudio is an IDE (Integrated Development Environment). Basically, it’s purpose is to make working in R easier and more reproducible. You can download RStudio from here.\nWhen you open RStudio you might see something like this:\n\nWe’ll go through each pane one by one and see what they are used for.\n\n\n\nFirst pane: R scripts\n\n\nThe first pane is where your R scripts are displayed. This is the place where you will most commonly write your code. Scripts are files that contain your code and can be saved.\n\n\n\nSecond pane: console\n\n\nSecond pane is where the console is. That’s where the results of your analyses will appear. You can also write code in the console but you can’t easily save it. Writing code in the console is best if you need to check something quickly and you don’t need it saved. However, it’s best to write the code in the script because then you have all your steps saved and can easily retrace them.\n\n\n\nThird pane\n\n\nThird pane contains a few tabs. The most important for us is the Environment tab. This will by default show the global environment - a place where all the objects that you create in your script are stored (e.g. datasets that you load, results of analyses, plots, etc.). The other tabs show the history (so all the functions you ran), connections to databases and allow access to simple tutorials in R.\n\n\n\nFourth pane\n\n\nFourth pane again contains a few tabs. It’s mainly used for viewing various things that are results of your code. The plots that you create will be displayed here, as well as some other visualizations like tables, maps etc. (the latter two in the Viewer tab). If you look up help for a function (you’ll see in a second how to do that) the documentation for that function will also show up here. You can also view which files are available in your current directory (the place where R will try to look for or save files by default) and the list of available packages."
  },
  {
    "objectID": "01r_intro.html#getting-help",
    "href": "01r_intro.html#getting-help",
    "title": "r_intro",
    "section": "Getting help",
    "text": "Getting help\nWorking in R, especially in the beginning, might be quite overwhelming. Fortunately there is plenty of places where you can look for help. It’s fairly common to look for help.\nLooking up help\n\nBuilt-in help: you can check the documentation of a function in R by calling ?function_name. The documentation of the function will appear in the help tab. It should contain the basic information on the function: what it does, how it should be called, what arguments the function accepts. It can also contain more detailed information like what is the result of a given function or some examples of how to use it.\nCRAN: short for Comprehensive R Archive Network. You can download the newest version of R from CRAN. It’s also the place where many R packages are stored and can be downloaded from. Documentation of packages can be found there as well. You can visit its website here.\nBooks: there is plenty of great books on R that cover a range of various topics, many of them are available for free on the web. Books allow you to get a much more detailed account of various things you can do in R, they are also often written by people who developed specific packages for doing the things described in the books.\nThe Internet: R community is generally very welcoming and helpful and there is plenty of places where you can look for help on the web, from social media like Twitter to common forums like StackOverflow. One thing to keep in mind about answers from the web is that they do not constitute the ‘official’ solutions so they might be wrong - it’s often a good idea to double check or be sure that you understand the solution proposed by someone on a forum."
  },
  {
    "objectID": "01r_intro.html#basic-operations",
    "href": "01r_intro.html#basic-operations",
    "title": "r_intro",
    "section": "Basic operations",
    "text": "Basic operations\nNow that we know how the basic interface for working in R looks like and where to look for help if we need it, we can move on to making basic operations in R. As you could see, there are 2 places where we can write code: the console or in the scripts. If you type a command in the console you can run it by pressing enter. In order to run a command from the script go to the line where your command is or highlight it and press ctrl + enter.\nIf you’re feeling like using a very fancy calculator you can turn R into one. All the basic mathematical operations work in the same way. So e.g.\n\n2 + 2\n\n[1] 4\n\n\nOther operations work in a similar way. Raising to a power can be achievied with ^:\n\n2^3 + (3/2)\n\n[1] 9.5\n\n\nOk, so we know we can use R as a calculator. However, that is not very useful."
  },
  {
    "objectID": "01r_intro.html#basic-values",
    "href": "01r_intro.html#basic-values",
    "title": "r_intro",
    "section": "Basic values",
    "text": "Basic values\nWhen analyzing something or writing a program you will probably encounter a number of different types of values. Above you saw one of them: numeric values. These can store numbers with values after the decimal point.. The other types are:\n\ninteger: it stores numbers without decimal point. They can be created by putting capital l after the number e.g. 3L.\nstring: this type refers to to text values. Text is created using quotation marks e.g. ‘this is text’\nboolean/logical: this type refers to logical values: TRUE or FALSE\n\nThese types form a certain hierarchy: some of them can be converted to other but not the other way around. The reason why this is important is that if R encounters more than one type of value in one operation it will often try to convert the types so that they match. It will do so in a way to create the highest matching type. The hierarchy goes in this way: boolean -&gt; integer -&gt; numeric -&gt; string. All the types can be converted to text but not the other way around. Boolean values are converted so that TRUE = 1 and FALSE = 0. o e.g. if you try to add boolean and numeric value, R will convert the boolean to numeric.\n\nTRUE + 6\n\n[1] 7\n\n\nSome operations can’t be performed on certain types of values, e.g. you can’t multiply two strings. If R encoutners two types it can’t work with it will throw an error.\n\n6 * 'this is gonna throw an error'\n\nError in 6 * \"this is gonna throw an error\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWhoops, we got our first error. An error means that R was not able to execute your command and stopped. You won’t get results of the operation if you get an error (which is different from getting a warning! A warning means that something happened that R wants to tell you about: e.g. it encountered something unusual and had to deal with it in a certain way. Warnings will start with the word Warning. Sometimes it may be a bit confusing because depending on you color settings errors and warnings may be displayed in the same color and it can be a bit scary in the beginning if you get flashing red letter saying something went wrong).\nGenerally an error message will tell you 2 things:\n\nWhere the error happened (in our case in the 6 * 'this is gonna throw an error' line)\nWhat the error is (in our case non-numeric argument to binary operator which just means we used something that is not a number in an operation that requires numbers).\n\nHow exactly an error message is structured largely depends on how well the functions were prepared. If the functions you use are well documented then the error messages should be pretty clear and understandable (which unfortunately is not always the case. This is another reason to document functions well when you start writing your own functions. Be nice to people who start using them!)."
  },
  {
    "objectID": "02data_types.html",
    "href": "02data_types.html",
    "title": "Basic data",
    "section": "",
    "text": "Now we have a very fancy calculator lets see what R can actually be more useful for. One of the most basic but at the same time useful things is the ability to save objects in R. An object can be anything with something assigned to it. You can store a number, text or whole datasets in an object. Another way to think about objects is that they are like pointers. You assign a name to an object which then points to something. Whenever you call the name of the object it refers to whatever it points to. This means that you can easily create objects and then make calculations using the objects. These calculations can then be easily reused no matter what is inside the object as long as you don’t get any errors.\n\n\nCreating an object in R is actually very simple. You just assign whatever you want to store in the object to a name you want to use to refer to your object. There are a few ways in which you can make the assignment: <-, = or sometimes ->. After creating an object it should appear in your global environment. When you want to access whatever is stored in your object you can just use the name of the object.\n\n my_first_object <- 1\n my_first_object\n\n[1] 1\n\n\n\n\n\nYou can also easily update/change an object. As a general rule in R you can’t change an object once you created it. The way to actually change it is to recreate an object by assigning a new value to the same name.\n\nmy_object <- 5\nmy_object\n\n[1] 5\n\nmy_object <- 10\nmy_object\n\n[1] 10\n\n\n\n\n\n\n\nYou can perform basic mathematical operations on objects just like you would make them on values. You can also assign such result to a new object\n\nnumber_1 <- 5\nnumber_2 <- 3\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 8\n\n\nIf we now update one of the objects and repeat the same command we will get updated result.\n\nnumber_2 <- 1\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 6\n\n\n\n\n\nYou can also make a number of logical operations. These will generally either be comparisons (is something larger than something else?) or logical operation (just like logic 101 operations like and, or, not). These operations result in a boolean value.\nComparisons are fairly straightforward. You make them with >, < and =. One thing to remember is that to test equality you need double equality sign: ==. A single = is used for assigning objects:\n\nsmall_object <- 5\nlarge_object <- 10\nsmall_object <= large_object\n\n[1] TRUE\n\nsmall_object == large_object\n\n[1] FALSE\n\n\nThere are a few things to keep in mind when making comparisons, especially of other types of objects. If you compare a true/false value to a number, R will convert the boolean to 0 or 1. If you compare strings R will start with the first letter and if they are the same it will move on to the second letter and so on. Alphabetical order determines which string is ‘larger’. Note that for some special signs (like polish signs etc) this might get weird. Another thing to note is that R is case sensitive - ‘A’ is not the same as ‘a’.\n\n'hello' < 'world'\n\n[1] TRUE\n\n\nThe other type of logical operations allow you to create more complicated comparisons. Generally & evaluates as TRUE only if both parts of the expression are TRUE. | (or) evaluates to true only if at least one part of the expression is TRUE. The last operator is the not one: !. It turns TRUE into FALSE and FALSE into TRUE.\n\nsomething_true <- TRUE\nsomething_false <- FALSE\n\nsomething_true & something_false\n\n[1] FALSE\n\nsomething_true | something_false\n\n[1] TRUE\n\n!something_true\n\n[1] FALSE\n\n\nThese operation will be especially useful when we will be filtering datasets (e.g. we want only those observations that have age higher than some value and come from a given region).\n\n\n\nOne might think that perhaps you can add strings together? Ultimately maybe something will come out of it? Lets test it out:\n\n'hello' + 'world'\n\nError in \"hello\" + \"world\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nYou can’t add strings together the same way you would with numbers. WWorking with strings will require some functions. We’ll dig into more details on functions in the future but for now you can think about them as operations that take some input, do something with it and produce some output. Using function generally looks like this function_name(arguments). Arguments of the functions are the inputs.\nYou can put together two strings using paste0() and cat() functions. If you want to learn more about them you can look up their documentation with ?paste0 and ?cat. Notice that cat() inserts a space between the words and paaste0 does not.\n\ncat('Hello', 'World')\n\nHello World\n\npaste0('Hello', 'World')\n\n[1] \"HelloWorld\""
  },
  {
    "objectID": "02basic_data.html",
    "href": "02basic_data.html",
    "title": "Basic data",
    "section": "",
    "text": "Now we have a very fancy calculator lets see what R can actually be more useful for. One of the most basic but at the same time useful things is the ability to save objects in R. An object can be anything with something assigned to it. You can store a number, text or whole datasets in an object. Another way to think about objects is that they are like pointers. You assign a name to an object which then points to something. Whenever you call the name of the object it refers to whatever it points to. This means that you can easily create objects and then make calculations using the objects. These calculations can then be easily reused no matter what is inside the object as long as you don’t get any errors.\n\n\nCreating an object in R is actually very simple. You just assign whatever you want to store in the object to a name you want to use to refer to your object. There are a few ways in which you can make the assignment: <-, = or sometimes ->. After creating an object it should appear in your global environment. When you want to access whatever is stored in your object you can just use the name of the object.\n\n my_first_object <- 1\n my_first_object\n\n[1] 1\n\n\n\n\n\nYou can also easily update/change an object. As a general rule in R you can’t change an object once you created it. The way to actually change it is to recreate an object by assigning a new value to the same name.\n\nmy_object <- 5\nmy_object\n\n[1] 5\n\nmy_object <- 10\nmy_object\n\n[1] 10\n\n\n\n\n\n\n\nYou can perform basic mathematical operations on objects just like you would make them on values. You can also assign such result to a new object\n\nnumber_1 <- 5\nnumber_2 <- 3\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 8\n\n\nIf we now update one of the objects and repeat the same command we will get updated result.\n\nnumber_2 <- 1\nsum_of_numbers <- number_1 + number_2\nsum_of_numbers\n\n[1] 6\n\n\n\n\n\nYou can also make a number of logical operations. These will generally either be comparisons (is something larger than something else?) or logical operation (just like logic 101 operations like and, or, not). These operations result in a boolean value.\nComparisons are fairly straightforward. You make them with >, < and =. One thing to remember is that to test equality you need double equality sign: ==. A single = is used for assigning objects:\n\nsmall_object <- 5\nlarge_object <- 10\nsmall_object <= large_object\n\n[1] TRUE\n\nsmall_object == large_object\n\n[1] FALSE\n\n\nThere are a few things to keep in mind when making comparisons, especially of other types of objects. If you compare a true/false value to a number, R will convert the boolean to 0 or 1. If you compare strings R will start with the first letter and if they are the same it will move on to the second letter and so on. Alphabetical order determines which string is ‘larger’. Note that for some special signs (like polish signs etc) this might get weird. Another thing to note is that R is case sensitive - ‘A’ is not the same as ‘a’.\n\n'hello' < 'world'\n\n[1] TRUE\n\n\nThe other type of logical operations allow you to create more complicated comparisons. Generally & evaluates as TRUE only if both parts of the expression are TRUE. | (or) evaluates to true only if at least one part of the expression is TRUE. The last operator is the not one: !. It turns TRUE into FALSE and FALSE into TRUE.\n\nsomething_true <- TRUE\nsomething_false <- FALSE\n\nsomething_true & something_false\n\n[1] FALSE\n\nsomething_true | something_false\n\n[1] TRUE\n\n!something_true\n\n[1] FALSE\n\n\nThese operation will be especially useful when we will be filtering datasets (e.g. we want only those observations that have age higher than some value and come from a given region).\n\n\n\nOne might think that perhaps you can add strings together? Ultimately maybe something will come out of it? Lets test it out:\n\n'hello' + 'world'\n\nError in \"hello\" + \"world\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nYou can’t add strings together the same way you would with numbers. WWorking with strings will require some functions. We’ll dig into more details on functions in the future but for now you can think about them as operations that take some input, do something with it and produce some output. Using function generally looks like this function_name(arguments). Arguments of the functions are the inputs.\nYou can put together two strings using paste0() and cat() functions. If you want to learn more about them you can look up their documentation with ?paste0 and ?cat. Notice that cat() inserts a space between the words and paaste0 does not.\n\ncat('Hello', 'World')\n\nHello World\n\npaste0('Hello', 'World')\n\n[1] \"HelloWorld\"\n\n\n\n\n\n\nIf you are not sure what type of value you are working with you can use class() to check what type it is.\n\nquote <- 'Me, poor man, my library/was dukedom large enough'\nclass(quote)\n\n[1] \"character\"\n\n\nThere are situations in which you might want to change the class of the object you are working with. A common situation is when you load a dataset and a variable that should be numeric is loaded as character. It is possible to convert one type into another (but remember about the hierarchy, not everything can be converted to any other type). As a general rule all functions for converting types have the form as. so e.g. as.numeric() will convert a value to a numeric one.\n\nmessy_type <- '1'\ncorrect_type <- as.numeric(messy_type)\nclass(correct_type)\n\n[1] \"numeric\"\n\n\nHowever, be mindful that if something can’t be converted to the desired type R will try to coerce it anyway and will produce missing values (coded in R as NA).\n\nmessy_type <- 'Sweet lord, you play me false'\ncorrect_type <- as.logical(messy_type)\ncorrect_type\n\n[1] NA\n\nclass(correct_type)\n\n[1] \"logical\"\n\n\nNotice that the code above does produce an object of class logical but it stores only NA. Some other things will work however - you can convert numeric values into logical ones. 0 will be converted to FALSE and everything else into TRUE."
  },
  {
    "objectID": "03data_types.html",
    "href": "03data_types.html",
    "title": "Types of data",
    "section": "",
    "text": "So far we have worked only with single values. However, usually you want to work with whole sets of values like variables or whole datasets. There is a number of types of data you can encounter in R. A fairly easy way to orient yourself in the different types of data is:"
  },
  {
    "objectID": "04loops_conditionals.html",
    "href": "04loops_conditionals.html",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Now you know how to create different kinds of objects and how to perform simple operations with them. However, very often you want to add more control over how operations are ran in R. You might want to execute a command only if a condition is satisfied. Or you might want to make the same operations for a number of elements. These are the kinds of situations for which you want to use flow control. What this refers to is basically altering how the code is executed. In a regular situations all commands from your script are executed from the first line all the way down to the last line. Flow control alters that either by specifying conditinal statements that tell R to execute a given chunk of code only if a condition is met or by using loops that repeat a given chunk of code.\n\n\nAnother way conditional statements are referred to which may be more intuitive are if else statements. They allow you to tell R to execute given chunk of code if a condition is met and to do something else if the condition is not met.\nThe general logic of conditional statements looks like this:\n\nif (condition) {\n  Do this\n  And do this\n}\n\nA single if statement can have multiple conditions chained together with | and & operators. So, for example\n\nx &lt;- 5\ny &lt;- -5\n\nif (x &gt; 0 & y &lt; 0) {\n  print(\"Hooray!\")\n}\n\n[1] \"Hooray!\"\n\n\nIn many situations you want to state what is to be done if a condition is met and what to do otherwise. This turns your statement into an if else one. The only difference is that after the if statement you add else and specify what to do then in curly brackets. With this knowledge you can already create the rules for a simple game like paper, rock, scissors!\n\n#set the choice for each player\nplayer1 &lt;- 'scissors'\nplayer2 &lt;- 'rock'\n\n#define an if statement that outputs the result of the game\nif (player1 == player2) {\n  print('draw')\n} else if ((player1 == 'scissors' & player2 == 'paper') |\n           (player1 == 'paper' & player2 == 'rock') |\n           (player1 == 'rock' & player2 == 'scissors')) {\n  print('player 1 wins')\n} else if ((player2 == 'scissors' & player1 == 'paper') |\n           (player2 == 'paper' & player1 == 'rock') |\n           (player2 == 'rock' & player1 == 'scissors')) {\n  print('player 2 wins')\n} else {\n  print('these are not allowed moves')\n}\n\n[1] \"player 2 wins\"\n\n\nTake a moment to study the code above. Notice what kinds of conditions are included in that statement. When writing an if statement it’s a good idea to consider all possible situations and how your if statement maps to them. In a paper, rock, scissors game you can have 3 outcomes: both players choose the same option (a draw), player 1 wins or player 2 wins. Notice that the code above includes also a fourth options specified in the last else statement. What if someone makes a typo and writes rook instead of rock? That last else statement safeguards us for such situations. If we didn’t include it and someone made a type then our if else statement wouldn’t produce anything. You can play around with different values of player1 and player2 to see the results.\nOne more thing about if else statements: in many situations it is a good idea to give some thought to what exactly a given statement is supposed to do and how large the statement needs to be. A good example is an if statement that is supposed to run some check (e.g. make sure that we are working with a numeric value) and stop execution if it detects a problem. Imagine a situation in which we want to do some calculations on numbers and want to make sure that we are indeed working with numeric values. you could design an if else statement that would do it:\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n} else {\n  x + y\n}\n\nError in eval(expr, envir, enclos): This is not a number!\n\n\nTake a moment to look at the code above. Do you think it is good? It certainly gets the job done. Do you think it could be simplified?\nIn fact the else part is redundant in this case. The if statement runs the check on x and y and stops execution of the code if any of them is not numeric. If both values are numeric the execution of code simply proceeds. In this case adding an else statement makes the code harder to read (and imagine what would happen if we had to perform a number of checks like this! We would need a lot of if else statements that would make everything even less clear). The code below does the same thing as the if else statement above but is more clear.\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n}\n## Error in eval(expr, envir, enclos): This is not a number!\nx + y\n## Error in x + y: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\n\n\nAnother way of controlling the flow of your code is by repeating a given chunk of code. There are two basic ways to do that: repeat something a number of times or keep repeating until some condition is met. The first way is called a for loop and the second one a while loop.\n\n\nBefore we make our first for loop lets take a moment to see when a for loop is not needed. Recall again that a lot of things in R are vectorized. This means that operations on vectors are conducted element-wise. Thanks to this if you want to e.g. add 5 to each value stored in a numeric vector (in the language of a for loop: for every element of a vector, add 5 to it) you can just write vector_name + 5. No need for more complicated, explicit repetition. However, there are situations in which you have to make an explicit for loop to repeat something n times. The general structure of a for loops looks like this:\n\nfor (i in object) {\n  Do this to each element\n}\n\nIt’s worth keeping in mind what the i in the for loop is. In the example above i will be every consecutive element of object. However we could do a similar thing with:\n\nfor (i in 1:length(object)) {\n  do this to object[i]\n}\n\nNow each i is a number from 1 to thew length of object and we access each element of object by using a proper (ith) index. Which way of running a for loop you choose might depend on the context. looping explicitly over elements of an object rather than indexes can be more intuitive but imagine you don’t want to do something to every element of an object but only to to a subset (e.g. from 3rd inwards). Doing it with indexes is easier. Generally the best approach is to think what you need first and write code code second, not the other way around.\n\nlibrary(microbenchmark)\n\nWarning: pakiet 'microbenchmark' został zbudowany w wersji R 4.3.1\n\n\n\n\n\nWhile loops will keep executing a given chunk of code as long as some condition is met. They aren’t very common in R, at least not until you start building your own algorithms or simulations from scratch. However, it’s worth knowing what they are in case you encounter them.\nWe can use a while loop to make a very simple simulation. Lets say we want to see how temperatures change from a given temperature (lets say 20 degrees Celsius) across time and that we represent time by some random change from each previous temperature. We can create a vector with such predicted temperatures and see how long it takes for it to reach a certain level (lets say 30 degrees Celsius). We represent the change by adding a random value from a normal distribution with mean = .05 and standard deviation = .5 (this is what the rnorm(1,.05,.5) does). The while loop would look something like this: We first create the initial value and a vector to store all temperatures and next we keep adding the random value to our temperature and storing all temperatures until it reaches 30. The last line tells R to plot all the temperatures as a line plot. This is of course a very, very, very simplistic simulation (temperatures don’t change in such a simple way) but it works to show you the idea behind while loops. We can then calculate e.g. how long it took for the temperature to reach a certain level.\n\nC &lt;- 20\nresults &lt;- c(20)\nwhile (C &lt; 30) {\n  C &lt;- C + rnorm(1,.05,.5)\n  results &lt;- c(results, C)\n}\n\nplot(results, type = 'line', lwd = 2, col=4, xlab = \"days\", ylab = \"temperature\")\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\n\n\n\n\nBecause while loops do not have a fixed number of iteration they can potentially run infinitely. This is usually not something we want so it’s a good idea to make sure that your while loop eventually stops. In case you do get stuck in an infinite loop you can press Esc in your console and this should make RStudio stop the loop by force.\nTruth is while loops are not common in R. You will rarely find yourself in situation where you need to perform some actions while a given condition is true (e.g. keep a program running until a user presses exit; keep displaying a board of a game until a player makes a move). However, it’s still good to know what while loops are so that you will know one when you see it.\n\n\n\n\nThere is a special family of functions in R that makes working with for loops a bit easier. These functions let you specify what to loop over and what function to apply to each element but in a function rather than a whole loop with all the curly brackets and stuff.\nThe reason why this is a whole family of functions is that you can iterate in various ways and you can get the output in different formats. There are more functions in the family but the general ones are:\n\nlapply() - loops over elements of a vector/list and returns a list\nsapply() - same as lapply but tries to simplify the result to a vector or matrix\napply() - used for looping over 2 dimensional structures - it lets you specify if you want to loop over rows or columns\ntapply() - same as apply but lets you split the object you are looping over based on some factor (e.g. imagine you want to calculate the mean value of your dependent variable for each experimental condition).\n\nLets see some of these in action.\n\n\nImagine you are working with a list in R. You want to get information on how many elements each object in the list has. sapply makes it very easy:\n\nmy_list &lt;- list(\n  1:50,\n  sample(300, 5),\n  c(\"random\", \"vector\")\n)\n\nsapply(my_list, length)\n\n[1] 50  5  2\n\n\n\n\n\nThere is a dataset available in R on airquality in New York City called airquality. It stores information on ozone, sun, wind and temperature from 5 months One of the things that might be of interest when looking at the dataset is what was the average value of each of the variables informing on airquality:\n\ndata(\"airquality\")\nd &lt;- airquality\nd &lt;- na.omit(d)\napply(d[,1:4], 2, mean)\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\nNotice that the means calculated above are global means from the entire dataset. What is probably much more sensible is a mean for each month. There is one additional trick needed here. Tapply won’t allow us to split a number of columns by some vector and perform a given operation on each of the columns. That’s because tapply works on vectors. In order to get monthly means for all 4 columns we need to combine apply with tapply. What we need to do is start with apply and loop over the 4 columns of interest and for each of them use tapply that will split a given column by month and calculate the means. Combining functions can get us really far if only we give some thought to what each function does (including what are its inputs and outputs) and what we really need to do.\n\napply(d[,1:4], 2, function(x) tapply(x, d$Month, mean))\n\n     Ozone  Solar.R      Wind     Temp\n5 24.12500 182.0417 11.504167 66.45833\n6 29.44444 184.2222 12.177778 78.22222\n7 59.11538 216.4231  8.523077 83.88462\n8 60.00000 173.0870  8.860870 83.69565\n9 31.44828 168.2069 10.075862 76.89655\n\n\nOne important thing about apply functions is that they are generally faster than explicit for loops. Same thing goes for vectorized code as well - it’s faster than a for loop. We can make a simple comparison by using the microbenchmark package to make the same thing with a for loop and apply() function. We’ll save it as bench\nNow if we look at the results:\n\nbench\n\nUnit: milliseconds\n                                               expr    min      lq     mean\n for (i in 1:ncol(df)) {     print(mean(df[, i])) } 3.5722 4.45345 6.208726\n                                 apply(df, 2, mean) 4.4559 5.71735 9.050546\n  median       uq     max neval cld\n 5.76020  7.35190 16.2620   100  a \n 7.30355 10.58615 31.9904   100   b\n\n\n\n\n\n\n\nTry to code the logic of assigning points to players of a prisoner dilemma with a given matrix:\n\nCreate a for loop that will print out the first 50 numbers from the Fibonacci sequence\nGiven the iris dataframe (you can load it with data(\"iris\") loop over all of its columns and calculate the mean of every numeric column"
  },
  {
    "objectID": "05functions.html",
    "href": "05functions.html",
    "title": "Functions",
    "section": "",
    "text": "Most of the things we do in R doesn’t have to be written from scratch. We have many tools available to get what we want. These tools are functions.\n\n\nThe first approximation to how a function is built is to think of it as a kind of machine. The machine takes some inputs, processes them in some way and returns outputs. The inputs are the arguments you provide to a function like a vector or a dataset. The result is the output. Very often the insides of a function, the machinery within it that is responsible for getting from the input to the output is a black box to us. We have no clue how exactly a function arrives at its result. Sometimes we don’t need to know it but in many situations at least some knowledge is necessary to be certain that the function does exactly what we need it to do and won’t surprise us (an annoying example we will get to later is silent dropping of missing values by some functions).\n\n\n\nLets focus on the inputs. There are a few kinds of them. The most basic ones are input arguments - this is what you put into the machine. Apart from it there are a few other types of arguments that can allow you to have more control over the behavior of functions. They are a bit like toggles and switches on a machine that change how it operates.\nDefault arguments: arguments that are set to some default value. This value will be used unless specified otherwise. An example is the na.rm argument from mean or sum. This argument is set to FALSE by default so that the function will return an error if there are any missing values in the input argument. This is such a good example because it also stresses why choosing proper defaults is really important when writing functions. A lot of people, when they first encounter functions like mean or sum, are surprised or even annoyed. Why in the world set defaults that are more likely to produce errors? We are often fixated on avoiding errors in code but this is not always the way to go in data analysis. We often want functions to operate smoothly and seamlessly. But that is false peace. Smooth behavior is not always what we need from functions. Clunky functions are often good in data analysis because they force us to be explicit with what we do with data. Even if they make climbing the hill a bit more steep they are sure to lead us on the right path to the top.\nYou can also encounter alternative arguments. These arguments have a prespecified set of possible values (usually defined as a vector).= For example the table() function that can give us a frequency table of a factor has a useNA argument that specifies whether to use NA values.It can take three different valus that specify possible behaviour of the function. You can read more onwhat they do inthe documentation of the function.\nThe final type of argument is the … argument. It is a placeholder for any number and kind of arguments that will later on be passed inside the function usually as arguments in some internal function. Take lapply as an example. Apart from the argument X and FUN which specify what to loop over and what function to apply to each element of X it also has the … argument. It’s there because the function you want to apply to every element of X might take some additional arguments. How many and what kind of arguments these are might vary from function to function and the … argument allows us to handle this. Any arguments passed in the … will be used as argument of the function specified in the FUN argument of lapply.\n\n\n\nWhy spend time building your own functions? There are a few general cases. The first and probably most obvious one is when there is no available function that would do what you need. For example there is no available function to find a mode of a vector in R. If you want to find it you need to build your own function. Finding a mode is a simple example but there may be cases where you need to do something more complex or customize the behavior of an already existing function. Second reason is to avoid repetition. If you do similar operations a number of times (e.g. only the dataset or vriables change but all the rest stays the same) then copying and pasting code will soon become problematic. It makes code less readable, longer and more difficult to manage. Imagine you need to change one thing in that code. You’ll need to change it in every place where it was pasted. Writing a function instead means you can just change how you define the function.\nThe general logic for defining a function is as follows:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nTurning a chunk of code into a function can be done quite easily in a few steps. Imagine we want to see what is the probability that a random number drawn from one vector will be larger than the mean of another vector (we will simulate a few variables with rnorm() which draws random numbers from a normal distribution with a given mean and standard deviation):\n\n#simulate vectors\nvar1 &lt;-rnorm(100, 0, 1)\nvar2 &lt;- rnorm(100, 1, 3)\nvar3 &lt;- rnorm(100, .5, 2)\nvar4 &lt;- rnorm(100, 0, 3)\nvar5 &lt;- rnorm(100, 0.1, .2)\n\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n\n[1] 0.57\n\n\n\nBuild the scaffolding of the function. This is exactly what is in the code chunk above:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nPaste the code you want to turn into a function\n\nmy_function &lt;- function(arguments) {\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\nIdentify all the “moving parts”: What will change? Each of these things has to get its own argument (all the moving parts are marked on the right side of the code chunk):\n\nmy_function &lt;- function(arguments) {\n#calculate mean\n1mean_1 &lt;- mean(var1)\n\n#calculate lenght\n2length_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\n3n_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n1\n\nvar_1\n\n2\n\nvar_2\n\n3\n\nvar_2\n\n\n\n\nChange each of the “moving parts” in the code chunk into appropriate argument\n\nmy_function &lt;- function(x, y) {\n#calculate mean\nmean_1 &lt;- mean(y)\n\n#calculate lenght\nlength_2 &lt;- length(x)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(x[x &gt; mean_2])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n\n\nWhen building your own functions, especially if they are going to be used by other people, it’s a good idea to consider potential weird things that could happen. When first creating a function we usually have its typical behaviour in mind because we just want our function to work. However, there’s a whole bunch of weird stuff that might happen if you don’t prepare for it in advance. For example, imagine you want to create a function from scratch that will output the mean of a numeric vector. You could try to do something like a for loop (yes this is slow and inefficient but it’s just for the purpose of demonstration):\n\nmy_mean &lt;- function(x) {\n  sum &lt;- 0\n  for(i in x){\n    sum &lt;- sum + i\n  }\n  result &lt;- sum/length(x)\n  return(result)\n}\n\nPretty straightforward right? Now lets see our function in action on some typical use case and compare its results to the built-in mean() function:\n\nv &lt;- c(1,2,3,4,5,6,7,8,9)\n\nmy_mean(v)\n\n[1] 5\n\nmean(v)\n\n[1] 5\n\n\nYay, we get the same result! Seems like our function works! But before we call it a day and start using our own mean function lets see some less typical cases. E.g. What will happen if the vector has some missing values? Or if its an empty vector? Or if it is not a numeric vector? Lets see:\n\nv_na &lt;- c(1,2,3,NA,5)\nv_empty &lt;- c()\nv_char &lt;- c(\"A\", \"B\", \"C\")\n\nmy_mean(v_na)\n\n[1] NA\n\nmy_mean(v_empty)\n\n[1] NaN\n\nmy_mean(v_char)\n\nError in sum + i: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWe get some weird behaviour. Each of these calls to my_mean() function returned something different. First we got NA when the vector had NAs in it. Passing an empty vector resulted in NaN - short for Not a Number. Finally, passing a character vector gave us an error.. Notice that only the last case gave us an error so if we then implemented our functions in some calculations we might not even notice something is wrong - for example imagine we calculated a mean with our function from a vector with missiing values (e.g we asked a bunch of participants about their mood 5 times a day and now we want to calculate daily average mood and see how it relates to some variables of interest) and then tried to use its output in some other function that had na.rm argument set to TRUE. We’d lose a bunch of information without so much as a warning! That`s why considering possible but unusual cases for a new function is important. It allows us to prepare for possible future problems.\n\n\n\n\nThere are situations in which you might want to use a custom function but not necessarily save it with a name for future use. In such situations we often use what is called an anonymous function (sometimes you can also encounter the term lambda functions). The general way these functions are constructed in R is as follows:\n\n(function(x) WHAT THE FUCNTION DOES)(arguments)\n\nA pretty common situation where you can also encounter these functions is inside iterations like with apply:\n\nv1 &lt;- c(1,2,3,4)\nv2 &lt;- c(3,4,5,6,8)\nv3 &lt;- c(-1,4,3,2)\nv_list &lt;- list(v1,v2,v3)\n\nlapply(v_list, function(x) {x+x})\n\n[[1]]\n[1] 2 4 6 8\n\n[[2]]\n[1]  6  8 10 12 16\n\n[[3]]\n[1] -2  8  6  4\n\n\n\n\n\nOk, so we wrote our super cool new function. We tested it and are pretty confident it works properly. Can we finally call it a day? Again, not so fast. We need one more thing. Imagine you take a long holiday and get back towork after a month or two, How confident are you you will remember how exactly our new function works? Or imagine you share the function you ccreated with other people .Of course you (or others) can read the code of the function to learn that again but that is tedious. That`s why it’s so important to document code. This goes for functions but is just as true for any code that will be used by others or you in the future. Treat yourself in the future like you would treat another person. Documentation is super important! For a lot of people writing (or reading!) documentation is seen as tedious and redundant task. I guarantee you that if you don’t document your own functions you will regret this sooner or later (probably sooner). Very few functions are self explanatory enough to not need any form of documentation. In many cases simply using comments in code with # will be enough. Sometimes building vignettes that shows how to use some functions can be a better idea. Remember to always leave some description of what given code is about and what it does.\n\n\n\nSince R has a huge community people are constantly developing new things you can do in R. You don’t have to define everything from scratch. Usually if you need a function for some statistical procedure or e.g. for plotting some package out there already has it. There is no need to reinvent the wheel.\nIn order to use functions from packages you need to first install the package on your computer. You can do it by calling install.packages(\"PACKAGENAME\") functon. You need to do it only once on a given device (unless you want to update the package or you are using renv but more on that later). Once the package has been installed you can load it in a given R session by calling library(PACKAGENAME). Remember you need to run it every time you open a new R session. Alternatively, after installing a package you can call a function from it directly without loading the package first by using PACKAGENAME::function_name().\nAnother thing to know about functions from packages is name conflict. Since R is open source and most of the packages are developed and maintained by the community it is not so uncommon that two different packages have a function with the same name. You might wonder what will happen if you load both packages and then call this function? Generally, the last package loaded is going to mask previous packages. However this can be problematic e.g. if you are sharing scripts (and someone changes the order of loading packages) or if you actually want to use the function from the first package.\nThere are at least two ways of dealing with this problem. The first one is to be explicit. Above we described a second way of calling a function from a package: PACKAGENAME::function_name(). This way you explicitly state which package the function is from so you shield yourself from name conflict. The second way is by specifying additional arguments to the library() function. If you look up its documentation you can see that it has two optional arguments: exlcude and include.only. They allow you to load a package without some function or to load only some functions from a package. This is useful in situations where you want to load 2 packages with conflicting functions but you know you want to use the conflicting function from only one of them.\n\n\n\n\nRemember the for loop that generated n first numbers from Fibonacci sequence from the class on loops? Now turn it into a function that will return from ith to jth Fibonacci number. Document the function properly so it is clear what it does\nCreate a function that calculates a mode of a vector. Consider potential edge cases and provide tests that show your function behaves properly"
  },
  {
    "objectID": "06loading_data.html",
    "href": "06loading_data.html",
    "title": "loading_data",
    "section": "",
    "text": "So far we’ve been creating the object to work with ourselves. Usually you will work with already existing datasets though (like a file from an experiment, survey etc.). The first thing you need to do to start working on your file is to load it into R session. There are many ways of loading a dataset or file into your R session so that you can use it. How to do it depends mainly on how the data is stored.\nWhen loading a dataset there are generally a few things to consider:\n\nExtension: data can be stored in different ways. This can also be related to what kind of information is stored in a given file. Depending on how a file is stored you might need to load it differently because it stores different information. For example excel files can have multiple sheets while spss files can store labels attached to variables. When loading a file you always have to provide files with extensions.\nfile path: files are stored in different places on your computer (or on the web). In order to load it you need to tell R exactly where to look for the file. A file path speifies exactly where exactly a file is stored e.g. C:/Users/User/Documents specifies the path to the documents folder. A file path can be absolute like the one shown above - its unequivocal. There are also relative file paths that specifies the location of a file relative to current directory. By default R will look in what is called working directory. For simple scripts this by default is set to the documents folder. You can check the current working directory with getwd() function. If you want to change the working directory you can do it with setwd() and pass a string with new directory as argument. This way of working with directories will do for now but it’s not really a good way of managing directories. If you move your script to another folder or share the data and script with other people using setwd() will fail you. Because of this you should avoid manually setting working directory.. One way to deal with this problem is to use here() function from the here package. It uses some heuristics to determine the directory of your script. It can solve the problem of moving or sharing files but remember it uses heuristics so it might not always work. The other solution is to use R projects which automatically set the working directory to the project directory. We’ll learn about them later on."
  },
  {
    "objectID": "06loading_data.html#loading-flat-files",
    "href": "06loading_data.html#loading-flat-files",
    "title": "loading_data",
    "section": "Loading flat files",
    "text": "Loading flat files\nWe’ll start with loading what are called flat files: csv (short for comma separated values) and tsv (tab separated values). Both of these are basically plain text files just structures in a ver specific wayThe name kind of gives away how these formats store data: in csv columns are separated by commas and in tsv columns are separated by tabs. You can see an example of a csv file below:\n\nNot very readable right? There are 2 packages with similar functions for loading .csv and .tsv files: utils and readr. We`ll load the gapminder population data saved as a .csv file:\n\ndf1 &lt;- read.csv(\"datasets/pop.csv\")\nhead(df)\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\nUsing the readr package is very similar:\n\nlibrary(readr)\ndf2 &lt;- read_csv(\"datasets/pop.csv\")\n\nRows: 197 Columns: 302\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (302): country, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 18...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df2)\n\n# A tibble: 6 × 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 Afghani… 3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M \n2 Angola   1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M \n3 Albania  400k   402k   404k   405k   407k   409k   411k   413k   414k   416k  \n4 Andorra  2650   2650   2650   2650   2650   2650   2650   2650   2650   2650  \n5 UAE      40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k \n6 Argenti… 534k   520k   506k   492k   479k   466k   453k   441k   429k   417k  \n# ℹ 291 more variables: `1810` &lt;chr&gt;, `1811` &lt;chr&gt;, `1812` &lt;chr&gt;, `1813` &lt;chr&gt;,\n#   `1814` &lt;chr&gt;, `1815` &lt;chr&gt;, `1816` &lt;chr&gt;, `1817` &lt;chr&gt;, `1818` &lt;chr&gt;,\n#   `1819` &lt;chr&gt;, `1820` &lt;chr&gt;, `1821` &lt;chr&gt;, `1822` &lt;chr&gt;, `1823` &lt;chr&gt;,\n#   `1824` &lt;chr&gt;, `1825` &lt;chr&gt;, `1826` &lt;chr&gt;, `1827` &lt;chr&gt;, `1828` &lt;chr&gt;,\n#   `1829` &lt;chr&gt;, `1830` &lt;chr&gt;, `1831` &lt;chr&gt;, `1832` &lt;chr&gt;, `1833` &lt;chr&gt;,\n#   `1834` &lt;chr&gt;, `1835` &lt;chr&gt;, `1836` &lt;chr&gt;, `1837` &lt;chr&gt;, `1838` &lt;chr&gt;,\n#   `1839` &lt;chr&gt;, `1840` &lt;chr&gt;, `1841` &lt;chr&gt;, `1842` &lt;chr&gt;, `1843` &lt;chr&gt;, …\n\n\nOne big difference is that readr by default gives a message about number of rows, columns and types of variables that were loaded. Notice that it loaded every variable as character. That’s because the original file has “k” and “M” to indicate thousands and millions which is interpreted by R as text. We would need to convert those into proper numbers. Loading .tsv files is exactly the same, the only difference is that functions to do it are named read.tsv() and read_tsv().\nThere’s a bunch of things we can customize when loading data into R. Maybe you don’t need the entire dataset but only a subset of columns? Or you want to manually specify what types of variables you want? Or maybe the dataset you are using does not have variable names? You can specify all of these things as arguments to the loading functions. You can look at the documentation to lean more about them."
  },
  {
    "objectID": "06loading_data.html#loading-data-from-spss",
    "href": "06loading_data.html#loading-data-from-spss",
    "title": "loading_data",
    "section": "Loading data from SPSS",
    "text": "Loading data from SPSS\nData coming from statistical software can’t be loaded in such a simple way as above. Mainly because it stores more information. Here we’ll focus on .sav format which is used by SPSS. Apart from rows and columns (observations and variables) .sav format stores additional information e.g. on value labels - it is able to attach labels to numbers (like in Likert scales 1 can refer to ‘strongly disagree’). SPSS also allows to specify user-defined missing values (a common practice is to e.g. code missing values with 99).\nThis means we have to somehow deal with this additional information when loading .sav files. There are essentially two ways to go about it: reduce the amount of information stored or introduce a new type of values that can store this additional information. This first approach is taken by the foreign package. The second one is taken by haven package. Each of these approaches has some advantages and drawbacks. Stripping labels from values is generally easier and keeps consistency in terms of types of objects you are dealing with. You just keep working with numeric, integer, character or boolean values. The downside is that you lose some information and when e.g. saving a data file back to .sav format you can’t restore them. It might also cause problems if some variables need to be reversed beause you will lose information about that. Using haven allows you to keep all the labels but this is achieved by creating a new type of object: haven-labelled. Thanks to this you can keep all the information e.g. when saving back to .sav format. Most of the time you shouldn’t have problems with this new type of object but some statistical procedures from other packages might not like this and refuse to work. If that happens you can strip the labels using zap_labels().\n\nloading data with foreign\nThe main functiom in foreign is read.spss(). By default foregin will load data into a list rather than a dataframe. You can load into a data frame by setting the argument to.data.frame to TRUE. Another useful argument is use.value.labels which, if set to TRUE will convert the numerical values stored in .sav into their corresponding labels. This is the way foreign deals with labelled values: you can use either numeric values or their labels. In the documentation of the function you can read about additional arguments that control handling of labels.\nPlease notice though that the documentation for read.spss() function in foreign states that it was originally developed in 2000 and does not guarantee compatibility with newer versions of SPSS (whcih hasn’t changed much since but still. At least it doesn’t look like windows xp anymore).\nTo look at loading .sav files we’ll use a trimmed version (the whole file is huge and we don’t need it) of World Value Survey wave 6:\n\nlibrary(foreign)\ndf_foreign &lt;- read.spss(\"datasets/WVS6.sav\", to.data.frame = TRUE)\n\nOnce we have the data loaded lets see what types of values we have.\n\nstr(df_foreign[,1:3])\n\n'data.frame':   89565 obs. of  3 variables:\n $ V1 : Factor w/ 7 levels \"1981-1984\",\"1989-1993\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ V2 : Factor w/ 61 levels \"Not asked in survey\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ V2A: Factor w/ 61 levels \"Algeria\",\"Azerbaijan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nloading data with haven\nHaven package deals differently with loading labeled variables. It introduces a new type of variable: haven-labelled data. It is capable of storing both numeric values and labels attached to it by adding an attribute to the variable with labels. You can load a file in haven using read_spss() (notice the underscore).\n\nlibrary(haven)\ndf_haven &lt;- read_spss(\"datasets/WVS6.sav\")\n\nNow that we have loaded the dataset, lets look at the types of variables we have.\n\nstr(df_haven[,1:3])\n\ntibble [89,565 × 3] (S3: tbl_df/tbl/data.frame)\n $ V1 : dbl+lbl [1:89565] 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6...\n   ..@ label      : chr \"Wave\"\n   ..@ format.spss: chr \"F2.0\"\n   ..@ labels     : Named num [1:7] 1 2 3 4 5 6 7\n   .. ..- attr(*, \"names\")= chr [1:7] \"1981-1984\" \"1989-1993\" \"1994-1998\" \"1999-2004\" ...\n $ V2 : dbl+lbl [1:89565] 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...\n   ..@ label      : chr \"Country/region\"\n   ..@ format.spss: chr \"F4.0\"\n   ..@ labels     : Named num [1:61] -4 12 31 32 36 51 76 112 152 156 ...\n   .. ..- attr(*, \"names\")= chr [1:61] \"Not asked in survey\" \"Algeria\" \"Azerbaijan\" \"Argentina\" ...\n $ V2A: dbl+lbl [1:89565] 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...\n   ..@ label      : chr \"Country/regions [with split ups]\"\n   ..@ format.spss: chr \"F3.0\"\n   ..@ labels     : Named num [1:61] 12 31 32 36 51 76 112 152 156 158 ...\n   .. ..- attr(*, \"names\")= chr [1:61] \"Algeria\" \"Azerbaijan\" \"Argentina\" \"Australia\" ...\n - attr(*, \"label\")= chr \"filelabel\"\n\n\nAnd lets see what happens if we zap the labels.\n\ndf_nolab &lt;- zap_labels(df_haven)\nstr(df_nolab)\n\ntibble [89,565 × 8] (S3: tbl_df/tbl/data.frame)\n $ V1             : num [1:89565] 6 6 6 6 6 6 6 6 6 6 ...\n  ..- attr(*, \"label\")= chr \"Wave\"\n  ..- attr(*, \"format.spss\")= chr \"F2.0\"\n $ V2             : num [1:89565] 12 12 12 12 12 12 12 12 12 12 ...\n  ..- attr(*, \"label\")= chr \"Country/region\"\n  ..- attr(*, \"format.spss\")= chr \"F4.0\"\n $ V2A            : num [1:89565] 12 12 12 12 12 12 12 12 12 12 ...\n  ..- attr(*, \"label\")= chr \"Country/regions [with split ups]\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n $ COW            : num [1:89565] 615 615 615 615 615 615 615 615 615 615 ...\n  ..- attr(*, \"label\")= chr \"COW Country Code\"\n  ..- attr(*, \"format.spss\")= chr \"F2.0\"\n $ C_COW_ALPHA    : chr [1:89565] \"ALG\" \"ALG\" \"ALG\" \"ALG\" ...\n  ..- attr(*, \"label\")= chr \"Country code CoW alpha\"\n  ..- attr(*, \"format.spss\")= chr \"A8\"\n $ B_COUNTRY_ALPHA: chr [1:89565] \"DZA\" \"DZA\" \"DZA\" \"DZA\" ...\n  ..- attr(*, \"label\")= chr \"Country code ISO 3166 alpha\"\n  ..- attr(*, \"format.spss\")= chr \"A8\"\n $ V10            : num [1:89565] 2 2 2 2 1 2 2 2 2 1 ...\n  ..- attr(*, \"label\")= chr \"Feeling of happiness\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n $ V11            : num [1:89565] 1 2 2 1 3 1 2 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"State of health (subjective)\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n - attr(*, \"label\")= chr \"filelabel\"\n\n\nOne last thing about the haven package. SPSS has the habit of storing categorical variables as labelled values (e.g. 1 - blue, 2 - brown). This can be a big problem because R will try to treat them as numeric values. This can lead to some ridiculous errors in analysis. To change a variable from haven-labelled to a factors with values taken from labels you can use as_factor() (again notice the underscore instead of dot)."
  },
  {
    "objectID": "06loading_data.html#loading-excel-files",
    "href": "06loading_data.html#loading-excel-files",
    "title": "loading_data",
    "section": "Loading excel files",
    "text": "Loading excel files\nOne additional thing you have to take into account when loading data from excel is that it can store a number of sheets in a single file. This has to be taken into account when loading such file into R.\nOne of the packages available for loading excel data is readxl.\nThe main difference in excel files is that they can store multiple sheets in one file. Because of that we need to specify which sheet we want to load. You can inspect the names of sheets with excel_sheets().\n\nlibrary(readxl)\n\nexcel_sheets(\"datasets/pop.xlsx\")\n\n[1] \"pop\"\n\n\nOnce we know the sheet names we can load the one that interests us. We need to specify the path to the file and which sheet we want to load. There are of course many other arguments that give additional control over what is loaded but they are quite similar to other loading functions. You can read more on them in the documentation of the function. If you need to load multiple sheets you can save the sheet names as an object and then loop over them using lapply().\n\ndf_excel &lt;- read_excel(\"datasets/pop.xlsx\", sheet = \"pop\")\n\nNow we can look at the top of the file:\n\nhead(df_excel)\n\n# A tibble: 6 × 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 Afghani… 3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M \n2 Angola   1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M \n3 Albania  400k   402k   404k   405k   407k   409k   411k   413k   414k   416k  \n4 Andorra  2650   2650   2650   2650   2650   2650   2650   2650   2650   2650  \n5 UAE      40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k \n6 Argenti… 534k   520k   506k   492k   479k   466k   453k   441k   429k   417k  \n# ℹ 291 more variables: `1810` &lt;chr&gt;, `1811` &lt;chr&gt;, `1812` &lt;chr&gt;, `1813` &lt;chr&gt;,\n#   `1814` &lt;chr&gt;, `1815` &lt;chr&gt;, `1816` &lt;chr&gt;, `1817` &lt;chr&gt;, `1818` &lt;chr&gt;,\n#   `1819` &lt;chr&gt;, `1820` &lt;chr&gt;, `1821` &lt;chr&gt;, `1822` &lt;chr&gt;, `1823` &lt;chr&gt;,\n#   `1824` &lt;chr&gt;, `1825` &lt;chr&gt;, `1826` &lt;chr&gt;, `1827` &lt;chr&gt;, `1828` &lt;chr&gt;,\n#   `1829` &lt;chr&gt;, `1830` &lt;chr&gt;, `1831` &lt;chr&gt;, `1832` &lt;chr&gt;, `1833` &lt;chr&gt;,\n#   `1834` &lt;chr&gt;, `1835` &lt;chr&gt;, `1836` &lt;chr&gt;, `1837` &lt;chr&gt;, `1838` &lt;chr&gt;,\n#   `1839` &lt;chr&gt;, `1840` &lt;chr&gt;, `1841` &lt;chr&gt;, `1842` &lt;chr&gt;, `1843` &lt;chr&gt;, …"
  },
  {
    "objectID": "06loading_data.html#loading-jsons",
    "href": "06loading_data.html#loading-jsons",
    "title": "loading_data",
    "section": "Loading jsons",
    "text": "Loading jsons\nThere are situations in which you might work with data that does not come from simple tables but is stored in completely different way. One example that we’ll introduce here is the json format. Json is short for Javascript Object Notation and is a common way of storing data in the web.\nJson stores data as key - value pairs. These might not approximate tabular format and can be nested and fairly complicated. This type of data is especially common when downloading data directly from the web (e.g. social media data) or from APIs. You can imagine a file that stores information on each user of a website: their username, password and all posts that they have created along with information on each post like their creation date. It might look something like this:\n\n\n\n\n\nBecause json can be a complicated and nested structure the type of data that best approximates it in R is a list. There are ways to ask R to try and handle such list structure and try to convert it into a data frame but it does not always work. Cleaning an unevenly nested json can be a real pain sometimes!\nWe’ll look at an example of a NASA API that stores information on the number of people currently present on space stations. The package we’ll use to load the data is jsonlite.\n\nlibrary(jsonlite)\n\nnasa &lt;- fromJSON(\"http://api.open-notify.org/astros.json\")\n\nNotice how the loaded object looks like. It is a list with 3 elements: number of people as an integer, a dataframe of astronauts and a string “success”.\n\nnasa\n\n$number\n[1] 10\n\n$people\n               name    craft\n1  Sergey Prokopyev      ISS\n2    Dmitry Petelin      ISS\n3       Frank Rubio      ISS\n4     Stephen Bowen      ISS\n5     Warren Hoburg      ISS\n6   Sultan Alneyadi      ISS\n7    Andrey Fedyaev      ISS\n8      Jing Haiping Tiangong\n9       Gui Haichow Tiangong\n10      Zhu Yangzhu Tiangong\n\n$message\n[1] \"success\""
  },
  {
    "objectID": "07data_wrangling.html",
    "href": "07data_wrangling.html",
    "title": "data wrangling",
    "section": "",
    "text": "Now that we have our dataset loaded we can finally get to work with it! We’ll start with the basics of data wrangling: subsetting datasets, sorting variables, changing variables and getting basic summaries. These are the standard things that you might want to do before any statistical modelling.\n\n\nFor data wrangling we’ll be working within tidyverse throughout this course. Tidyverse is a set of packages designed for working with data in a clean, readable way. A huge advantage (apart from readability) is that all packages in tidyverse are designed to be compatible with each other and share common “grammar” and way of doing things. This way it is easy to combine them to do a lot of different things with your data. We’ve already met one package from this collection: readr. Other packages include:\n\ndplyr: package for data wrangling. We’ll focus on it in this class\ntidyr: package for tidying data and reshaping it. We’ll look at it in the next class\nggplot2: the go to package for data visualization in R. Absolutely the best of the best when it comes to plotting.\nforcats: package for working with factors\nstrings: package for working with text data\npurrr: functional programming stuff in R like easier iteration within tidyverse\ntibble: package that introduces slightly altered data frames\n\nTidyverse is not the only way in R for data wrangling (other package often used is data.table, a new alternative is polars). If you don’t want additional packages you don’t even need them, you can do almost everything in base R if you want to. So why choose tidyverse? First of all it’s extremely intuitive. Writing and reading code in tidyverse feels almost like writing plain text of what you want to do with your data. Thanks to pipes it also made code much more readable (more on that in a moment although there are now pipe alternatives). One downside of tidyverse is that it is significantly slower than other packages. If speed is paramount you might want to consider switching to data.table or polars.\n\n\n\nSo far if we wanted to use multiple functions in a single call we had to wrap one function inside another e.g. if we wanted to take a list of vectors, calculate the mean of each vector and then find out the highest mean we could do something like this:\n\nmax(sapply(list(c(1,2,3), c(4,5,6), c(6,7,8)), mean))\n\n[1] 7\n\n\nIt’s not the easiest code to read, right? When combining functions this way you need to read them inside out. This is not how people read. It would e much easier if we could read code more linearly e.g. from left to right and top to bottom. Enter the pipe! The pipe operator allows you to chain together functions in a readable way. The basic idea (there’s more to pipes though) is to take what is on the left hand side of the pipe and pass it as the first argument of whatever is on the right hand side of the pipe. This changes the inside-out into left-to-right code. There are 2 pipes in R. The first one comes from the magrittr package and this is the one used in tidyvese. This pipe looks like this: %&gt;%. If we wanted to rewrite the code above using this pipe it would look like this:\n\nlibrary(magrittr)\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) %&gt;%\n  sapply(mean) %&gt;%\n  max()\n\n[1] 7\n\n\nIt’s much easier to understand what this code does right? An alternative introduced in R 4.1 is the native pipe: |&gt;. The basic functionality is pretty much the same as in the magrittr pipe but you don’t need to load any packages to use it (you might need to enable native pipe in the global options in Tools bar in RStudio). The same code as above but with native pipe looks like this:\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) |&gt;\n  sapply(mean) |&gt;\n  max()\n\n[1] 7\n\n\nYou might wonder why have two kinds of pipes one of which needs loading a new package? The first reason is very simple: magrittr pipe is older. There are however a few differences. You can read about the details here. Remember that pipe automatically passes what is on the left as the first argument to whatever is on the right of the pipe? What if you need to pass it not as the first but second or third argument? Both pipe operators have a placeholder argument that can be used in such situations. %&gt;% has the . operator and |&gt; has _. The difference between them is that _ can only be used once and has to be used with named arguments. Here’s an example of how placeholder argument can work: append() allows you to join two vectors together. The vector passed as the second argument is appended to the one passed as the first argument:\n\nx &lt;- c(1,2,3)\ny &lt;- c(4,5,6)\nx %&gt;%\n  append(y, .)\n\n[1] 4 5 6 1 2 3\n\n\nGenerally, the differences boil down to simplicity: native pipe was deliberately created to be a simpler operator with less functionality. Most of the time you won’t notice much difference (maybe except for how the placeholder argument works).\n\n\n\nNow we can get to the basics of data wrangling in dplyr package. We’ll look at the storms dataset in the dplyr package. It stores information on date, place, status and some other things about storms from 1975 to 2021. The dataset stores multiple observations from each storm because measurements were made every few hours. Before we move one to working with data lets introduce one function: glimpse(). It’s a bit like str() but is a bit more readable for dataframes. This function can give you a concise look at what variables you have in your dataset. Lets load tidyverse, our dataset and look at it:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"storms\")\n\nglimpse(storms)\n\nRows: 19,066\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe dataset has 19066 rows and 13 columns. We can also see that we have various types of variables: many numeric, one factor and one character.\n\n\nYou can subset a dataframe either by columns or by rows. If you want to extract a subset of rows based on some logical conditions you can use filter(). Lets say we want only storms from 2020:\n\nstorms %&gt;%\n  filter(year == 2020)\n\n# A tibble: 863 × 13\n   name    year month   day  hour   lat  long status     category  wind pressure\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Arthur  2020     5    16    18  28   -78.7 tropical …       NA    30     1008\n 2 Arthur  2020     5    17     0  28.9 -78   tropical …       NA    35     1006\n 3 Arthur  2020     5    17     6  29.6 -77.6 tropical …       NA    35     1004\n 4 Arthur  2020     5    17    12  30.3 -77.5 tropical …       NA    35     1003\n 5 Arthur  2020     5    17    18  31   -77.3 tropical …       NA    40     1003\n 6 Arthur  2020     5    18     0  31.9 -77   tropical …       NA    40     1003\n 7 Arthur  2020     5    18     6  33.1 -76.7 tropical …       NA    40     1002\n 8 Arthur  2020     5    18    12  34.4 -75.9 tropical …       NA    45     1000\n 9 Arthur  2020     5    18    18  35.5 -74.7 tropical …       NA    45      993\n10 Arthur  2020     5    19     0  36.2 -73.1 tropical …       NA    50      991\n# ℹ 853 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nWe can also filter based on multiple conditions. This works exactly like all the logical operations we’ve seen previously. One difference is that you can use , instead of &. Lets say we want to get all storms from June 2020:\n\nstorms %&gt;%\n  filter(year == 2020, month == 6)\n\n# A tibble: 57 × 13\n   name       year month   day  hour   lat  long status  category  wind pressure\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Cristobal  2020     6     1    18  19.4 -90.9 tropic…       NA    25     1006\n 2 Cristobal  2020     6     2     0  19.6 -91.6 tropic…       NA    25     1005\n 3 Cristobal  2020     6     2     6  19.6 -92.1 tropic…       NA    30     1005\n 4 Cristobal  2020     6     2    12  19.5 -92.5 tropic…       NA    35     1004\n 5 Cristobal  2020     6     2    18  19.2 -92.6 tropic…       NA    40     1001\n 6 Cristobal  2020     6     3     0  19   -92.5 tropic…       NA    45      996\n 7 Cristobal  2020     6     3     6  18.9 -92.3 tropic…       NA    50      994\n 8 Cristobal  2020     6     3    12  18.8 -92.2 tropic…       NA    50      993\n 9 Cristobal  2020     6     3    13  18.7 -92.1 tropic…       NA    50      993\n10 Cristobal  2020     6     3    18  18.5 -91.9 tropic…       NA    45      994\n# ℹ 47 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nThere were 57 storms recorded in June 2020.\nIf you want to select only certain columns from a dataset you can use select(). E.g. if we want only the name, latitude and longitude of the storm we can do it like this:\n\nstorms %&gt;%\n  select(name, lat, long)\n\n# A tibble: 19,066 × 3\n   name    lat  long\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amy    27.5 -79  \n 2 Amy    28.5 -79  \n 3 Amy    29.5 -79  \n 4 Amy    30.5 -79  \n 5 Amy    31.5 -78.8\n 6 Amy    32.4 -78.7\n 7 Amy    33.3 -78  \n 8 Amy    34   -77  \n 9 Amy    34.4 -75.8\n10 Amy    34   -74.8\n# ℹ 19,056 more rows\n\n\nIf you want to select a range of columns you can use :\n\nstorms %&gt;%\n  select(year:hour)\n\n# A tibble: 19,066 × 4\n    year month   day  hour\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  1975     6    27     0\n 2  1975     6    27     6\n 3  1975     6    27    12\n 4  1975     6    27    18\n 5  1975     6    28     0\n 6  1975     6    28     6\n 7  1975     6    28    12\n 8  1975     6    28    18\n 9  1975     6    29     0\n10  1975     6    29     6\n# ℹ 19,056 more rows\n\n\nA particular situation in which you might want to subset a dataset is to get the rows with highest/lowest values of a variable, get the first/last rows or draw a random sample from the dataset. All of these can be achieved by different versions of slice(). slice_sample() will draw a random sample from the dataset (either by number or proportion). You can also specify if you want to draw with replacements:\n\nstorms %&gt;%\n  slice_sample(n = 100)\n\n# A tibble: 100 × 13\n   name      year month   day  hour   lat  long status   category  wind pressure\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Nicholas  2003    10    21    12  18.7 -51   tropica…       NA    45     1000\n 2 Barry     1983     8    29     0  25.5 -98.5 tropica…       NA    40      995\n 3 Jose      2017     9    15     0  25.4 -67.6 tropica…       NA    60      988\n 4 Olga      2019    10    25    12  24.7 -94.8 tropica…       NA    35     1004\n 5 Luis      1995     9     7     6  21.3 -66   hurrica…        4   120      936\n 6 Michael   2018    10    10    18  30.2 -85.4 hurrica…        4   135      920\n 7 Emmy      1976     8    22    12  16.2 -56   tropica…       NA    35     1006\n 8 Lili      2002    10     1     6  21   -82.2 hurrica…        1    75      970\n 9 Georges   1998     9    28     0  29.3 -88.5 hurrica…        2    95      961\n10 Rina      2017    11     6     0  29.1 -51.2 tropica…       NA    30     1011\n# ℹ 90 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nslice_min() and slice_max() allow you to get rows with highest values on a variable.\n\nstorms %&gt;%\n  slice_max(wind, n = 10)\n\n# A tibble: 24 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 14 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nFinally slice_head() and slice_tail() allow you to get n first or last rows from a dataset.\n\nstorms %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 13\n  name   year month   day  hour   lat  long status       category  wind pressure\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1 Amy    1975     6    27     0  27.5 -79   tropical de…       NA    25     1013\n2 Amy    1975     6    27     6  28.5 -79   tropical de…       NA    25     1013\n3 Amy    1975     6    27    12  29.5 -79   tropical de…       NA    25     1013\n4 Amy    1975     6    27    18  30.5 -79   tropical de…       NA    25     1013\n5 Amy    1975     6    28     0  31.5 -78.8 tropical de…       NA    25     1012\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nOne last thing about filtering. Sometimes you want to filter all unique values of a variable. In order to do it you can use distinct(). It will extract all unique values of a variable. By default it will return only the column with distinct values and drop all the other columns from the dataframe. If you want to keep all the other variables (though remember that it will probably keep only the first entry for each unique value!) you can set the .keep_all argument to TRUE.\n\nstorms %&gt;%\n  distinct(category)\n\n# A tibble: 6 × 1\n  category\n     &lt;dbl&gt;\n1       NA\n2        1\n3        3\n4        2\n5        4\n6        5\n\n\n\n\n\nSorting datasets based on variables is super simple. You can use the arrange() function and if you need to sort in descending order use desc() inside it. Lets say we want to find the storm with the strongest wind:\n\nstorms %&gt;%\n  arrange(desc(wind))\n\n# A tibble: 19,066 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 19,056 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nLooks like Allen from 1980 was the strongest storm.\n\n\n\nCounting values in variables is very simple, just use count(). One thing to remember is that count() will returned a different dataframe. Unless you specify anything additional it will return a dataframe with 2 columns: one will contain all unique values of the variable you counted and the other one, named n will contain their counts (you can specify the name variable to change that to something else). Setting sort argument to TRUE will sort the counts in descending order. E.g. if we want to find out which year had the most measurements of storms (and not the number of storms! Remember that each row is 1 measurement of 1 storm) we can do it with:\n\nstorms %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 47 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2005   873\n 2  2020   863\n 3  1995   762\n 4  2010   663\n 5  2012   654\n 6  2017   610\n 7  2018   608\n 8  2004   604\n 9  2003   593\n10  2021   592\n# ℹ 37 more rows\n\n\n\n\n\nA common task when wrangling data is creating new variables in an already existing dataset. You can do it by using mutate(). Lets say we want to create a new variable that stores information on whether the storm was during summer (June, July, August) or not:\n\nstorms %&gt;%\n  mutate(summer = ifelse(month == 6 | month == 7 | month == 8, TRUE, FALSE))\n\n# A tibble: 19,066 × 14\n   name   year month   day  hour   lat  long status      category  wind pressure\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n# ℹ 19,056 more rows\n# ℹ 3 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;, summer &lt;lgl&gt;\n\n\nRemember that if you want to keep the variable you need to assign the new dataset to an object.\n\n\n\nAnother extremely common task is to get summaries about our dataset. We can do it with summarise() function. e.g. what if we want to see the mean and standard deviation of strength of wind (for now across all measurements):\n\nstorms %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T),\n            sd_wind = sd(wind, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_wind sd_wind\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      50.0    25.5\n\n\nNotice that the shape of the dataset has changed now. The columns are now the summaries and not the original variables.\n\n\n\nSo far we have been calculating things on entire datasets. In many situations you want to calculate something separately for each level of a categorical variable (much like tapply() earlier). To group a dataset we can use group_by(). You can also group by multiple variables at once by separating them by a coma. R will group by the first variable and then by the second etc. This is especially useful for creating summaries. E.g. if we want to get average wind speed for each storm we can easily do it:\n\nstorms %&gt;%\n  group_by(name) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n# A tibble: 258 × 2\n   name     mean_wind\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 AL011993      29.5\n 2 AL012000      25  \n 3 AL021992      29  \n 4 AL021994      24.2\n 5 AL021999      28.8\n 6 AL022000      29.2\n 7 AL022001      25  \n 8 AL022003      30  \n 9 AL022006      31.5\n10 AL031987      21.2\n# ℹ 248 more rows\n\n\nSimilarly if we want to calculate average wind speed from all measurements for each year and month:\n\nstorms %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 253 × 3\n# Groups:   year [47]\n    year month mean_wind\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1  1975     6      37.5\n 2  1975     7      49.7\n 3  1975     8      44.5\n 4  1975     9      55.9\n 5  1975    10      62.4\n 6  1976     8      55.9\n 7  1976     9      61.3\n 8  1976    10      51.0\n 9  1977     8      53  \n10  1977     9      50.6\n# ℹ 243 more rows\n\n\nActually the summarise() function has its own argument for calculating grouped summaries. You can specify .by argument inside the summarise() function. If you use group_by() then summarise() will by default drop the last level of grouping.\nOne more important thing about the group_by() function is that it works by adding an attribute to the dataframe. This means that after a group_by() all subsequent operations will be conducted on the grouped dataframe. E.g. if you sort after grouping then sorting will be conducted within each group separately. You can drop the grouping with ungroup(). There is one last special kind of grouping you might want to use - sometimes you want to perform some operation separately on each row (e.g. calculate the average of a multi item scale for each participant). You can do it with rowwise().\n\n\n\nThere are situations when you want to perform the same operation on multiple columns (e.g. calculate the mean and standard deviation of multiple variables). You can do it by hand but this can be tedious. To simplify it you can use across() inside mutate() or summarise(). the syntax of across() is as follows: the first argument, .cols specifies which columns to perform operations on. The second argument .fns specifies which functions to apply. You have to provide it a list of functions (preferably named list) or a formula. Finally the .names argument specifies how to automatically assign new variable names. E.g. “{.col}_{.fn}” will create variables with column name, underscore and function name (that’s why named list is useful here). If we want to get the mean and standard deviation of wind speed and pressure in each category of storms we could do it in a few lines of code (by the way, notice how group_by() by default includes NA as a separate category):\n\nstorms %&gt;%\n  group_by(category) %&gt;%\n  summarise(across(.cols = c(\"wind\", \"pressure\"), .fns = list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 6 × 5\n  category wind_mean wind_sd pressure_mean pressure_sd\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1        1      71.0    5.55          981.        9.21\n2        2      89.5    3.77          967.        8.94\n3        3     104.     4.18          955.        8.91\n4        4     122.     6.39          940.        9.43\n5        5     147.     6.22          918.       12.0 \n6       NA      38.1   12.0          1002.        9.24\n\n\n\n\n\nOne more useful thing for data wrangling is a set of functions making it easier to select multiple columns based on some condition. There are a few helper functions you can use to do that. They do basically what their names suggest. These verbs are as follows: starts_with(), ends_with() and contains(). This way you don’t have to manually type all the names if they have something in common (e.g. they are items from the same scale so they are named ``scale_1, scale_2 etc.). E.g. lets say we want to get all the columns that end with “diameter”:\n\nstorms %&gt;%\n  select(ends_with(\"diameter\"))\n\n# A tibble: 19,066 × 2\n   tropicalstorm_force_diameter hurricane_force_diameter\n                          &lt;int&gt;                    &lt;int&gt;\n 1                           NA                       NA\n 2                           NA                       NA\n 3                           NA                       NA\n 4                           NA                       NA\n 5                           NA                       NA\n 6                           NA                       NA\n 7                           NA                       NA\n 8                           NA                       NA\n 9                           NA                       NA\n10                           NA                       NA\n# ℹ 19,056 more rows\n\n\nNice! These verbs also work nicely inside across(). One thing to be aware of: if no column matches what you ask for you won’t get an error but a dataframe with all the rows but 0 columns:\n\nstorms %&gt;%\n  select(contains(\"some_weird_name\"))\n\n# A tibble: 19,066 × 0\n\n\n\n\n\n\nWhere tidyverse really shines is in combining multiple functions together with pipes. Through different orders of the functions described above we can get a ton of things out of our dataset. This already gives us the ability to anwser a number of questions that might be very interesting for analysis.\n\nExample 1: Lets say we want to find out the name of storm from each category that had the highest average pressure in 1989.\n\nstorms %&gt;%\n  filter(year == 1989) %&gt;%\n  group_by(category, name) %&gt;%\n  summarise(mean_pressure = mean(pressure, na.rm = T)) %&gt;%\n  slice_max(mean_pressure, n = 1)\n\n`summarise()` has grouped output by 'category'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   category [6]\n  category name      mean_pressure\n     &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1        1 Chantal            987 \n2        2 Dean               972.\n3        3 Hugo               953.\n4        4 Gabrielle          943.\n5        5 Hugo               918 \n6       NA Barry             1012.\n\n\nExample 2: Lets say we want to find the average wind speed at each hour of the day but we want that in kilometers per hours rather than knots (as is in the database). 1 knot is around 1.852 km/h.\n\nstorms %&gt;%\n  mutate(wind_km = wind*1.852) %&gt;%\n  group_by(hour) %&gt;%\n  summarise(mean_wind_km = mean(wind, na.rm = T))\n\n# A tibble: 24 × 2\n    hour mean_wind_km\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1     0         49.6\n 2     1         76.2\n 3     2         64.4\n 4     3         76.7\n 5     4         70.6\n 6     5         76.2\n 7     6         49.7\n 8     7         80.8\n 9     8         68.2\n10     9         60.9\n# ℹ 14 more rows\n\n\n\n\n\n\nThroughout the exercises you’ll still work with the storms dataset\n\nFind out which year had the most storms (remember that each row is 1 measurement of 1 storm. You have to find the year with the most storms and not the most measurements!)\nFind the average wind speed for each storm in 2018 and then sort from the highest to the lowest.\nCalculate the mean and standard deviation of measurement pressure for each month of the year"
  },
  {
    "objectID": "08join_restructure.html",
    "href": "08join_restructure.html",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "We now know a bit about wrangling data. This time we will deal with situations in which your data is not in the correct shape to allow you to calculate what you want. We will look at separating and uniting variables (e.g. what if your dataset has separate columns for year, month and day but you need those in 1 variable?), joining datasets (e.g. what if some information is in 1 dataset but other information you need is in another one?) and reshaping data (going from wide to long format and back again)\n\nWe’ll work with a lovely dataset on all (?) Scooby Doo episodes + reference\n\n\n\nIn order to separate 1 variable into more you can use separate(). The opposite operation can be done with unite().When separating you need to specify which variable to split, what are the names of the new variables (passed as a character vector) and what is the separator which basically tells R where to “cut” the old variable into new ones. By default the old variable is removed from the dataset:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# separating example\n\nUniting is very similar, it just has a reversed effect. You specify what should the name of the new variable be, what are the names of variables to unite and what R should use to separate the values from the old variables.\n\n\nAnother way to separate is by separating into rows. This way each new value will be added in additional row rather than a column. You can do it with separate_rows().\nA potential problem with separate() is when various rows have different number of values. Then you might get conflicting number of columns to create.\n\n\n\n\nIn many situations the information that you need is not stored in a single dataset but in multiple ones. For example you might be working on a longitudinal study and each wave is saved in a separate dataset.\n\nbind cols and bind rows\n\n\n\nDifferent types of joins: inner, left, right, full,\n\n\n\nFiltering joinssemi, anti\nGotchas in joins: duplicated values\n\n\n\n\n\nUN roll calls data?\npivot longer\npivot wider"
  },
  {
    "objectID": "09data_viz_1.html",
    "href": "09data_viz_1.html",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Visualization is an indispensible part of data analysis. If done properly it allows us to understand a lot more about our analysis/data and to understand it much faster than by wading through text. It also looks really nice! And good news is that R is absolutely great for plotting! In this class we’ll look at some basic R plotting functions first and then dive into the world of ggplot2, easily the best plotting package out there.\nIn this class we’ll use the midwest dataset from ggplot2 package. It stores a bunch of information about population of 5 midwestern states: Illinois, Indiana, Michigan, Ohio and Wisconsin. The data are at county level. Lets briefly look at our dataset for this class (all the variables are also described here):\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"midwest\")\nglimpse(midwest)\n\nRows: 437\nColumns: 28\n$ PID                  &lt;int&gt; 561, 562, 563, 564, 565, 566, 567, 568, 569, 570,…\n$ county               &lt;chr&gt; \"ADAMS\", \"ALEXANDER\", \"BOND\", \"BOONE\", \"BROWN\", \"…\n$ state                &lt;chr&gt; \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"…\n$ area                 &lt;dbl&gt; 0.052, 0.014, 0.022, 0.017, 0.018, 0.050, 0.017, …\n$ poptotal             &lt;int&gt; 66090, 10626, 14991, 30806, 5836, 35688, 5322, 16…\n$ popdensity           &lt;dbl&gt; 1270.9615, 759.0000, 681.4091, 1812.1176, 324.222…\n$ popwhite             &lt;int&gt; 63917, 7054, 14477, 29344, 5264, 35157, 5298, 165…\n$ popblack             &lt;int&gt; 1702, 3496, 429, 127, 547, 50, 1, 111, 16, 16559,…\n$ popamerindian        &lt;int&gt; 98, 19, 35, 46, 14, 65, 8, 30, 8, 331, 51, 26, 17…\n$ popasian             &lt;int&gt; 249, 48, 16, 150, 5, 195, 15, 61, 23, 8033, 89, 3…\n$ popother             &lt;int&gt; 124, 9, 34, 1139, 6, 221, 0, 84, 6, 1596, 20, 7, …\n$ percwhite            &lt;dbl&gt; 96.71206, 66.38434, 96.57128, 95.25417, 90.19877,…\n$ percblack            &lt;dbl&gt; 2.57527614, 32.90043290, 2.86171703, 0.41225735, …\n$ percamerindan        &lt;dbl&gt; 0.14828264, 0.17880670, 0.23347342, 0.14932156, 0…\n$ percasian            &lt;dbl&gt; 0.37675897, 0.45172219, 0.10673071, 0.48691813, 0…\n$ percother            &lt;dbl&gt; 0.18762294, 0.08469791, 0.22680275, 3.69733169, 0…\n$ popadults            &lt;int&gt; 43298, 6724, 9669, 19272, 3979, 23444, 3583, 1132…\n$ perchsd              &lt;dbl&gt; 75.10740, 59.72635, 69.33499, 75.47219, 68.86152,…\n$ percollege           &lt;dbl&gt; 19.63139, 11.24331, 17.03382, 17.27895, 14.47600,…\n$ percprof             &lt;dbl&gt; 4.355859, 2.870315, 4.488572, 4.197800, 3.367680,…\n$ poppovertyknown      &lt;int&gt; 63628, 10529, 14235, 30337, 4815, 35107, 5241, 16…\n$ percpovertyknown     &lt;dbl&gt; 96.27478, 99.08714, 94.95697, 98.47757, 82.50514,…\n$ percbelowpoverty     &lt;dbl&gt; 13.151443, 32.244278, 12.068844, 7.209019, 13.520…\n$ percchildbelowpovert &lt;dbl&gt; 18.011717, 45.826514, 14.036061, 11.179536, 13.02…\n$ percadultpoverty     &lt;dbl&gt; 11.009776, 27.385647, 10.852090, 5.536013, 11.143…\n$ percelderlypoverty   &lt;dbl&gt; 12.443812, 25.228976, 12.697410, 6.217047, 19.200…\n$ inmetro              &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0…\n$ category             &lt;chr&gt; \"AAR\", \"LHR\", \"AAR\", \"ALU\", \"AAR\", \"AAR\", \"LAR\", …\n\n\n\n\nBefore we move on to ggplot2 lets look at some built-in base R graphics. The graphics and stats packages have some function for plotting already available.\n\nThe most generic is the plot() function. It allows you to create simple plots in R. The first arguments are usually the variables you want to map onto the axes:\n\nplot(midwest$percadultpoverty, midwest$percollege)\n\n\n\n\nplot() function has a bunch of arguments you can use to customize the plot. For example we can change the color and thickness of the points and add a title to the plot and axes:\n\nplot(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2,\n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nUnfortunately the plot() function does not have great documentation and finding some of the arguments can be quite difficult. Some of these arguments also have very unintuitive names (e.g. argument named lty specifies line type, good luck memorizing that!). Base R has additional functions for more specific plots. Namely, lines() will create a line plot, points() will create a scatterplot and boxplot() will create a boxplot. These functions have better documentation than the general plot() though I still don’t consider it great. Lets see some of these in action. lines() and points() require that you first initialize the plots with the coordinates set to the variables of interest with the plot() function. You can specify what kind of plot (line plot, scatterplot etc) you want inside plot() by setting the type argument. E.g. setting it to l will create a line plot.\nLines: Lets say we want to make a line plot that will display the 10th, 50th and 90th quantile of percent of adults in poverty in all states. We’ll also introduce the axis() function which gives you some control over how the x and y axis should look like. Here we need it because lines() does not like categorical values at x axis so we need to add the state names (we add them at the top) with axis():\n\n#get the quantiles\nlibrary(dplyr)\n#calculate quantiles at of percadultpoverty for each state\nlist_q &lt;- tapply(midwest$percadultpoverty, midwest$state, quantile, c(.1,.5,.9))\n\n#convert the result into a dataframe with each column as one quantile and each row as one state\ndf_q &lt;- data.frame()\ndf_q &lt;- bind_rows(list_q[[1]], list_q[[2]], list_q[[3]], list_q[[4]], list_q[[5]])\ncolnames(df_q) &lt;- c(\"q_10\", \"q_50\", \"q_90\")\n\n#plot them\nplot(1:5, df_q$q_10, ylim = c(0,20), type = \"l\", col = 2, lwd = 2, lty = 2)\nlines(1:5, df_q$q_50, col = 4, lwd = 2, lty = 1)\nlines(1:5, df_q$q_90, col = 2, lwd = 2, lty = 2)\naxis(side = 3, at = 1:5, labels = unique(midwest$state))\n\n\n\n\nOne important thing here: notice how each element is added to the plot on a new line. These functions are not chained together in any way and we are not saving any intermediate objects. This is pretty unusual in R (although perfectly normal in other programming languages). It’s just how base R plotting works.\nPoints: We can recreate the plot that we made with the generic plot() function but using points():\n\nplot(midwest$percadultpoverty, midwest$percollege)\npoints(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2, \n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nBoxplots are useful for showing differences in distributions of some continuous variable between levels of some factor. For example lets say we want to\n\nboxplot(midwest$percadultpoverty ~ midwest$state)\n\n\n\n\n\nThere are also built-in functions for plotting distributions: hist() for histograms, plot(density()) for density functions and plot.ecdf() for cumulative distribution plots. Lets look at all 3 plots for percent of population in college:\n\nhist(midwest$percollege)\n\n\n\n\n\nplot(density(midwest$percollege))\n\n\n\n\n\nplot.ecdf(midwest$percollege)\n\n\n\n\nOne problem with base R plots is that they are not intuitive. Lots of arguments have weird names and doing some things is really not so easy. Making more complicated plots (e.g. adding text annotations on the plot) is also generally hard to do. That’s why we’ll focus on ggplot2 which is much more intuitive and versatile.\n\n\n\nggplot2 is a package in th tidyverse designed for making data visualizations. One of the great things about it is that it breaks down each plot into a number of layers that can be changed (more or less) independently. This idea is encapsulated in what is called the grammar of graphics.\n\n\n\nThe name comes from a book by Leland Wilkinson under the same title. Basically making a plot with the grammar of graphics is like making a building with lego blocks with different colors. You can mix the colors of the blocks to get exactly the building you want. Similalry in ggplot2 you can use different layers to make the plot that you want (e.g. mix different datasets or add a line to a scatterplot). Ok, but what are those layers? In ggplot2 there are following layers:\n\nData: the datasets (usually data frames) you want to plot\nAesthetics: mapping between data and the plot, e.g. what is to be mapped to x and y axis\nGeometry: shapes used to represent data\nStatistics: any statistics like means or confidence intervals that you want to add\nCoordinates: coordinates of the plot (axes limits, cartesian vs polar coordinates etc.)\nFacets: this layer is used for making subplots (e.g. separate plot for each level of a categorical variable)\nTheme: All non-data stuff like fonts, titles\n\nYou always start the plot with ggplot() function. All the lego blocks that you want to add are chained together with +. Lets see what happens when we pass the first layer, data, to our plot:\n\nmidwest %&gt;%\n  ggplot()\n\n\n\n\nHmm, we get en empty plot. Why is that? Well, all we supplied so far is the data we want to plot but we did not include any additional information so R doesn’t know yet what exactly should be displayed on the plot. We need to add the next layer: aesthetics.\n\n\n\nAesthetics map what variables should be displayed on the plot in what way. In this layer you declare e.g. what should be on the x and y axis. The basic aesthetics (the list is not exhaustive) are:\n\nx axis\ny axis\ncolour: colour of points and lines (that includes borders of e.g. rectangles)\nfill: colour of filling\nalpha: transparency. 0 means completely transparent, 1 means not transparency\nsize: point size\nlinewidth: width of lines\nlinetype: type of line (e.g. dashed)\nshape: shape of points (circles, rectangles, etc.)\nlabels: text\n\nAesthetics are declared within aes(). It can be declared within ggplot() function, separately or inside geoms (more on those in a second). Lets see what happens when we add aesthetics to our plot:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege)\n\n\n\n\nNow we got our axes! Notice there is no data on the plot though. That’s because so far we have declared which dataset to plot and which variables to map onto axes but we did not specify how to represent the data. This is declared in the next layer: geometry.\n\n\n\nAfter providing a dataset and aesthetics we have the variables and their mapping to axes on the plot. However, we still don’t have any shapes to actually represent the data. Do we want a scatter plot? Or maybe a bar plot? Or a line plot? The shapes used to represent the data are defined in the geometry layer. Generally all geometries start with geom_ so for example geom_point() will make a scatter plot while geom_bar() will make a bar plot.\nGeometries differ in what aesthetics they accept. You can look up what these are by looking up help for a given geometry. Each geometry has all the aesthetics it accepts in its documentation. They also differ in what kinds of variables they expect (any combination of categorical vs continuous variables).\nLets expand the initial plot with geom_point() to make a scatterplot!\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege) +\n  geom_point()\n\n\n\n\nWe got a plot very similar to the one we made with base R! We can customize it further if we want to. Lets say we want to represent each state with a different color. We just need to add a color aesthetic. R will add a legend automatically:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege, color = state) +\n  geom_point()\n\n\n\n\nThere is one more thing about geometries and aesthetics. Remember you can declare aesthetics in different places? You can set global aesthetics inside the ggplot() function. If you do so these aesthetics will be used by default by all geometries in that plot. You can also set aesthetics inside a given geometry (in fact you can even set a different dataset for a given geometry; this is what we meant by independence of layers) but then they will be used only for this particular geometry and won’t be inherited by other ones.\nCompare the to plots below. They produce the same result:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point()\n\n\n\n\nAnd the second plot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege))\n\n\n\n\nSome common geometries you can encounter are as follows:\n\ngeom_histogram() and geom_density() for displaying distributions of continuous variables\ngeom_bar() for displaying counts of categorical variables (you can change counts to other summary statistic but more on that later)\ngeom_col() for displaying differences in some continuous variable between levels of a factor\ngeom_point(): your good old scatterplot\ngeom_boxplot()\ngeom_violin() a bit like boxplot but displays a distribution of a continuous variable for each level of a factor\ngeom_smooth() for displaying lines of best fit (e.g. from a linear model)\n\nYou can check them out if you want to to see how they look like.\nLets look at 2 thing in a bit more details with regard to geometries. First, lets look at combining geoms. We can add a line of best fit to our scatterplot if we want to by adding an additional geometry. geom_smooth() will use a GAM or LOESS by default (depending on how many unique values x variable has) but we can set it to a linear model by adding method = \"lm\". A linear model probably won’t do well here but it’s just for demonstration:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point() +\n  geom_smooth(method  =\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee? Just like building with lego blocks.\nLast thing before we move on: a few words on geom_bar() and geom_col(). They often get mixed up at the beginning because they both show bar plots. geom_bar() takes a single aesthetic and is generally used to display counts of some factor variable. For example if we wanted to see how many counties are there in each state we could use geom_bar() (later we’ll see how to display e.g. means of variables across levels of a factor):\n\nmidwest %&gt;%\n  ggplot(aes(x=state)) +\n  geom_bar()\n\n\n\n\ngeom_col() takes at least 2 aesthetics: x and y. It needs one categorical and one continuous variable. By default it is going to sum all values in a given category. e.g. lets look at the sum of area of all counties in each of the states:\n\nmidwest %&gt;%\n  ggplot(aes(x=state, y = area)) +\n  geom_col()\n\n\n\n\nWhat if we wanted to look at the average county area in each state? We can e.g. first summarise the dataset and then pipe it to plot:\n\nmidwest %&gt;%\n  summarise(mean_area = mean(area), .by = state) %&gt;%\n  ggplot(aes(x = state, y = mean_area)) +\n  geom_col()\n\n\n\n\nSince ggplot2 is part of tidyverse it’s really easy to pipe a set of dplyr functions into a plot in a single call!\n\n\n\nThere are situations in which you don’t want to set some feature to be represented by a given variable but to set them to a fixed value for the entire plot/geometry. For example you might want to set the color or size of all points in a scatter plot. That’s when you set attributes. They are declared inside geometries but outside of aesthetics. Lets say we want to take our scatterplot and change the transparency and color of the points. We can do it by defining them inside geometry. The names for the arguments are the same as for aesthetics. Just remember to use them outside of aes()!\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3)\n\n\n\n\nBy the way, you can define colors either with hex values as RGB but you can also use one of the built-in colors in R. You can check all the names of those built-in colors with colors() function. Another neat thing is that in newer version of RStudio you can see the preview of the colors when you type them in a script.\nOne potential problem is when aesthetic and scale come into conflict. E.g. what will happen if we map color to state variable and then set it as attribute? Lets see:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), color = \"red\", alpha = .3)\n\n\n\n\nThe attribute overrides the aesthetic. It’s something to remember about.\n\n\n\nOne more thing are functions for working with scales: they allow you to have more control over how each scale is represented (e.g. what the breaks and values are, should the scale be transformed). For example notice that R automatically chose some limits for the scales and displayed breaks (the values on the axes). You can have more control over that with the scale_ family of functions. It’s a family of functions because you need to declare: 1) which aesthetic you want to change and 2) what kind of scale you are working with (continuous, discrete or binned). So for example scale_x_continuous() allows you to customize a continuous x axis. The basic things that you can change inside the scale_ function are: breaks, labels, limits and the expand argument (the last one controls additional space - notice that e.g. on the x axis there is a little space on the left of 0 and there is similarly a little space to the right of the point with the highest percadultpoverty value).\nLets say now we want to work a little with our scales. Lets change the breaks to be every 5%, change the labels to be actually in percentage format (there is a very neat function in scales package called label_percent() that does that. We just need to set scale argument in it to 1 because in our dataset e.g. 10% is represented as 10 and not 0.1):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,40,5),labels = scales::label_percent(scale = 1)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\n\n\n\nA cautionary tale about using limits in scale_ functions: These limits work by filtering the data to be plotted. This can be a serious problem because you basically lose data when plotting and only get a warning about it. This can be especially problematic if you are trying to display bars or ranges because if e.g. one side of an interval falls out of the limits, the entire range will not be displayed. Lets see what happens if we set limits on one of our axes:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,25,5),labels = scales::label_percent(scale = 1), limits = c(0, 25)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice the warning about removed data. This is especially problematic in bar plots because by default they start from 0 (which can be a good thing because it reduces misinterpretation of visual differences between bars but can be undesired e.g. if displaying results of a 1-5 Likert scale which doesn’t have a 0). Another problematic situation is when plotting some summaries e.g. means and limiting axes. Limits will work before calculating the summaries so you might get nonsensical plots because of this like below where we try to plot mean percadultpoverty by each state and limit the y axis to 10:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percadultpoverty)) +\n  geom_bar(stat = \"summary\", fun = mean) +\n  scale_y_continuous(limits = c(0,10))\n\nWarning: Removed 220 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nThe warning now says that 220 rows were removed! What’s even weirded we still got our plot but the means were calculated based on trimmed variable.\n\n\n\nScale functions also allow you to control the color and fill aesthetics. You can change which color palette you want to use. There are many packages available with predefined color palettes (e.g. viridis or MetBrewer). We’ll focus on the built-in Brewer palettes and on making manual palettes. Brewer palettes can be invoked with scale_color_brewer() and scale_fill_brewer(). You can choose color palette by setting the palette argument. I can never remember the names of all the palettes but you can easily google them (just type something like “R brewer palettes). Lets change our palette on the scatterplot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also set your own colors and create your own palette. In order to do that use scale_color_manual() and scale_fill_manual(). You can set the colors as hex or built-in colors in values argument. You can provide it a simple text vector or named vector to explicitly map each categorical variable (if you have categorical variables mapped to color or fill):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_manual(values = c(\"IL\" = \"#5f0f40\", \"IN\" = \"#9a031e\", \"MI\" = \"#fb8b24\", \"OH\" = \"#e36414\", \"WI\" = \"#0f4c5c\"))\n\n\n\n\nfor contiunous variables matched to color aesthetic this works a little different. You need scale_color_gradient() function. scale_color_gradient2() allows you to specify a midpoint and thus make a divergent palette. Lets code county area with color this time and specify our own palette with midpoint at the mean county area:\n\nmidpoint &lt;- mean(midwest$area)\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = area)) +\n  scale_color_gradient2(low = \"#e63946\", mid = \"#8d99ae\", high = \"#1d3557\", midpoint = midpoint)\n\n\n\n\nWhen using colors on a plot you need to be mindful of a number of things. Color is used to convey information so it should be visible. Try to avoid very small contrasts if you want something to stand out (e.g. using light grey on white background). Also remember that not everyone perceives color the same way so it is worth checking if your palette is suitable for everyone. Finally, color shouldn’t be used just because “it looks cool”. Generally everything that is in a plot should be there for a reason. Remember that data visualizations should convey information. That’s their primary purpose. Using color can help with that but it can also make things more difficult to understand. E.g. using a lot of flashy colors just for the sake of using colors might make a plot much more difficult to understand for viewers. Finally, colors can be used to convey different kinds of information, from continuity (e.g. a palette going from light to dark blue to code a continuous variable liek percentage of adults in college), divergence (a palette going from dark blue through light blue and red to dark red to display polarization of opinions) or contrast (e.g. contrasting colors in a palette to display different US states).\n\n\n\nOne issue that pops up quite often, especially when adding color to aesthetics, is how to deal with overlapping values. On the plots above there were some overlapping points on the scatterplot but we could easily deal with it by adjusting transparency. There are situations when this is not so easy or even not desirable. For example you can’t just as easily deal with overlapping bars. ggplot2 has a number of position adjustments that help us deal with this problem. THey allow us to stack, normalize or nudge shapes so that they won’t overlap. The basic position adjustments are:\n\ndodge: move shapes to the side (how much is controlled with the width argument)\nstack: stack shapes on top of each other\nfill: stack shapes on top of each other and normalize height (good for displaying proportions)\njitter: add some random noise (you can adjust how much with height and width arguments). It’s good for cluttered scatterplots\nnudge: slightly move shapes, good for nudging text\n\nOk, lets see some of them in action. We’ll start with jitter. In some situation you might want to display point with a categorical x axis. This can be useful when showing a distribution, e.g. with geom_violin() and adding all the data points on top of it with geom_point(). IF we don’t use any position adjustments we’ll get something like this (we want to look at distribution of percollege in each state):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point()\n\n\n\n\nOk, lets add some jitter but constrain it to be only in width (adding jitter in height might change how we interpret some values!):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point(position = position_jitter(height = 0, width = .2))\n\n\n\n\nThis makes it much easier to see all the points!\nNow we’ll move on to position adjustments in barplots. Lets say we want to look at number of counties in each state that are or are not in metro area(inmetro variable). We can do it by adding a fill aesthetic to the geom_bar():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar()\n\n\n\n\nBy default geom_bar() uses the stack position adjustment. Lets experiment with it a little and see what happens if we add a fill adjustment:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar(position = \"fill\")\n\n\n\n\nNow we have proportions rather than counts. Remember that order here matters (proportions sum to 1 within each state. If we wanted to see the share of each state in metro vs non-metro counties in the midwest we would need to switch x and fill aesthetics).\nTo see position dodge we will look at geom_col(). Lets say we want to plot the average percentage of adults in poverty in metro and nonmetro counties in each state:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(state, inmetro)) %&gt;%\n  ggplot(aes(x = inmetro, y = mean_perc, fill = state)) +\n  geom_col()\n\n\n\n\nThis looks pretty bad right? geom_col() also uses stack adjustment by default. To make it more readable and compare the percentages we can use the dodge adjustment:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(inmetro, state)) %&gt;%\n  ggplot(aes(x = as.factor(inmetro), y = mean_perc, fill = state)) +\n  geom_col(position = position_dodge(width = .9))\n\n\n\n\nMuch better! Now we can compare the percentages across state and metro vs non-metro counties!"
  },
  {
    "objectID": "10data_viz_2.html",
    "href": "10data_viz_2.html",
    "title": "Data visualization part 2",
    "section": "",
    "text": "We know how to make various plots to display the data and how to control the attributes and scales. Now we’ll move on to additional layers in ggplot2\n\n\nThere are situations in which there is too much information to put it in a single plot.\n\n\n\n\nhow to represent statistics on a plot\nPre-calculating values\nstat_summary\n\n\n\n\nThis layer controls how the coordinates of the plot should be handled. Most likely you are used to plots with the cartesian space: an x and y axis that are perpendicular. However you can change that e.g. by fising the ratio of x to y axis, zooming in on a particualr part of the plot or even bending the axis altogether.\n\n\n\nThe final layer, called theme, controlls all the non-data part of the plot: setting fonts, typeface, text size, controlling the background color, plot legend and the grid.\nThere is a number of pre-specified themes that you can use but you can also control everything manually. You can check the documentation of theme() to see just how many elements you can control.\n\nsome example themes\ncontrolling theme yourself\n\n\n\nA note on working with fonts: if you want to work with more fonts than the built-in ones you will have to load them into R. This can sometimes prove quite problematic.\n\nshowtext package"
  },
  {
    "objectID": "11organizingwork.html",
    "href": "11organizingwork.html",
    "title": "Organizing your work",
    "section": "",
    "text": "Organizing your work:"
  },
  {
    "objectID": "11organizingwork.html#working-directories",
    "href": "11organizingwork.html#working-directories",
    "title": "Organizing your work",
    "section": "Working directories",
    "text": "Working directories"
  },
  {
    "objectID": "11organizingwork.html#environments",
    "href": "11organizingwork.html#environments",
    "title": "Organizing your work",
    "section": "Environments",
    "text": "Environments"
  },
  {
    "objectID": "11organizingwork.html#projects",
    "href": "11organizingwork.html#projects",
    "title": "Organizing your work",
    "section": "Projects",
    "text": "Projects\n-what is a project\n-why is a project useful"
  },
  {
    "objectID": "11organizingwork.html#renv",
    "href": "11organizingwork.html#renv",
    "title": "Organizing your work",
    "section": "Renv",
    "text": "Renv\n-managing package versions\n-groundhog package"
  },
  {
    "objectID": "11organizingwork.html#rprofile",
    "href": "11organizingwork.html#rprofile",
    "title": "Organizing your work",
    "section": ".Rprofile",
    "text": ".Rprofile\n-what these are and why they might be useful"
  },
  {
    "objectID": "12eda.html",
    "href": "12eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Some introduction on moving on to section about data analysis.\nExploring and cleaning data is often a necessary and fairly long process\nWhy even bother with it?\nIt is a necessary steps because there is a near-infinite number of things that can go wrong when preparing the data from errors in how data was coded to errors in preprocessing or loading the files. Lots of things can also happen that can make analysis or drawing conclusions difficult (like how reliable are your scales, are the items recoded correctly etc.)"
  },
  {
    "objectID": "12eda.html#getting-the-basic-information-about-a-dataset",
    "href": "12eda.html#getting-the-basic-information-about-a-dataset",
    "title": "Exploratory Data Analysis",
    "section": "getting the basic information about a dataset",
    "text": "getting the basic information about a dataset\n\nif you have it, start with any documentation on the dataset (like codebooks etc.)\nGetting basic info on the data: glimpse etc.\nDescribe from psych package\nGetting information on categorical variables"
  },
  {
    "objectID": "12eda.html#exploring-via-plots",
    "href": "12eda.html#exploring-via-plots",
    "title": "Exploratory Data Analysis",
    "section": "Exploring via plots",
    "text": "Exploring via plots\nWhy plotting data is always important\nA cautionary tale the boring way: Anscombe quartet\nThe fun way: Gorilla in the data\nThe key takeaway is that a lot of things that are wrong or at least problematic can be immediately spotted when you plot the data. e.g. in when comparing two conditions of an experiment you might spot that the whole effect is driven but just a few outliers."
  },
  {
    "objectID": "12eda.html#reliabilities",
    "href": "12eda.html#reliabilities",
    "title": "Exploratory Data Analysis",
    "section": "reliabilities",
    "text": "reliabilities\n-getting reliability analysis with alpha()"
  },
  {
    "objectID": "14anova.html",
    "href": "14anova.html",
    "title": "Anova",
    "section": "",
    "text": "-many groups, many means\n\n\n-how to make a one-way anova.\n\n\n-What these are and how to make them\n\n\n\n-How these are different from post hocs\n-How to make them\n\n\n\n\n\n\n\n-how to interpret the results\n-plotting results\n-effect size and power"
  },
  {
    "objectID": "15correlations.html",
    "href": "15correlations.html",
    "title": "correlations",
    "section": "",
    "text": "-what are those"
  },
  {
    "objectID": "16regression.html",
    "href": "16regression.html",
    "title": "Regression",
    "section": "",
    "text": "What type of problems are we talking about?\n-What is a regression analysis?\n\n\nRegression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest.\nHousing market example: imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on.\n\n\n\nThe very idea of making a regression analysis is ultimately an optimization problem. We want to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones.\nMaking a guess:\nSo, we know we need to optimize our prediction - we want to make the best guess possible. But what does it mean ‘the best guess possible’? We need some rule on what it would mean to make a best guess. Since we are predicting a continuous variable we can calculate how much we miss for every prediction (subtract the actual value from the predicted value) and then choose the one that has the smallest error. This is already a start but we can miss in two ways: we can predict too little or too much. The first one is going to produce a negative error and the other one a positive one. Negative numbers are smaller than positive ones so our rule so far will always favor predicting too little! Fortunately there is a very easy way to deal with this - we can square the errors (there are reasons why squaring is preferred to taking absolute values but they are beyond the scope of this course). Now, we can sum all the squared errors for each observation in our dataset. The prediction that gets the smallest sum of squared errors wins! This is pretty much how the Ordinary Least Squares (OLS) regression works.\nLets start with the simplest example possible. Lets say we have no other information except our variable that we want to predict. Which value will minimize the sum of squared errors? There’s actually a fancy formula for this because OLS regression has a closed form but perhaps a better way to learn this is to actually make a bunch of guesses and see what happens. This is what simulation is perfect for.\nLets first simulate a bunch of observations from a normal distribution: our dependent variable. We will keep mean = 0 and standard deviation = 1.\n\ny <- rnorm(1000)\n\nNext, we’ll make a bunch of guesses and see how each of them performs (what is their sum of squared errors)\n\nsse <- c() #initialize vector to store sum of squared errors\npred <- seq(-1,1,length.out = 10) #we'll make 10 guesses from -1 to 1, equally spaced\nfor (i in pred) {\n  res <- sum((i - y)^2) # for each guess calculate sum of squared errors\n  sse <- c(sse, res) #append the sum to the vector\n}\npred_sse <-data.frame(pred,sse) #put these \n\nYou can go ahead and inspect the sse and pred. Can you see for which guess we get the smallest error? You can also look at the animation below:\n\n\n\npredictions and sums of squared errors\n\n\nYou can see that the points fall on a really nice parabola. For a guess of mean - 1 standard deviation we get a really big sum of squared errors, they gradually get smaller and smaller and then start to get bigger up to mean + 1 standard deviation. The lowest point of the parabola is at the mean and that is in fact our best guess. When making regression analysis we will be working with means all the time. We’ll just be adding more information to the model (e.g. if we add belonging to experimental vs control group into the model, then our best guess will be the mean in each of those groups and we get a two sample t-test).\nA more general way which we can use to think about linear models is that we are modelling Y as following a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). The mean is then determined by the variables we put into the model. If we don’t add anything \\(\\mu\\) is going to be the actual mean in our sample. If we start adding variables into the model then \\(\\mu_i\\) will be determined by them (that’s why we have the i).\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma)\n\\\\\n\\mu_i =\\alpha\n\\]\n\n\n\n-how to make a regression analysis\n-how to look at the results\n\n\n\n-how to make it\n-Why you should always think first: putting things into regression"
  },
  {
    "objectID": "13t-tests.html",
    "href": "13t-tests.html",
    "title": "t tests",
    "section": "",
    "text": "-The whole idea behind the t-tests.\nThe logic behind a t test."
  },
  {
    "objectID": "03data_types.html#vectors",
    "href": "03data_types.html#vectors",
    "title": "Types of data",
    "section": "Vectors",
    "text": "Vectors\nThe most basic type of data is a vector. Vectors can store any number of values of the same type in 1 dimension. You can create a vector using c() function.\n\nmy_very_first_vector <- c(1,2,3)\nmy_very_first_vector\n\n[1] 1 2 3\n\n\nVectors are indexed: they have a first, second value etc. This means that you can access part of a vector -subset them. Subsetting is accomplished with []. You can also subset a range of values from a vector wirh [:].\n\nlong_vector <- c(1,2,3,4,5,6,7,8,9,10)\nlong_vector[3:5]\n\n[1] 3 4 5\n\n\nIf you try to put different types of values into one vector R will convert the types to a matching one. This is especially important when due to some mistake/error a single value of a different type gets lost in some other variable. Just a single value will trigger the whole variable to be converted.\n\nmy_vector <- c(1, TRUE, 'some text')\nmy_vector\n\n[1] \"1\"         \"TRUE\"      \"some text\"\n\nclass(my_vector)\n\n[1] \"character\"\n\n\nYou can make pretty much the same operations on vectors as on single values. One of the great features of R is that by default it will make operations element wise - if you try to add two vectors together then the first element from vector 1 will be added to first element of vector 2 and so on (the fancy name for this is vectorization).\n\nnumbers <- c(1,2,3,4,5)\nnumbers2 <- c(6,7,8,9,10)\nnumbers + numbers2\n\n[1]  7  9 11 13 15\n\n\nIf the vectors have different lengths then R will start to recycle values from the shorter vectors. But it will output a warning if the length of one vector is not a multiple of the other vector.\n\nshort_v <- c(1,2,3)\nlong_v <- c(1,2,3,4,5)\nshort_v + long_v\n\nWarning in short_v + long_v: długość dłuszego obiektu nie jest wielokrotnością\ndługości krótszego obiektu\n\n\n[1] 2 4 6 5 7"
  },
  {
    "objectID": "03data_types.html#factors",
    "href": "03data_types.html#factors",
    "title": "Types of data",
    "section": "Factors",
    "text": "Factors\nFactors are much like vectors except that they are used for storing categorical values - they have levels. You can store variables such as country or experimental condition of participants in a factor. You can create a factor by calling factor() and passing it a vector as an argument.\n\nmy_vector <- c('a', 'b', 'a', 'b')\nmy_factor <- factor(my_vector)\nmy_factor\n\n[1] a b a b\nLevels: a b\n\n\nFactors can also have ordered levels. You can make an ordered factor by setting ordered = T argument when creating a factor. Notice how the output looks different now: it includes information on the order.\n\nordered_vec <- c('low', 'high', 'high', 'low', 'low')\nordered_fac <- factor(ordered_vec, ordered = T)\nordered_fac\n\n[1] low  high high low  low \nLevels: high < low\n\n\nYou can also manually set the levels of a factor. You can do it when creating the factor. Notice that for ordered factor the order in which you pass the levels will determine the order of levels in the factor.\n\nordered_vec <- c('low', 'high', 'high', 'low', 'low')\nordered_fac <- factor(ordered_vec, ordered = T, levels = c('low', 'high', 'medium'))\nordered_fac\n\n[1] low  high high low  low \nLevels: low < high < medium"
  },
  {
    "objectID": "03data_types.html#matrices",
    "href": "03data_types.html#matrices",
    "title": "Types of data",
    "section": "Matrices",
    "text": "Matrices\nMatrices are a bit like vectors but they have two dimensions. They have rows and columns but treat them in the same way. Because of this they can store only one type of values (much like vectors). You can create a matrix from scratch with the matrix() function. This function takes a vectors of values as its input (these are the values we will fill our matrix with) and additional information on how the matrix has to look - how many columns and rows it should have and whether to fill the matrix with values by rows or columns\n\nnumbered_vector <- c(1,2,3,4,5,6,7,8,9)\nmy_matrix <- matrix(numbered_vector, nrow = 3, ncol = 3)\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nYou can also create a matrix by ‘glueing’ vectors together. You can bind them either as rows (rbind() function) or by columns (cbind() function). Notice that the names of the vectors will be used either as names of rows or columns.\n\nvec1 <- c(1,2,3)\nvec2 <- c(4,5,6)\ncbind(vec1, vec2)\n\n     vec1 vec2\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nSince we have two dimensions subsetting matrices can work both on rows and columns. The general idea is still the same but we have to specify whether we are subsetting rows or columns. Rows always come first, columns second separated by a comma like this matrix[rows,columns]. You can select ranges of rows or columns just like in a vector.\n\nmy_matrix[2,2]\n\n[1] 5\n\n\nIf you want to select all rows or columns you can leave the space blank. Remember to keep the comma though!\n\nmy_matrix[,3]\n\n[1] 7 8 9"
  },
  {
    "objectID": "03data_types.html#data-frames",
    "href": "03data_types.html#data-frames",
    "title": "Types of data",
    "section": "Data frames",
    "text": "Data frames\nIn a day to day analysis you will likely work with data frames most of the time. A data frame is like a matrix in that it has rows and columns but can store different types of values in each column (so that e.g. you can have some variables that are numeric and others that are text). A different way of thinking about data frames is as a list of vectors of the same length with each vector representing a different variable. Each row represents a different observation (e.g. participant).\nYou can create a data frame with data.frame() function passing all the variables as arguments. Lets create 3 vectors: author, title and year.\n\nauthor <- c('Allport', 'Heider', 'Lewin', 'Allport', 'Heider')\ntitle <- c('Nature of Prejudice', 'Psychology of interpersonal relations',\n           'Principles of Topological Psychology', 'Psychology of Rumor', 'The life of a psychologist: An autobiography')\nyear <- c(1954, 1958, 1936, 1947, 1983)\n\npsych_books <- data.frame(author, title, year)\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n\n\nSubsetting data frames works the same way as matrices. You can subset both on rows and columns. An important thing to remember (and one of the reasons a lot of people switch to tibbles which are kind of data frames+. We’ll get to tibbles some time in the future) is that if you subset a single column the result will be a vector and not a dataframe. This sometimes is annoying if you are designing something that is supposed to work on data frames specifically.\nThere is one additional way of subsetting a data frame. Subsetting variables based on their position is tiresome because we rarely remember the order of all the columns (especially as our data frames get bigger). You can select a single variable using $:\n\npsych_books$author\n\n[1] \"Allport\" \"Heider\"  \"Lewin\"   \"Allport\" \"Heider\" \n\n\nUsing the $ operator reflects a way of thinking about datasets that is pretty common: data frames are ordered collections of variables and each variable has its name. You can also use $ to create new variables. Just assign a vector of values to a new name in your dataframe:\n\npsych_books$discipline <- c('intergroup relations', 'social psychology',\n                            'general psychology', 'social psychology',\n                            'biography')\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n            discipline\n1 intergroup relations\n2    social psychology\n3   general psychology\n4    social psychology\n5            biography\n\n\nRemember how we talked about element-wise operations on vectors? You can leverage it to easily create new variables that are results of operations on other variables. Thanks to this you can add a whole new variable that is a result of a mathematical operation in just one line (imagine adding a variable that is a sum of all points from a quiz for each student). Here’s an example if we wanted to calculate how many years ago each book from our data frame was published:\n\npsych_books$book_age <- 2022 - psych_books$year\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n            discipline book_age\n1 intergroup relations       68\n2    social psychology       64\n3   general psychology       86\n4    social psychology       75\n5            biography       39"
  },
  {
    "objectID": "03data_types.html#lists",
    "href": "03data_types.html#lists",
    "title": "Types of data",
    "section": "Lists",
    "text": "Lists\nLists are the final type of basic data in R we will discuss here. They are the most versatile ones - they can store anything inside of them: single values, vectors, matrices, dataframes or even other lists! One important feature of lists is that they are ordered: you can access their elements by position. So you can think of lists as collections of objects but there aren’t really limits to what these objects are (in fact dataframes are very specific lists: they are collections of variables that have the same length and form a nice rectangular table). Lists are created with list(). Lets create a list of the plants in a house along with a value storing information on how many days ago did we last water them:\n\nlist_of_objects <- list(\n  plants = c('Calathea', 'Chamedora', 'Pilea', 'Philodendron'),\n  days_since_watering = 5\n)\nlist_of_objects\n\n$plants\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\n$days_since_watering\n[1] 5\n\n\nYou might encounter lists if you need to store a number of different things together. E.g. results of many statistical analyses are stored in lists. In fact as you dive deeper into R you will start to encounter more and more lists because they are very versatile.\nYou can access objects stored in lists in a few ways. You can use the [] you used for all other types of data. An important feature of this type of subsetting is that the result will always be a list (even if it has only 1 element). The other option is to use double square brackets [[]]. This will extract the object inside a list so the result won’t be a list (you can think of it as ‘getting deeper’ into the list to extract the exact element you want).\n\nlist_of_objects[1]\n\n$plants\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\nlist_of_objects[[1]]\n\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\n\nIf the elements in your list are named you can also use $ to extract them.\\\n\nlist_of_objects$days_since_watering\n\n[1] 5"
  },
  {
    "objectID": "05functions.html#how-is-a-function-built",
    "href": "05functions.html#how-is-a-function-built",
    "title": "Functions",
    "section": "",
    "text": "The first approximation to how a function is built is to think of it as a kind of machine. The machine takes some inputs, processes them in some way and returns outputs. The inputs are the arguments you provide to a function like a vector or a dataset. The result is the output. Very often the insides of a function, the machinery within it that is responsible for getting from the input to the output is a black box to us. We have no clue how exactly a function arrives at its result. Sometimes we don’t need to know it but in many situations at least some knowledge is necessary to be certain that the function does exactly what we need it to do and won’t surprise us (an annoying example we will get to later is silent dropping of missing values by some functions)."
  },
  {
    "objectID": "05functions.html#types-of-arguments",
    "href": "05functions.html#types-of-arguments",
    "title": "Functions",
    "section": "",
    "text": "Lets focus on the inputs. There are a few kinds of them. The most basic ones are input arguments - this is what you put into the machine. Apart from it there are a few other types of arguments that can allow you to have more control over the behavior of functions. They are a bit like toggles and switches on a machine that change how it operates.\nDefault arguments: arguments that are set to some default value. This value will be used unless specified otherwise. An example is the na.rm argument from mean or sum. This argument is set to FALSE by default so that the function will return an error if there are any missing values in the input argument. This is such a good example because it also stresses why choosing proper defaults is really important when writing functions. A lot of people, when they first encounter functions like mean or sum, are surprised or even annoyed. Why in the world set defaults that are more likely to produce errors? We are often fixated on avoiding errors in code but this is not always the way to go in data analysis. We often want functions to operate smoothly and seamlessly. But that is false peace. Smooth behavior is not always what we need from functions. Clunky functions are often good in data analysis because they force us to be explicit with what we do with data. Even if they make climbing the hill a bit more steep they are sure to lead us on the right path to the top.\nYou can also encounter alternative arguments. These arguments have a prespecified set of possible values (usually defined as a vector).= For example the table() function that can give us a frequency table of a factor has a useNA argument that specifies whether to use NA values.It can take three different valus that specify possible behaviour of the function. You can read more onwhat they do inthe documentation of the function.\nThe final type of argument is the … argument. It is a placeholder for any number and kind of arguments that will later on be passed inside the function usually as arguments in some internal function. Take lapply as an example. Apart from the argument X and FUN which specify what to loop over and what function to apply to each element of X it also has the … argument. It’s there because the function you want to apply to every element of X might take some additional arguments. How many and what kind of arguments these are might vary from function to function and the … argument allows us to handle this. Any arguments passed in the … will be used as argument of the function specified in the FUN argument of lapply."
  },
  {
    "objectID": "05functions.html#building-your-own-functions",
    "href": "05functions.html#building-your-own-functions",
    "title": "Functions",
    "section": "",
    "text": "Why spend time building your own functions? There are a few general cases. The first and probably most obvious one is when there is no available function that would do what you need. For example there is no available function to find a mode of a vector in R. If you want to find it you need to build your own function. Finding a mode is a simple example but there may be cases where you need to do something more complex or customize the behavior of an already existing function. Second reason is to avoid repetition. If you do similar operations a number of times (e.g. only the dataset or vriables change but all the rest stays the same) then copying and pasting code will soon become problematic. It makes code less readable, longer and more difficult to manage. Imagine you need to change one thing in that code. You’ll need to change it in every place where it was pasted. Writing a function instead means you can just change how you define the function.\nThe general logic for defining a function is as follows:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nTurning a chunk of code into a function can be done quite easily in a few steps. Imagine we want to see what is the probability that a random number drawn from one vector will be larger than the mean of another vector (we will simulate a few variables with rnorm() which draws random numbers from a normal distribution with a given mean and standard deviation):\n\n#simulate vectors\nvar1 &lt;-rnorm(100, 0, 1)\nvar2 &lt;- rnorm(100, 1, 3)\nvar3 &lt;- rnorm(100, .5, 2)\nvar4 &lt;- rnorm(100, 0, 3)\nvar5 &lt;- rnorm(100, 0.1, .2)\n\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n\n[1] 0.57\n\n\n\nBuild the scaffolding of the function. This is exactly what is in the code chunk above:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nPaste the code you want to turn into a function\n\nmy_function &lt;- function(arguments) {\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\nIdentify all the “moving parts”: What will change? Each of these things has to get its own argument (all the moving parts are marked on the right side of the code chunk):\n\nmy_function &lt;- function(arguments) {\n#calculate mean\n1mean_1 &lt;- mean(var1)\n\n#calculate lenght\n2length_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\n3n_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n1\n\nvar_1\n\n2\n\nvar_2\n\n3\n\nvar_2\n\n\n\n\nChange each of the “moving parts” in the code chunk into appropriate argument\n\nmy_function &lt;- function(x, y) {\n#calculate mean\nmean_1 &lt;- mean(y)\n\n#calculate lenght\nlength_2 &lt;- length(x)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(x[x &gt; mean_2])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n\n\nWhen building your own functions, especially if they are going to be used by other people, it’s a good idea to consider potential weird things that could happen. When first creating a function we usually have its typical behaviour in mind because we just want our function to work. However, there’s a whole bunch of weird stuff that might happen if you don’t prepare for it in advance. For example, imagine you want to create a function from scratch that will output the mean of a numeric vector. You could try to do something like a for loop (yes this is slow and inefficient but it’s just for the purpose of demonstration):\n\nmy_mean &lt;- function(x) {\n  sum &lt;- 0\n  for(i in x){\n    sum &lt;- sum + i\n  }\n  result &lt;- sum/length(x)\n  return(result)\n}\n\nPretty straightforward right? Now lets see our function in action on some typical use case and compare its results to the built-in mean() function:\n\nv &lt;- c(1,2,3,4,5,6,7,8,9)\n\nmy_mean(v)\n\n[1] 5\n\nmean(v)\n\n[1] 5\n\n\nYay, we get the same result! Seems like our function works! But before we call it a day and start using our own mean function lets see some less typical cases. E.g. What will happen if the vector has some missing values? Or if its an empty vector? Or if it is not a numeric vector? Lets see:\n\nv_na &lt;- c(1,2,3,NA,5)\nv_empty &lt;- c()\nv_char &lt;- c(\"A\", \"B\", \"C\")\n\nmy_mean(v_na)\n\n[1] NA\n\nmy_mean(v_empty)\n\n[1] NaN\n\nmy_mean(v_char)\n\nError in sum + i: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWe get some weird behaviour. Each of these calls to my_mean() function returned something different. First we got NA when the vector had NAs in it. Passing an empty vector resulted in NaN - short for Not a Number. Finally, passing a character vector gave us an error.. Notice that only the last case gave us an error so if we then implemented our functions in some calculations we might not even notice something is wrong - for example imagine we calculated a mean with our function from a vector with missiing values (e.g we asked a bunch of participants about their mood 5 times a day and now we want to calculate daily average mood and see how it relates to some variables of interest) and then tried to use its output in some other function that had na.rm argument set to TRUE. We’d lose a bunch of information without so much as a warning! That`s why considering possible but unusual cases for a new function is important. It allows us to prepare for possible future problems."
  },
  {
    "objectID": "05functions.html#anonymous-functions",
    "href": "05functions.html#anonymous-functions",
    "title": "Functions",
    "section": "",
    "text": "There are situations in which you might want to use a custom function but not necessarily save it with a name for future use. In such situations we often use what is called an anonymous function (sometimes you can also encounter the term lambda functions). The general way these functions are constructed in R is as follows:\n\n(function(x) WHAT THE FUCNTION DOES)(arguments)\n\nA pretty common situation where you can also encounter these functions is inside iterations like with apply:\n\nv1 &lt;- c(1,2,3,4)\nv2 &lt;- c(3,4,5,6,8)\nv3 &lt;- c(-1,4,3,2)\nv_list &lt;- list(v1,v2,v3)\n\nlapply(v_list, function(x) {x+x})\n\n[[1]]\n[1] 2 4 6 8\n\n[[2]]\n[1]  6  8 10 12 16\n\n[[3]]\n[1] -2  8  6  4"
  },
  {
    "objectID": "05functions.html#documentation",
    "href": "05functions.html#documentation",
    "title": "Functions",
    "section": "",
    "text": "Ok, so we wrote our super cool new function. We tested it and are pretty confident it works properly. Can we finally call it a day? Again, not so fast. We need one more thing. Imagine you take a long holiday and get back towork after a month or two, How confident are you you will remember how exactly our new function works? Or imagine you share the function you ccreated with other people .Of course you (or others) can read the code of the function to learn that again but that is tedious. That`s why it’s so important to document code. This goes for functions but is just as true for any code that will be used by others or you in the future. Treat yourself in the future like you would treat another person. Documentation is super important! For a lot of people writing (or reading!) documentation is seen as tedious and redundant task. I guarantee you that if you don’t document your own functions you will regret this sooner or later (probably sooner). Very few functions are self explanatory enough to not need any form of documentation. In many cases simply using comments in code with # will be enough. Sometimes building vignettes that shows how to use some functions can be a better idea. Remember to always leave some description of what given code is about and what it does."
  },
  {
    "objectID": "05functions.html#functions-from-packages",
    "href": "05functions.html#functions-from-packages",
    "title": "Functions",
    "section": "",
    "text": "Since R has a huge community people are constantly developing new things you can do in R. You don’t have to define everything from scratch. Usually if you need a function for some statistical procedure or e.g. for plotting some package out there already has it. There is no need to reinvent the wheel.\nIn order to use functions from packages you need to first install the package on your computer. You can do it by calling install.packages(\"PACKAGENAME\") functon. You need to do it only once on a given device (unless you want to update the package or you are using renv but more on that later). Once the package has been installed you can load it in a given R session by calling library(PACKAGENAME). Remember you need to run it every time you open a new R session. Alternatively, after installing a package you can call a function from it directly without loading the package first by using PACKAGENAME::function_name().\nAnother thing to know about functions from packages is name conflict. Since R is open source and most of the packages are developed and maintained by the community it is not so uncommon that two different packages have a function with the same name. You might wonder what will happen if you load both packages and then call this function? Generally, the last package loaded is going to mask previous packages. However this can be problematic e.g. if you are sharing scripts (and someone changes the order of loading packages) or if you actually want to use the function from the first package.\nThere are at least two ways of dealing with this problem. The first one is to be explicit. Above we described a second way of calling a function from a package: PACKAGENAME::function_name(). This way you explicitly state which package the function is from so you shield yourself from name conflict. The second way is by specifying additional arguments to the library() function. If you look up its documentation you can see that it has two optional arguments: exlcude and include.only. They allow you to load a package without some function or to load only some functions from a package. This is useful in situations where you want to load 2 packages with conflicting functions but you know you want to use the conflicting function from only one of them."
  },
  {
    "objectID": "05functions.html#exercises",
    "href": "05functions.html#exercises",
    "title": "Functions",
    "section": "",
    "text": "Remember the for loop that generated n first numbers from Fibonacci sequence from the class on loops? Now turn it into a function that will return from ith to jth Fibonacci number. Document the function properly so it is clear what it does\nCreate a function that calculates a mode of a vector. Consider potential edge cases and provide tests that show your function behaves properly"
  },
  {
    "objectID": "04loops_conditionals.html#conditional-statements",
    "href": "04loops_conditionals.html#conditional-statements",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Another way conditional statements are referred to which may be more intuitive are if else statements. They allow you to tell R to execute given chunk of code if a condition is met and to do something else if the condition is not met.\nThe general logic of conditional statements looks like this:\n\nif (condition) {\n  Do this\n  And do this\n}\n\nA single if statement can have multiple conditions chained together with | and & operators. So, for example\n\nx &lt;- 5\ny &lt;- -5\n\nif (x &gt; 0 & y &lt; 0) {\n  print(\"Hooray!\")\n}\n\n[1] \"Hooray!\"\n\n\nIn many situations you want to state what is to be done if a condition is met and what to do otherwise. This turns your statement into an if else one. The only difference is that after the if statement you add else and specify what to do then in curly brackets. With this knowledge you can already create the rules for a simple game like paper, rock, scissors!\n\n#set the choice for each player\nplayer1 &lt;- 'scissors'\nplayer2 &lt;- 'rock'\n\n#define an if statement that outputs the result of the game\nif (player1 == player2) {\n  print('draw')\n} else if ((player1 == 'scissors' & player2 == 'paper') |\n           (player1 == 'paper' & player2 == 'rock') |\n           (player1 == 'rock' & player2 == 'scissors')) {\n  print('player 1 wins')\n} else if ((player2 == 'scissors' & player1 == 'paper') |\n           (player2 == 'paper' & player1 == 'rock') |\n           (player2 == 'rock' & player1 == 'scissors')) {\n  print('player 2 wins')\n} else {\n  print('these are not allowed moves')\n}\n\n[1] \"player 2 wins\"\n\n\nTake a moment to study the code above. Notice what kinds of conditions are included in that statement. When writing an if statement it’s a good idea to consider all possible situations and how your if statement maps to them. In a paper, rock, scissors game you can have 3 outcomes: both players choose the same option (a draw), player 1 wins or player 2 wins. Notice that the code above includes also a fourth options specified in the last else statement. What if someone makes a typo and writes rook instead of rock? That last else statement safeguards us for such situations. If we didn’t include it and someone made a type then our if else statement wouldn’t produce anything. You can play around with different values of player1 and player2 to see the results.\nOne more thing about if else statements: in many situations it is a good idea to give some thought to what exactly a given statement is supposed to do and how large the statement needs to be. A good example is an if statement that is supposed to run some check (e.g. make sure that we are working with a numeric value) and stop execution if it detects a problem. Imagine a situation in which we want to do some calculations on numbers and want to make sure that we are indeed working with numeric values. you could design an if else statement that would do it:\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n} else {\n  x + y\n}\n\nError in eval(expr, envir, enclos): This is not a number!\n\n\nTake a moment to look at the code above. Do you think it is good? It certainly gets the job done. Do you think it could be simplified?\nIn fact the else part is redundant in this case. The if statement runs the check on x and y and stops execution of the code if any of them is not numeric. If both values are numeric the execution of code simply proceeds. In this case adding an else statement makes the code harder to read (and imagine what would happen if we had to perform a number of checks like this! We would need a lot of if else statements that would make everything even less clear). The code below does the same thing as the if else statement above but is more clear.\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n}\n## Error in eval(expr, envir, enclos): This is not a number!\nx + y\n## Error in x + y: argument nieliczbowy przekazany do operatora dwuargumentowego"
  },
  {
    "objectID": "04loops_conditionals.html#loops",
    "href": "04loops_conditionals.html#loops",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Another way of controlling the flow of your code is by repeating a given chunk of code. There are two basic ways to do that: repeat something a number of times or keep repeating until some condition is met. The first way is called a for loop and the second one a while loop.\n\n\nBefore we make our first for loop lets take a moment to see when a for loop is not needed. Recall again that a lot of things in R are vectorized. This means that operations on vectors are conducted element-wise. Thanks to this if you want to e.g. add 5 to each value stored in a numeric vector (in the language of a for loop: for every element of a vector, add 5 to it) you can just write vector_name + 5. No need for more complicated, explicit repetition. However, there are situations in which you have to make an explicit for loop to repeat something n times. The general structure of a for loops looks like this:\n\nfor (i in object) {\n  Do this to each element\n}\n\nIt’s worth keeping in mind what the i in the for loop is. In the example above i will be every consecutive element of object. However we could do a similar thing with:\n\nfor (i in 1:length(object)) {\n  do this to object[i]\n}\n\nNow each i is a number from 1 to thew length of object and we access each element of object by using a proper (ith) index. Which way of running a for loop you choose might depend on the context. looping explicitly over elements of an object rather than indexes can be more intuitive but imagine you don’t want to do something to every element of an object but only to to a subset (e.g. from 3rd inwards). Doing it with indexes is easier. Generally the best approach is to think what you need first and write code code second, not the other way around.\n\nlibrary(microbenchmark)\n\nWarning: pakiet 'microbenchmark' został zbudowany w wersji R 4.3.1\n\n\n\n\n\nWhile loops will keep executing a given chunk of code as long as some condition is met. They aren’t very common in R, at least not until you start building your own algorithms or simulations from scratch. However, it’s worth knowing what they are in case you encounter them.\nWe can use a while loop to make a very simple simulation. Lets say we want to see how temperatures change from a given temperature (lets say 20 degrees Celsius) across time and that we represent time by some random change from each previous temperature. We can create a vector with such predicted temperatures and see how long it takes for it to reach a certain level (lets say 30 degrees Celsius). We represent the change by adding a random value from a normal distribution with mean = .05 and standard deviation = .5 (this is what the rnorm(1,.05,.5) does). The while loop would look something like this: We first create the initial value and a vector to store all temperatures and next we keep adding the random value to our temperature and storing all temperatures until it reaches 30. The last line tells R to plot all the temperatures as a line plot. This is of course a very, very, very simplistic simulation (temperatures don’t change in such a simple way) but it works to show you the idea behind while loops. We can then calculate e.g. how long it took for the temperature to reach a certain level.\n\nC &lt;- 20\nresults &lt;- c(20)\nwhile (C &lt; 30) {\n  C &lt;- C + rnorm(1,.05,.5)\n  results &lt;- c(results, C)\n}\n\nplot(results, type = 'line', lwd = 2, col=4, xlab = \"days\", ylab = \"temperature\")\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\n\n\n\n\nBecause while loops do not have a fixed number of iteration they can potentially run infinitely. This is usually not something we want so it’s a good idea to make sure that your while loop eventually stops. In case you do get stuck in an infinite loop you can press Esc in your console and this should make RStudio stop the loop by force.\nTruth is while loops are not common in R. You will rarely find yourself in situation where you need to perform some actions while a given condition is true (e.g. keep a program running until a user presses exit; keep displaying a board of a game until a player makes a move). However, it’s still good to know what while loops are so that you will know one when you see it."
  },
  {
    "objectID": "04loops_conditionals.html#apply-family",
    "href": "04loops_conditionals.html#apply-family",
    "title": "Loops and conditionals",
    "section": "",
    "text": "There is a special family of functions in R that makes working with for loops a bit easier. These functions let you specify what to loop over and what function to apply to each element but in a function rather than a whole loop with all the curly brackets and stuff.\nThe reason why this is a whole family of functions is that you can iterate in various ways and you can get the output in different formats. There are more functions in the family but the general ones are:\n\nlapply() - loops over elements of a vector/list and returns a list\nsapply() - same as lapply but tries to simplify the result to a vector or matrix\napply() - used for looping over 2 dimensional structures - it lets you specify if you want to loop over rows or columns\ntapply() - same as apply but lets you split the object you are looping over based on some factor (e.g. imagine you want to calculate the mean value of your dependent variable for each experimental condition).\n\nLets see some of these in action.\n\n\nImagine you are working with a list in R. You want to get information on how many elements each object in the list has. sapply makes it very easy:\n\nmy_list &lt;- list(\n  1:50,\n  sample(300, 5),\n  c(\"random\", \"vector\")\n)\n\nsapply(my_list, length)\n\n[1] 50  5  2\n\n\n\n\n\nThere is a dataset available in R on airquality in New York City called airquality. It stores information on ozone, sun, wind and temperature from 5 months One of the things that might be of interest when looking at the dataset is what was the average value of each of the variables informing on airquality:\n\ndata(\"airquality\")\nd &lt;- airquality\nd &lt;- na.omit(d)\napply(d[,1:4], 2, mean)\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\nNotice that the means calculated above are global means from the entire dataset. What is probably much more sensible is a mean for each month. There is one additional trick needed here. Tapply won’t allow us to split a number of columns by some vector and perform a given operation on each of the columns. That’s because tapply works on vectors. In order to get monthly means for all 4 columns we need to combine apply with tapply. What we need to do is start with apply and loop over the 4 columns of interest and for each of them use tapply that will split a given column by month and calculate the means. Combining functions can get us really far if only we give some thought to what each function does (including what are its inputs and outputs) and what we really need to do.\n\napply(d[,1:4], 2, function(x) tapply(x, d$Month, mean))\n\n     Ozone  Solar.R      Wind     Temp\n5 24.12500 182.0417 11.504167 66.45833\n6 29.44444 184.2222 12.177778 78.22222\n7 59.11538 216.4231  8.523077 83.88462\n8 60.00000 173.0870  8.860870 83.69565\n9 31.44828 168.2069 10.075862 76.89655\n\n\nOne important thing about apply functions is that they are generally faster than explicit for loops. Same thing goes for vectorized code as well - it’s faster than a for loop. We can make a simple comparison by using the microbenchmark package to make the same thing with a for loop and apply() function. We’ll save it as bench\nNow if we look at the results:\n\nbench\n\nUnit: milliseconds\n                                               expr    min      lq     mean\n for (i in 1:ncol(df)) {     print(mean(df[, i])) } 3.5722 4.45345 6.208726\n                                 apply(df, 2, mean) 4.4559 5.71735 9.050546\n  median       uq     max neval cld\n 5.76020  7.35190 16.2620   100  a \n 7.30355 10.58615 31.9904   100   b"
  },
  {
    "objectID": "04loops_conditionals.html#exercises",
    "href": "04loops_conditionals.html#exercises",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Try to code the logic of assigning points to players of a prisoner dilemma with a given matrix:\n\nCreate a for loop that will print out the first 50 numbers from the Fibonacci sequence\nGiven the iris dataframe (you can load it with data(\"iris\") loop over all of its columns and calculate the mean of every numeric column"
  },
  {
    "objectID": "06loading_data.html#loading-data-in-r",
    "href": "06loading_data.html#loading-data-in-r",
    "title": "loading_data",
    "section": "",
    "text": "So far we’ve been creating the object to work with ourselves. Usually you will work with already existing datasets though (like a file from an experiment, survey etc.). The first thing you need to do to start working on your file is to load it into R session. There are many ways of loading a dataset or file into your R session so that you can use it. How to do it depends mainly on how the data is stored.\nWhen loading a dataset there are generally a few things to consider:\n\nExtension: data can be stored in different ways. This can also be related to what kind of information is stored in a given file. Depending on how a file is stored you might need to load it differently because it stores different information. For example excel files can have multiple sheets while spss files can store labels attached to variables. When loading a file you always have to provide files with extensions.\nfile path: files are stored in different places on your computer (or on the web). In order to load it you need to tell R exactly where to look for the file. A file path speifies exactly where exactly a file is stored e.g. C:/Users/User/Documents specifies the path to the documents folder. A file path can be absolute like the one shown above - its unequivocal. There are also relative file paths that specifies the location of a file relative to current directory. By default R will look in what is called working directory. For simple scripts this by default is set to the documents folder. You can check the current working directory with getwd() function. If you want to change the working directory you can do it with setwd() and pass a string with new directory as argument. This way of working with directories will do for now but it’s not really a good way of managing directories. If you move your script to another folder or share the data and script with other people using setwd() will fail you. Because of this you should avoid manually setting working directory.. One way to deal with this problem is to use here() function from the here package. It uses some heuristics to determine the directory of your script. It can solve the problem of moving or sharing files but remember it uses heuristics so it might not always work. The other solution is to use R projects which automatically set the working directory to the project directory. We’ll learn about them later on."
  },
  {
    "objectID": "17linear_models.html",
    "href": "17linear_models.html",
    "title": "Linear Models",
    "section": "",
    "text": "-what is a linear model?\n-How to think about linear models\n-Basics of specifying a linear model\n-T-test, between-anova and regression are the same thing - a linear model\n-Buiding a simple linear model\n-Adding predictors\n-Categorical predictors"
  },
  {
    "objectID": "17linear_models.html#linear-models",
    "href": "17linear_models.html#linear-models",
    "title": "Linear Models",
    "section": "",
    "text": "-what is a linear model?\n-How to think about linear models\n-Basics of specifying a linear model\n-T-test, between-anova and regression are the same thing - a linear model\n-Buiding a simple linear model\n-Adding predictors\n-Categorical predictors"
  },
  {
    "objectID": "18statistical_control.html",
    "href": "18statistical_control.html",
    "title": "Statistical control",
    "section": "",
    "text": "-What is statistical control?\n-To control or not to control - confounders"
  },
  {
    "objectID": "19postprocessing.html",
    "href": "19postprocessing.html",
    "title": "Postprocessing results",
    "section": "",
    "text": "-general logic of working with results in R: you get a saved object\n-Making predictions\n-Marginal effects\n-Contrasts\n-reporting?"
  },
  {
    "objectID": "06loading_data.html#exercises",
    "href": "06loading_data.html#exercises",
    "title": "loading_data",
    "section": "exercises",
    "text": "exercises\n\nloading a flat file\nloading excel\nloading json"
  },
  {
    "objectID": "07data_wrangling.html#tidyverse",
    "href": "07data_wrangling.html#tidyverse",
    "title": "data wrangling",
    "section": "",
    "text": "For data wrangling we’ll be working within tidyverse throughout this course. Tidyverse is a set of packages designed for working with data in a clean, readable way. A huge advantage (apart from readability) is that all packages in tidyverse are designed to be compatible with each other and share common “grammar” and way of doing things. This way it is easy to combine them to do a lot of different things with your data. We’ve already met one package from this collection: readr. Other packages include:\n\ndplyr: package for data wrangling. We’ll focus on it in this class\ntidyr: package for tidying data and reshaping it. We’ll look at it in the next class\nggplot2: the go to package for data visualization in R. Absolutely the best of the best when it comes to plotting.\nforcats: package for working with factors\nstrings: package for working with text data\npurrr: functional programming stuff in R like easier iteration within tidyverse\ntibble: package that introduces slightly altered data frames\n\nTidyverse is not the only way in R for data wrangling (other package often used is data.table, a new alternative is polars). If you don’t want additional packages you don’t even need them, you can do almost everything in base R if you want to. So why choose tidyverse? First of all it’s extremely intuitive. Writing and reading code in tidyverse feels almost like writing plain text of what you want to do with your data. Thanks to pipes it also made code much more readable (more on that in a moment although there are now pipe alternatives). One downside of tidyverse is that it is significantly slower than other packages. If speed is paramount you might want to consider switching to data.table or polars."
  },
  {
    "objectID": "07data_wrangling.html#the-pipe",
    "href": "07data_wrangling.html#the-pipe",
    "title": "data wrangling",
    "section": "",
    "text": "So far if we wanted to use multiple functions in a single call we had to wrap one function inside another e.g. if we wanted to take a list of vectors, calculate the mean of each vector and then find out the highest mean we could do something like this:\n\nmax(sapply(list(c(1,2,3), c(4,5,6), c(6,7,8)), mean))\n\n[1] 7\n\n\nIt’s not the easiest code to read, right? When combining functions this way you need to read them inside out. This is not how people read. It would e much easier if we could read code more linearly e.g. from left to right and top to bottom. Enter the pipe! The pipe operator allows you to chain together functions in a readable way. The basic idea (there’s more to pipes though) is to take what is on the left hand side of the pipe and pass it as the first argument of whatever is on the right hand side of the pipe. This changes the inside-out into left-to-right code. There are 2 pipes in R. The first one comes from the magrittr package and this is the one used in tidyvese. This pipe looks like this: %&gt;%. If we wanted to rewrite the code above using this pipe it would look like this:\n\nlibrary(magrittr)\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) %&gt;%\n  sapply(mean) %&gt;%\n  max()\n\n[1] 7\n\n\nIt’s much easier to understand what this code does right? An alternative introduced in R 4.1 is the native pipe: |&gt;. The basic functionality is pretty much the same as in the magrittr pipe but you don’t need to load any packages to use it (you might need to enable native pipe in the global options in Tools bar in RStudio). The same code as above but with native pipe looks like this:\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) |&gt;\n  sapply(mean) |&gt;\n  max()\n\n[1] 7\n\n\nYou might wonder why have two kinds of pipes one of which needs loading a new package? The first reason is very simple: magrittr pipe is older. There are however a few differences. You can read about the details here. Remember that pipe automatically passes what is on the left as the first argument to whatever is on the right of the pipe? What if you need to pass it not as the first but second or third argument? Both pipe operators have a placeholder argument that can be used in such situations. %&gt;% has the . operator and |&gt; has _. The difference between them is that _ can only be used once and has to be used with named arguments. Here’s an example of how placeholder argument can work: append() allows you to join two vectors together. The vector passed as the second argument is appended to the one passed as the first argument:\n\nx &lt;- c(1,2,3)\ny &lt;- c(4,5,6)\nx %&gt;%\n  append(y, .)\n\n[1] 4 5 6 1 2 3\n\n\nGenerally, the differences boil down to simplicity: native pipe was deliberately created to be a simpler operator with less functionality. Most of the time you won’t notice much difference (maybe except for how the placeholder argument works)."
  },
  {
    "objectID": "07data_wrangling.html#the-basic-dplyr-function",
    "href": "07data_wrangling.html#the-basic-dplyr-function",
    "title": "data wrangling",
    "section": "",
    "text": "Now we can get to the basics of data wrangling in dplyr package. We’ll look at the storms dataset in the dplyr package. It stores information on date, place, status and some other things about storms from 1975 to 2021. The dataset stores multiple observations from each storm because measurements were made every few hours. Before we move one to working with data lets introduce one function: glimpse(). It’s a bit like str() but is a bit more readable for dataframes. This function can give you a concise look at what variables you have in your dataset. Lets load tidyverse, our dataset and look at it:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"storms\")\n\nglimpse(storms)\n\nRows: 19,066\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe dataset has 19066 rows and 13 columns. We can also see that we have various types of variables: many numeric, one factor and one character.\n\n\nYou can subset a dataframe either by columns or by rows. If you want to extract a subset of rows based on some logical conditions you can use filter(). Lets say we want only storms from 2020:\n\nstorms %&gt;%\n  filter(year == 2020)\n\n# A tibble: 863 × 13\n   name    year month   day  hour   lat  long status     category  wind pressure\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Arthur  2020     5    16    18  28   -78.7 tropical …       NA    30     1008\n 2 Arthur  2020     5    17     0  28.9 -78   tropical …       NA    35     1006\n 3 Arthur  2020     5    17     6  29.6 -77.6 tropical …       NA    35     1004\n 4 Arthur  2020     5    17    12  30.3 -77.5 tropical …       NA    35     1003\n 5 Arthur  2020     5    17    18  31   -77.3 tropical …       NA    40     1003\n 6 Arthur  2020     5    18     0  31.9 -77   tropical …       NA    40     1003\n 7 Arthur  2020     5    18     6  33.1 -76.7 tropical …       NA    40     1002\n 8 Arthur  2020     5    18    12  34.4 -75.9 tropical …       NA    45     1000\n 9 Arthur  2020     5    18    18  35.5 -74.7 tropical …       NA    45      993\n10 Arthur  2020     5    19     0  36.2 -73.1 tropical …       NA    50      991\n# ℹ 853 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nWe can also filter based on multiple conditions. This works exactly like all the logical operations we’ve seen previously. One difference is that you can use , instead of &. Lets say we want to get all storms from June 2020:\n\nstorms %&gt;%\n  filter(year == 2020, month == 6)\n\n# A tibble: 57 × 13\n   name       year month   day  hour   lat  long status  category  wind pressure\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Cristobal  2020     6     1    18  19.4 -90.9 tropic…       NA    25     1006\n 2 Cristobal  2020     6     2     0  19.6 -91.6 tropic…       NA    25     1005\n 3 Cristobal  2020     6     2     6  19.6 -92.1 tropic…       NA    30     1005\n 4 Cristobal  2020     6     2    12  19.5 -92.5 tropic…       NA    35     1004\n 5 Cristobal  2020     6     2    18  19.2 -92.6 tropic…       NA    40     1001\n 6 Cristobal  2020     6     3     0  19   -92.5 tropic…       NA    45      996\n 7 Cristobal  2020     6     3     6  18.9 -92.3 tropic…       NA    50      994\n 8 Cristobal  2020     6     3    12  18.8 -92.2 tropic…       NA    50      993\n 9 Cristobal  2020     6     3    13  18.7 -92.1 tropic…       NA    50      993\n10 Cristobal  2020     6     3    18  18.5 -91.9 tropic…       NA    45      994\n# ℹ 47 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nThere were 57 storms recorded in June 2020.\nIf you want to select only certain columns from a dataset you can use select(). E.g. if we want only the name, latitude and longitude of the storm we can do it like this:\n\nstorms %&gt;%\n  select(name, lat, long)\n\n# A tibble: 19,066 × 3\n   name    lat  long\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amy    27.5 -79  \n 2 Amy    28.5 -79  \n 3 Amy    29.5 -79  \n 4 Amy    30.5 -79  \n 5 Amy    31.5 -78.8\n 6 Amy    32.4 -78.7\n 7 Amy    33.3 -78  \n 8 Amy    34   -77  \n 9 Amy    34.4 -75.8\n10 Amy    34   -74.8\n# ℹ 19,056 more rows\n\n\nIf you want to select a range of columns you can use :\n\nstorms %&gt;%\n  select(year:hour)\n\n# A tibble: 19,066 × 4\n    year month   day  hour\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  1975     6    27     0\n 2  1975     6    27     6\n 3  1975     6    27    12\n 4  1975     6    27    18\n 5  1975     6    28     0\n 6  1975     6    28     6\n 7  1975     6    28    12\n 8  1975     6    28    18\n 9  1975     6    29     0\n10  1975     6    29     6\n# ℹ 19,056 more rows\n\n\nA particular situation in which you might want to subset a dataset is to get the rows with highest/lowest values of a variable, get the first/last rows or draw a random sample from the dataset. All of these can be achieved by different versions of slice(). slice_sample() will draw a random sample from the dataset (either by number or proportion). You can also specify if you want to draw with replacements:\n\nstorms %&gt;%\n  slice_sample(n = 100)\n\n# A tibble: 100 × 13\n   name      year month   day  hour   lat  long status   category  wind pressure\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Nicholas  2003    10    21    12  18.7 -51   tropica…       NA    45     1000\n 2 Barry     1983     8    29     0  25.5 -98.5 tropica…       NA    40      995\n 3 Jose      2017     9    15     0  25.4 -67.6 tropica…       NA    60      988\n 4 Olga      2019    10    25    12  24.7 -94.8 tropica…       NA    35     1004\n 5 Luis      1995     9     7     6  21.3 -66   hurrica…        4   120      936\n 6 Michael   2018    10    10    18  30.2 -85.4 hurrica…        4   135      920\n 7 Emmy      1976     8    22    12  16.2 -56   tropica…       NA    35     1006\n 8 Lili      2002    10     1     6  21   -82.2 hurrica…        1    75      970\n 9 Georges   1998     9    28     0  29.3 -88.5 hurrica…        2    95      961\n10 Rina      2017    11     6     0  29.1 -51.2 tropica…       NA    30     1011\n# ℹ 90 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nslice_min() and slice_max() allow you to get rows with highest values on a variable.\n\nstorms %&gt;%\n  slice_max(wind, n = 10)\n\n# A tibble: 24 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 14 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nFinally slice_head() and slice_tail() allow you to get n first or last rows from a dataset.\n\nstorms %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 13\n  name   year month   day  hour   lat  long status       category  wind pressure\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1 Amy    1975     6    27     0  27.5 -79   tropical de…       NA    25     1013\n2 Amy    1975     6    27     6  28.5 -79   tropical de…       NA    25     1013\n3 Amy    1975     6    27    12  29.5 -79   tropical de…       NA    25     1013\n4 Amy    1975     6    27    18  30.5 -79   tropical de…       NA    25     1013\n5 Amy    1975     6    28     0  31.5 -78.8 tropical de…       NA    25     1012\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nOne last thing about filtering. Sometimes you want to filter all unique values of a variable. In order to do it you can use distinct(). It will extract all unique values of a variable. By default it will return only the column with distinct values and drop all the other columns from the dataframe. If you want to keep all the other variables (though remember that it will probably keep only the first entry for each unique value!) you can set the .keep_all argument to TRUE.\n\nstorms %&gt;%\n  distinct(category)\n\n# A tibble: 6 × 1\n  category\n     &lt;dbl&gt;\n1       NA\n2        1\n3        3\n4        2\n5        4\n6        5\n\n\n\n\n\nSorting datasets based on variables is super simple. You can use the arrange() function and if you need to sort in descending order use desc() inside it. Lets say we want to find the storm with the strongest wind:\n\nstorms %&gt;%\n  arrange(desc(wind))\n\n# A tibble: 19,066 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 19,056 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nLooks like Allen from 1980 was the strongest storm.\n\n\n\nCounting values in variables is very simple, just use count(). One thing to remember is that count() will returned a different dataframe. Unless you specify anything additional it will return a dataframe with 2 columns: one will contain all unique values of the variable you counted and the other one, named n will contain their counts (you can specify the name variable to change that to something else). Setting sort argument to TRUE will sort the counts in descending order. E.g. if we want to find out which year had the most measurements of storms (and not the number of storms! Remember that each row is 1 measurement of 1 storm) we can do it with:\n\nstorms %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 47 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2005   873\n 2  2020   863\n 3  1995   762\n 4  2010   663\n 5  2012   654\n 6  2017   610\n 7  2018   608\n 8  2004   604\n 9  2003   593\n10  2021   592\n# ℹ 37 more rows\n\n\n\n\n\nA common task when wrangling data is creating new variables in an already existing dataset. You can do it by using mutate(). Lets say we want to create a new variable that stores information on whether the storm was during summer (June, July, August) or not:\n\nstorms %&gt;%\n  mutate(summer = ifelse(month == 6 | month == 7 | month == 8, TRUE, FALSE))\n\n# A tibble: 19,066 × 14\n   name   year month   day  hour   lat  long status      category  wind pressure\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n# ℹ 19,056 more rows\n# ℹ 3 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;, summer &lt;lgl&gt;\n\n\nRemember that if you want to keep the variable you need to assign the new dataset to an object.\n\n\n\nAnother extremely common task is to get summaries about our dataset. We can do it with summarise() function. e.g. what if we want to see the mean and standard deviation of strength of wind (for now across all measurements):\n\nstorms %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T),\n            sd_wind = sd(wind, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_wind sd_wind\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      50.0    25.5\n\n\nNotice that the shape of the dataset has changed now. The columns are now the summaries and not the original variables.\n\n\n\nSo far we have been calculating things on entire datasets. In many situations you want to calculate something separately for each level of a categorical variable (much like tapply() earlier). To group a dataset we can use group_by(). You can also group by multiple variables at once by separating them by a coma. R will group by the first variable and then by the second etc. This is especially useful for creating summaries. E.g. if we want to get average wind speed for each storm we can easily do it:\n\nstorms %&gt;%\n  group_by(name) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n# A tibble: 258 × 2\n   name     mean_wind\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 AL011993      29.5\n 2 AL012000      25  \n 3 AL021992      29  \n 4 AL021994      24.2\n 5 AL021999      28.8\n 6 AL022000      29.2\n 7 AL022001      25  \n 8 AL022003      30  \n 9 AL022006      31.5\n10 AL031987      21.2\n# ℹ 248 more rows\n\n\nSimilarly if we want to calculate average wind speed from all measurements for each year and month:\n\nstorms %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 253 × 3\n# Groups:   year [47]\n    year month mean_wind\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1  1975     6      37.5\n 2  1975     7      49.7\n 3  1975     8      44.5\n 4  1975     9      55.9\n 5  1975    10      62.4\n 6  1976     8      55.9\n 7  1976     9      61.3\n 8  1976    10      51.0\n 9  1977     8      53  \n10  1977     9      50.6\n# ℹ 243 more rows\n\n\nActually the summarise() function has its own argument for calculating grouped summaries. You can specify .by argument inside the summarise() function. If you use group_by() then summarise() will by default drop the last level of grouping.\nOne more important thing about the group_by() function is that it works by adding an attribute to the dataframe. This means that after a group_by() all subsequent operations will be conducted on the grouped dataframe. E.g. if you sort after grouping then sorting will be conducted within each group separately. You can drop the grouping with ungroup(). There is one last special kind of grouping you might want to use - sometimes you want to perform some operation separately on each row (e.g. calculate the average of a multi item scale for each participant). You can do it with rowwise().\n\n\n\nThere are situations when you want to perform the same operation on multiple columns (e.g. calculate the mean and standard deviation of multiple variables). You can do it by hand but this can be tedious. To simplify it you can use across() inside mutate() or summarise(). the syntax of across() is as follows: the first argument, .cols specifies which columns to perform operations on. The second argument .fns specifies which functions to apply. You have to provide it a list of functions (preferably named list) or a formula. Finally the .names argument specifies how to automatically assign new variable names. E.g. “{.col}_{.fn}” will create variables with column name, underscore and function name (that’s why named list is useful here). If we want to get the mean and standard deviation of wind speed and pressure in each category of storms we could do it in a few lines of code (by the way, notice how group_by() by default includes NA as a separate category):\n\nstorms %&gt;%\n  group_by(category) %&gt;%\n  summarise(across(.cols = c(\"wind\", \"pressure\"), .fns = list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 6 × 5\n  category wind_mean wind_sd pressure_mean pressure_sd\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1        1      71.0    5.55          981.        9.21\n2        2      89.5    3.77          967.        8.94\n3        3     104.     4.18          955.        8.91\n4        4     122.     6.39          940.        9.43\n5        5     147.     6.22          918.       12.0 \n6       NA      38.1   12.0          1002.        9.24\n\n\n\n\n\nOne more useful thing for data wrangling is a set of functions making it easier to select multiple columns based on some condition. There are a few helper functions you can use to do that. They do basically what their names suggest. These verbs are as follows: starts_with(), ends_with() and contains(). This way you don’t have to manually type all the names if they have something in common (e.g. they are items from the same scale so they are named ``scale_1, scale_2 etc.). E.g. lets say we want to get all the columns that end with “diameter”:\n\nstorms %&gt;%\n  select(ends_with(\"diameter\"))\n\n# A tibble: 19,066 × 2\n   tropicalstorm_force_diameter hurricane_force_diameter\n                          &lt;int&gt;                    &lt;int&gt;\n 1                           NA                       NA\n 2                           NA                       NA\n 3                           NA                       NA\n 4                           NA                       NA\n 5                           NA                       NA\n 6                           NA                       NA\n 7                           NA                       NA\n 8                           NA                       NA\n 9                           NA                       NA\n10                           NA                       NA\n# ℹ 19,056 more rows\n\n\nNice! These verbs also work nicely inside across(). One thing to be aware of: if no column matches what you ask for you won’t get an error but a dataframe with all the rows but 0 columns:\n\nstorms %&gt;%\n  select(contains(\"some_weird_name\"))\n\n# A tibble: 19,066 × 0"
  },
  {
    "objectID": "07data_wrangling.html#combining-functions-together",
    "href": "07data_wrangling.html#combining-functions-together",
    "title": "data wrangling",
    "section": "",
    "text": "Where tidyverse really shines is in combining multiple functions together with pipes. Through different orders of the functions described above we can get a ton of things out of our dataset. This already gives us the ability to anwser a number of questions that might be very interesting for analysis.\n\nExample 1: Lets say we want to find out the name of storm from each category that had the highest average pressure in 1989.\n\nstorms %&gt;%\n  filter(year == 1989) %&gt;%\n  group_by(category, name) %&gt;%\n  summarise(mean_pressure = mean(pressure, na.rm = T)) %&gt;%\n  slice_max(mean_pressure, n = 1)\n\n`summarise()` has grouped output by 'category'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   category [6]\n  category name      mean_pressure\n     &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1        1 Chantal            987 \n2        2 Dean               972.\n3        3 Hugo               953.\n4        4 Gabrielle          943.\n5        5 Hugo               918 \n6       NA Barry             1012.\n\n\nExample 2: Lets say we want to find the average wind speed at each hour of the day but we want that in kilometers per hours rather than knots (as is in the database). 1 knot is around 1.852 km/h.\n\nstorms %&gt;%\n  mutate(wind_km = wind*1.852) %&gt;%\n  group_by(hour) %&gt;%\n  summarise(mean_wind_km = mean(wind, na.rm = T))\n\n# A tibble: 24 × 2\n    hour mean_wind_km\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1     0         49.6\n 2     1         76.2\n 3     2         64.4\n 4     3         76.7\n 5     4         70.6\n 6     5         76.2\n 7     6         49.7\n 8     7         80.8\n 9     8         68.2\n10     9         60.9\n# ℹ 14 more rows"
  },
  {
    "objectID": "07data_wrangling.html#exercises",
    "href": "07data_wrangling.html#exercises",
    "title": "data wrangling",
    "section": "",
    "text": "Throughout the exercises you’ll still work with the storms dataset\n\nFind out which year had the most storms (remember that each row is 1 measurement of 1 storm. You have to find the year with the most storms and not the most measurements!)\nFind the average wind speed for each storm in 2018 and then sort from the highest to the lowest.\nCalculate the mean and standard deviation of measurement pressure for each month of the year"
  },
  {
    "objectID": "08join_restructure.html#joining-data",
    "href": "08join_restructure.html#joining-data",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "In many situations the information that you need is not stored in a single dataset but in multiple ones. For example you might be working on a longitudinal study and each wave is saved in a separate dataset.\n\nbind cols and bind rows\n\n\n\nDifferent types of joins: inner, left, right, full,\n\n\n\nFiltering joinssemi, anti\nGotchas in joins: duplicated values"
  },
  {
    "objectID": "08join_restructure.html#pivoting-data",
    "href": "08join_restructure.html#pivoting-data",
    "title": "restructuring_and_joining_data",
    "section": "",
    "text": "-pivot longer\n-pivot wider"
  },
  {
    "objectID": "09data_viz_1.html#some-base-r-graphics",
    "href": "09data_viz_1.html#some-base-r-graphics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Before we move on to ggplot2 lets look at some built-in base R graphics. The graphics and stats packages have some function for plotting already available.\n\nThe most generic is the plot() function. It allows you to create simple plots in R. The first arguments are usually the variables you want to map onto the axes:\n\nplot(midwest$percadultpoverty, midwest$percollege)\n\n\n\n\nplot() function has a bunch of arguments you can use to customize the plot. For example we can change the color and thickness of the points and add a title to the plot and axes:\n\nplot(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2,\n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nUnfortunately the plot() function does not have great documentation and finding some of the arguments can be quite difficult. Some of these arguments also have very unintuitive names (e.g. argument named lty specifies line type, good luck memorizing that!). Base R has additional functions for more specific plots. Namely, lines() will create a line plot, points() will create a scatterplot and boxplot() will create a boxplot. These functions have better documentation than the general plot() though I still don’t consider it great. Lets see some of these in action. lines() and points() require that you first initialize the plots with the coordinates set to the variables of interest with the plot() function. You can specify what kind of plot (line plot, scatterplot etc) you want inside plot() by setting the type argument. E.g. setting it to l will create a line plot.\nLines: Lets say we want to make a line plot that will display the 10th, 50th and 90th quantile of percent of adults in poverty in all states. We’ll also introduce the axis() function which gives you some control over how the x and y axis should look like. Here we need it because lines() does not like categorical values at x axis so we need to add the state names (we add them at the top) with axis():\n\n#get the quantiles\nlibrary(dplyr)\n#calculate quantiles at of percadultpoverty for each state\nlist_q &lt;- tapply(midwest$percadultpoverty, midwest$state, quantile, c(.1,.5,.9))\n\n#convert the result into a dataframe with each column as one quantile and each row as one state\ndf_q &lt;- data.frame()\ndf_q &lt;- bind_rows(list_q[[1]], list_q[[2]], list_q[[3]], list_q[[4]], list_q[[5]])\ncolnames(df_q) &lt;- c(\"q_10\", \"q_50\", \"q_90\")\n\n#plot them\nplot(1:5, df_q$q_10, ylim = c(0,20), type = \"l\", col = 2, lwd = 2, lty = 2)\nlines(1:5, df_q$q_50, col = 4, lwd = 2, lty = 1)\nlines(1:5, df_q$q_90, col = 2, lwd = 2, lty = 2)\naxis(side = 3, at = 1:5, labels = unique(midwest$state))\n\n\n\n\nOne important thing here: notice how each element is added to the plot on a new line. These functions are not chained together in any way and we are not saving any intermediate objects. This is pretty unusual in R (although perfectly normal in other programming languages). It’s just how base R plotting works.\nPoints: We can recreate the plot that we made with the generic plot() function but using points():\n\nplot(midwest$percadultpoverty, midwest$percollege)\npoints(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2, \n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nBoxplots are useful for showing differences in distributions of some continuous variable between levels of some factor. For example lets say we want to\n\nboxplot(midwest$percadultpoverty ~ midwest$state)\n\n\n\n\n\nThere are also built-in functions for plotting distributions: hist() for histograms, plot(density()) for density functions and plot.ecdf() for cumulative distribution plots. Lets look at all 3 plots for percent of population in college:\n\nhist(midwest$percollege)\n\n\n\n\n\nplot(density(midwest$percollege))\n\n\n\n\n\nplot.ecdf(midwest$percollege)\n\n\n\n\nOne problem with base R plots is that they are not intuitive. Lots of arguments have weird names and doing some things is really not so easy. Making more complicated plots (e.g. adding text annotations on the plot) is also generally hard to do. That’s why we’ll focus on ggplot2 which is much more intuitive and versatile."
  },
  {
    "objectID": "09data_viz_1.html#enter-ggplot2",
    "href": "09data_viz_1.html#enter-ggplot2",
    "title": "Data visualization part 1",
    "section": "",
    "text": "ggplot2 is a package in th tidyverse designed for making data visualizations. One of the great things about it is that it breaks down each plot into a number of layers that can be changed (more or less) independently. This idea is encapsulated in what is called the grammar of graphics."
  },
  {
    "objectID": "09data_viz_1.html#grammar-of-graphics",
    "href": "09data_viz_1.html#grammar-of-graphics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "The name comes from a book by Leland Wilkinson under the same title. Basically making a plot with the grammar of graphics is like making a building with lego blocks with different colors. You can mix the colors of the blocks to get exactly the building you want. Similalry in ggplot2 you can use different layers to make the plot that you want (e.g. mix different datasets or add a line to a scatterplot). Ok, but what are those layers? In ggplot2 there are following layers:\n\nData: the datasets (usually data frames) you want to plot\nAesthetics: mapping between data and the plot, e.g. what is to be mapped to x and y axis\nGeometry: shapes used to represent data\nStatistics: any statistics like means or confidence intervals that you want to add\nCoordinates: coordinates of the plot (axes limits, cartesian vs polar coordinates etc.)\nFacets: this layer is used for making subplots (e.g. separate plot for each level of a categorical variable)\nTheme: All non-data stuff like fonts, titles\n\nYou always start the plot with ggplot() function. All the lego blocks that you want to add are chained together with +. Lets see what happens when we pass the first layer, data, to our plot:\n\nmidwest %&gt;%\n  ggplot()\n\n\n\n\nHmm, we get en empty plot. Why is that? Well, all we supplied so far is the data we want to plot but we did not include any additional information so R doesn’t know yet what exactly should be displayed on the plot. We need to add the next layer: aesthetics."
  },
  {
    "objectID": "09data_viz_1.html#aesthetics",
    "href": "09data_viz_1.html#aesthetics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Aesthetics map what variables should be displayed on the plot in what way. In this layer you declare e.g. what should be on the x and y axis. The basic aesthetics (the list is not exhaustive) are:\n\nx axis\ny axis\ncolour: colour of points and lines (that includes borders of e.g. rectangles)\nfill: colour of filling\nalpha: transparency. 0 means completely transparent, 1 means not transparency\nsize: point size\nlinewidth: width of lines\nlinetype: type of line (e.g. dashed)\nshape: shape of points (circles, rectangles, etc.)\nlabels: text\n\nAesthetics are declared within aes(). It can be declared within ggplot() function, separately or inside geoms (more on those in a second). Lets see what happens when we add aesthetics to our plot:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege)\n\n\n\n\nNow we got our axes! Notice there is no data on the plot though. That’s because so far we have declared which dataset to plot and which variables to map onto axes but we did not specify how to represent the data. This is declared in the next layer: geometry."
  },
  {
    "objectID": "09data_viz_1.html#geometries",
    "href": "09data_viz_1.html#geometries",
    "title": "Data visualization part 1",
    "section": "",
    "text": "After providing a dataset and aesthetics we have the variables and their mapping to axes on the plot. However, we still don’t have any shapes to actually represent the data. Do we want a scatter plot? Or maybe a bar plot? Or a line plot? The shapes used to represent the data are defined in the geometry layer. Generally all geometries start with geom_ so for example geom_point() will make a scatter plot while geom_bar() will make a bar plot.\nGeometries differ in what aesthetics they accept. You can look up what these are by looking up help for a given geometry. Each geometry has all the aesthetics it accepts in its documentation. They also differ in what kinds of variables they expect (any combination of categorical vs continuous variables).\nLets expand the initial plot with geom_point() to make a scatterplot!\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege) +\n  geom_point()\n\n\n\n\nWe got a plot very similar to the one we made with base R! We can customize it further if we want to. Lets say we want to represent each state with a different color. We just need to add a color aesthetic. R will add a legend automatically:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege, color = state) +\n  geom_point()\n\n\n\n\nThere is one more thing about geometries and aesthetics. Remember you can declare aesthetics in different places? You can set global aesthetics inside the ggplot() function. If you do so these aesthetics will be used by default by all geometries in that plot. You can also set aesthetics inside a given geometry (in fact you can even set a different dataset for a given geometry; this is what we meant by independence of layers) but then they will be used only for this particular geometry and won’t be inherited by other ones.\nCompare the to plots below. They produce the same result:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point()\n\n\n\n\nAnd the second plot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege))\n\n\n\n\nSome common geometries you can encounter are as follows:\n\ngeom_histogram() and geom_density() for displaying distributions of continuous variables\ngeom_bar() for displaying counts of categorical variables (you can change counts to other summary statistic but more on that later)\ngeom_col() for displaying differences in some continuous variable between levels of a factor\ngeom_point(): your good old scatterplot\ngeom_boxplot()\ngeom_violin() a bit like boxplot but displays a distribution of a continuous variable for each level of a factor\ngeom_smooth() for displaying lines of best fit (e.g. from a linear model)\n\nYou can check them out if you want to to see how they look like.\nLets look at 2 thing in a bit more details with regard to geometries. First, lets look at combining geoms. We can add a line of best fit to our scatterplot if we want to by adding an additional geometry. geom_smooth() will use a GAM or LOESS by default (depending on how many unique values x variable has) but we can set it to a linear model by adding method = \"lm\". A linear model probably won’t do well here but it’s just for demonstration:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point() +\n  geom_smooth(method  =\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee? Just like building with lego blocks.\nLast thing before we move on: a few words on geom_bar() and geom_col(). They often get mixed up at the beginning because they both show bar plots. geom_bar() takes a single aesthetic and is generally used to display counts of some factor variable. For example if we wanted to see how many counties are there in each state we could use geom_bar() (later we’ll see how to display e.g. means of variables across levels of a factor):\n\nmidwest %&gt;%\n  ggplot(aes(x=state)) +\n  geom_bar()\n\n\n\n\ngeom_col() takes at least 2 aesthetics: x and y. It needs one categorical and one continuous variable. By default it is going to sum all values in a given category. e.g. lets look at the sum of area of all counties in each of the states:\n\nmidwest %&gt;%\n  ggplot(aes(x=state, y = area)) +\n  geom_col()\n\n\n\n\nWhat if we wanted to look at the average county area in each state? We can e.g. first summarise the dataset and then pipe it to plot:\n\nmidwest %&gt;%\n  summarise(mean_area = mean(area), .by = state) %&gt;%\n  ggplot(aes(x = state, y = mean_area)) +\n  geom_col()\n\n\n\n\nSince ggplot2 is part of tidyverse it’s really easy to pipe a set of dplyr functions into a plot in a single call!"
  },
  {
    "objectID": "09data_viz_1.html#attributes",
    "href": "09data_viz_1.html#attributes",
    "title": "Data visualization part 1",
    "section": "",
    "text": "There are situations in which you don’t want to set some feature to be represented by a given variable but to set them to a fixed value for the entire plot/geometry. For example you might want to set the color or size of all points in a scatter plot. That’s when you set attributes. They are declared inside geometries but outside of aesthetics. Lets say we want to take our scatterplot and change the transparency and color of the points. We can do it by defining them inside geometry. The names for the arguments are the same as for aesthetics. Just remember to use them outside of aes()!\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3)\n\n\n\n\nBy the way, you can define colors either with hex values as RGB but you can also use one of the built-in colors in R. You can check all the names of those built-in colors with colors() function. Another neat thing is that in newer version of RStudio you can see the preview of the colors when you type them in a script.\nOne potential problem is when aesthetic and scale come into conflict. E.g. what will happen if we map color to state variable and then set it as attribute? Lets see:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), color = \"red\", alpha = .3)\n\n\n\n\nThe attribute overrides the aesthetic. It’s something to remember about."
  },
  {
    "objectID": "09data_viz_1.html#scales",
    "href": "09data_viz_1.html#scales",
    "title": "Data visualization part 1",
    "section": "",
    "text": "One more thing are functions for working with scales: they allow you to have more control over how each scale is represented (e.g. what the breaks and values are, should the scale be transformed). For example notice that R automatically chose some limits for the scales and displayed breaks (the values on the axes). You can have more control over that with the scale_ family of functions. It’s a family of functions because you need to declare: 1) which aesthetic you want to change and 2) what kind of scale you are working with (continuous, discrete or binned). So for example scale_x_continuous() allows you to customize a continuous x axis. The basic things that you can change inside the scale_ function are: breaks, labels, limits and the expand argument (the last one controls additional space - notice that e.g. on the x axis there is a little space on the left of 0 and there is similarly a little space to the right of the point with the highest percadultpoverty value).\nLets say now we want to work a little with our scales. Lets change the breaks to be every 5%, change the labels to be actually in percentage format (there is a very neat function in scales package called label_percent() that does that. We just need to set scale argument in it to 1 because in our dataset e.g. 10% is represented as 10 and not 0.1):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,40,5),labels = scales::label_percent(scale = 1)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\n\n\n\nA cautionary tale about using limits in scale_ functions: These limits work by filtering the data to be plotted. This can be a serious problem because you basically lose data when plotting and only get a warning about it. This can be especially problematic if you are trying to display bars or ranges because if e.g. one side of an interval falls out of the limits, the entire range will not be displayed. Lets see what happens if we set limits on one of our axes:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,25,5),labels = scales::label_percent(scale = 1), limits = c(0, 25)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice the warning about removed data. This is especially problematic in bar plots because by default they start from 0 (which can be a good thing because it reduces misinterpretation of visual differences between bars but can be undesired e.g. if displaying results of a 1-5 Likert scale which doesn’t have a 0). Another problematic situation is when plotting some summaries e.g. means and limiting axes. Limits will work before calculating the summaries so you might get nonsensical plots because of this like below where we try to plot mean percadultpoverty by each state and limit the y axis to 10:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percadultpoverty)) +\n  geom_bar(stat = \"summary\", fun = mean) +\n  scale_y_continuous(limits = c(0,10))\n\nWarning: Removed 220 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nThe warning now says that 220 rows were removed! What’s even weirded we still got our plot but the means were calculated based on trimmed variable."
  },
  {
    "objectID": "09data_viz_1.html#colors",
    "href": "09data_viz_1.html#colors",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Scale functions also allow you to control the color and fill aesthetics. You can change which color palette you want to use. There are many packages available with predefined color palettes (e.g. viridis or MetBrewer). We’ll focus on the built-in Brewer palettes and on making manual palettes. Brewer palettes can be invoked with scale_color_brewer() and scale_fill_brewer(). You can choose color palette by setting the palette argument. I can never remember the names of all the palettes but you can easily google them (just type something like “R brewer palettes). Lets change our palette on the scatterplot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also set your own colors and create your own palette. In order to do that use scale_color_manual() and scale_fill_manual(). You can set the colors as hex or built-in colors in values argument. You can provide it a simple text vector or named vector to explicitly map each categorical variable (if you have categorical variables mapped to color or fill):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_manual(values = c(\"IL\" = \"#5f0f40\", \"IN\" = \"#9a031e\", \"MI\" = \"#fb8b24\", \"OH\" = \"#e36414\", \"WI\" = \"#0f4c5c\"))\n\n\n\n\nfor contiunous variables matched to color aesthetic this works a little different. You need scale_color_gradient() function. scale_color_gradient2() allows you to specify a midpoint and thus make a divergent palette. Lets code county area with color this time and specify our own palette with midpoint at the mean county area:\n\nmidpoint &lt;- mean(midwest$area)\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = area)) +\n  scale_color_gradient2(low = \"#e63946\", mid = \"#8d99ae\", high = \"#1d3557\", midpoint = midpoint)\n\n\n\n\nWhen using colors on a plot you need to be mindful of a number of things. Color is used to convey information so it should be visible. Try to avoid very small contrasts if you want something to stand out (e.g. using light grey on white background). Also remember that not everyone perceives color the same way so it is worth checking if your palette is suitable for everyone. Finally, color shouldn’t be used just because “it looks cool”. Generally everything that is in a plot should be there for a reason. Remember that data visualizations should convey information. That’s their primary purpose. Using color can help with that but it can also make things more difficult to understand. E.g. using a lot of flashy colors just for the sake of using colors might make a plot much more difficult to understand for viewers. Finally, colors can be used to convey different kinds of information, from continuity (e.g. a palette going from light to dark blue to code a continuous variable liek percentage of adults in college), divergence (a palette going from dark blue through light blue and red to dark red to display polarization of opinions) or contrast (e.g. contrasting colors in a palette to display different US states)."
  },
  {
    "objectID": "09data_viz_1.html#positions",
    "href": "09data_viz_1.html#positions",
    "title": "Data visualization part 1",
    "section": "",
    "text": "One issue that pops up quite often, especially when adding color to aesthetics, is how to deal with overlapping values. On the plots above there were some overlapping points on the scatterplot but we could easily deal with it by adjusting transparency. There are situations when this is not so easy or even not desirable. For example you can’t just as easily deal with overlapping bars. ggplot2 has a number of position adjustments that help us deal with this problem. THey allow us to stack, normalize or nudge shapes so that they won’t overlap. The basic position adjustments are:\n\ndodge: move shapes to the side (how much is controlled with the width argument)\nstack: stack shapes on top of each other\nfill: stack shapes on top of each other and normalize height (good for displaying proportions)\njitter: add some random noise (you can adjust how much with height and width arguments). It’s good for cluttered scatterplots\nnudge: slightly move shapes, good for nudging text\n\nOk, lets see some of them in action. We’ll start with jitter. In some situation you might want to display point with a categorical x axis. This can be useful when showing a distribution, e.g. with geom_violin() and adding all the data points on top of it with geom_point(). IF we don’t use any position adjustments we’ll get something like this (we want to look at distribution of percollege in each state):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point()\n\n\n\n\nOk, lets add some jitter but constrain it to be only in width (adding jitter in height might change how we interpret some values!):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point(position = position_jitter(height = 0, width = .2))\n\n\n\n\nThis makes it much easier to see all the points!\nNow we’ll move on to position adjustments in barplots. Lets say we want to look at number of counties in each state that are or are not in metro area(inmetro variable). We can do it by adding a fill aesthetic to the geom_bar():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar()\n\n\n\n\nBy default geom_bar() uses the stack position adjustment. Lets experiment with it a little and see what happens if we add a fill adjustment:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar(position = \"fill\")\n\n\n\n\nNow we have proportions rather than counts. Remember that order here matters (proportions sum to 1 within each state. If we wanted to see the share of each state in metro vs non-metro counties in the midwest we would need to switch x and fill aesthetics).\nTo see position dodge we will look at geom_col(). Lets say we want to plot the average percentage of adults in poverty in metro and nonmetro counties in each state:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(state, inmetro)) %&gt;%\n  ggplot(aes(x = inmetro, y = mean_perc, fill = state)) +\n  geom_col()\n\n\n\n\nThis looks pretty bad right? geom_col() also uses stack adjustment by default. To make it more readable and compare the percentages we can use the dodge adjustment:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(inmetro, state)) %&gt;%\n  ggplot(aes(x = as.factor(inmetro), y = mean_perc, fill = state)) +\n  geom_col(position = position_dodge(width = .9))\n\n\n\n\nMuch better! Now we can compare the percentages across state and metro vs non-metro counties!"
  },
  {
    "objectID": "08join_restructure.html#separating-and-uniting-variables",
    "href": "08join_restructure.html#separating-and-uniting-variables",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "In order to separate 1 variable into more you can use separate(). The opposite operation can be done with unite().When separating you need to specify which variable to split, what are the names of the new variables (passed as a character vector) and what is the separator which basically tells R where to “cut” the old variable into new ones. By default the old variable is removed from the dataset:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# separating example\n\nUniting is very similar, it just has a reversed effect. You specify what should the name of the new variable be, what are the names of variables to unite and what R should use to separate the values from the old variables.\n\n\nAnother way to separate is by separating into rows. This way each new value will be added in additional row rather than a column. You can do it with separate_rows().\nA potential problem with separate() is when various rows have different number of values. Then you might get conflicting number of columns to create."
  },
  {
    "objectID": "08join_restructure.html#from-wide-to-long-format-and-back-again",
    "href": "08join_restructure.html#from-wide-to-long-format-and-back-again",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "UN roll calls data?\npivot longer\npivot wider"
  }
]