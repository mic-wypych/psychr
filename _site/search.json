[
  {
    "objectID": "01r_intro.html",
    "href": "01r_intro.html",
    "title": "First steps in R",
    "section": "",
    "text": "This chapter introduces R and RStudio and shows you how to perform the most basic operations in R. We’ll go through the basics of what is R, how to navigate in the most common program for working with R called RStudio and we’ll also look into where to look for help when you might need it."
  },
  {
    "objectID": "01r_intro.html#what-is-r",
    "href": "01r_intro.html#what-is-r",
    "title": "First steps in R",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language. It was designed in the 90s mainly with data analysis, data visualization and statistics in mind. What does it mean it is a programming language? To put it simply it provides you the words and the syntax to tell your computer exactly what to do. It means you need to be very precise in what you write (computers aren’t very good at understanding typos or vague instructions) but you can accomplish a lot with it by harnessing the computing power of your machine!\nMany people might wonder why bother with R rather than stick to some point and click software like SPSS or Excel? There is a number of reasons:\n\nIs open source and free! This becomes especially important when you leave academia and very often can’t work on proprietary software like SPSS.\nHas a great community! The first point also makes it much easier for people to collaborate and create new things for R. It’s also much easier to ask questions and get answers from others\nIs more reproducible! Yes, some proprietary software allows you to write code (like SPSS) but what good is that if you need to buy the software to run it?\nIs more flexible than SPSS. There is generally more things you can do in R and you can customize the code so that it does exactly what you need. Don’t like the default settings? You can change them! If you need to you can just write your own functions to calculate just what you need.\nMore things are being developed in R. Generally more newer stuff is developed in R before it reaches e.g. SPSS.\nIs much faster. Once you start working with large datasets and computation heavy analyses you will need things to run fast. R is pretty good with it (although for some really large things it won’t do and you might want to switch to other programming languages like C++. Ultimately for some big things R can serve as just interface for C++).\n\nYou can download R from here. For this course it’s best you work with R version of at least 4.2.0."
  },
  {
    "objectID": "01r_intro.html#what-is-rstudio",
    "href": "01r_intro.html#what-is-rstudio",
    "title": "First steps in R",
    "section": "What is RStudio?",
    "text": "What is RStudio?\nWhile R is a programming language RStudio is an IDE (Integrated Development Environment). Basically, it’s purpose is to make working in R easier and more reproducible. It makes writing and managing code much easier. It can also show you some basic results like plots and tables. You can download RStudio from here.\nWhen you open RStudio you might see something like this:\n\nWe’ll go through each pane one by one and see what they are used for. Lets start with top-left:\n\n\n\nFirst pane: R scripts\n\n\nThe first pane is where your R scripts are displayed. This is the place where you will most commonly write your code. Scripts are files that contain your code and can be saved.\n\n\n\nSecond pane: console\n\n\nSecond pane is where the console is. That’s where the results of your analyses will appear. You can also write code in the console but you can’t easily save it. Writing code in the console is best if you need to check something quickly and you don’t need it saved. However, it’s best to write the code in the script because then you have all your steps saved and can easily retrace them.\n\n\n\nThird pane\n\n\nThird pane contains a few tabs. The most important for us is the Environment tab. This will by default show the global environment - a place where all the objects that you create in your script are stored (e.g. datasets that you load, results of analyses, plots, etc.). The other tabs show the history (so all the functions you ran), connections to databases and allow access to simple tutorials in R.\n\n\n\nFourth pane\n\n\nFourth pane also contains a few tabs. It’s mainly used for viewing various things that are results of your code. The plots that you create will be displayed here, as well as some other visualizations like tables, maps etc. (the latter two in the Viewer tab). If you look up help for a function (you’ll see in a second how to do that) the documentation for that function will also show up here. You can also view which files are available in your current directory (the place where R will try to look for or save files by default) and the list of available packages."
  },
  {
    "objectID": "01r_intro.html#getting-help",
    "href": "01r_intro.html#getting-help",
    "title": "First steps in R",
    "section": "Getting help",
    "text": "Getting help\nWorking in R, especially in the beginning, might be quite overwhelming. Fortunately there is plenty of places where you can look for help. You don’t need to know every single function by heart. Looking for help is perfectly normal. Lets see what are the most common places you can find it:\n\nBuilt-in help: you can check the documentation of a function in R by calling ?function_name. The documentation of the function will appear in the help tab. It should contain the basic information on the function: what it does, how it should be called, what arguments the function accepts. It can also contain more detailed information like what is the result of a given function or some examples of how to use it.\nCRAN: short for Comprehensive R Archive Network. You can download the newest version of R from CRAN. It’s also the place where many R packages are stored and can be downloaded from. Documentation of packages can be found there as well. You can visit its website here.\nBooks: there is plenty of great books on R that cover a range of various topics, many of them are available for free on the web. Books allow you to get a much more detailed account of various things you can do in R, they are also often written by people who developed specific packages for doing the things described in the books.\nThe Internet: R community is generally very welcoming and helpful and there is plenty of places where you can look for help on the web, from social media like Twitter to common forums like StackOverflow. One thing to keep in mind about answers from the web is that they do not constitute the ‘official’ solutions so they might be wrong - it’s often a good idea to double check or be sure that you understand the solution proposed by someone on a forum."
  },
  {
    "objectID": "01r_intro.html#basic-operations",
    "href": "01r_intro.html#basic-operations",
    "title": "First steps in R",
    "section": "Basic operations",
    "text": "Basic operations\nNow that we know how the basic interface for working in R looks like, we can move on to making basic operations in R. As you could see, there are 2 places where we can write code: the console or in the scripts. If you type a command in the console you can run it by pressing enter. In order to run a command from the script go to the line where your command is or highlight it and press ctrl + enter.\nIf you’re feeling like using a very fancy calculator you can turn R into one. All the basic mathematical operations work in the same way. So e.g. addition is made with +:\n\n2 + 2\n\n[1] 4\n\n\nOther operations work in a similar way. Raising to a power can be achieved with ^:\n\n2^3 + (3/2)\n\n[1] 9.5\n\n\nOk, so we know we can use R as a calculator. However, that’s not a lot and might seem like a an overkill. Numbers are not the only thing R can work with though."
  },
  {
    "objectID": "01r_intro.html#basic-values",
    "href": "01r_intro.html#basic-values",
    "title": "First steps in R",
    "section": "Basic values",
    "text": "Basic values\nWhen analyzing something or writing a program you will probably encounter a number of different types of values. Above you saw one of them: numeric values. These can store numbers with values after the decimal point.. The other types are:\n\ninteger: it stores numbers without decimal point. They can be created by putting capital l after the number e.g. 3L.\nstring: this type refers to to text values. Text is created using quotation marks e.g. 'this is text'\nboolean/logical: this type refers to logical values: TRUE or FALSE\n\nThese types form a certain hierarchy: some of them can be converted to other but not the other way around. The reason why this is important is that if R encounters more than one type of value in one operation it will often try to convert the types so that they match. It will do so in a way to create the highest matching type. The hierarchy goes in this way: boolean -&gt; integer -&gt; numeric -&gt; string. All the types can be converted to text but not the other way around. Boolean values are converted so that TRUE = 1 and FALSE = 0. o e.g. if you try to add boolean and numeric value, R will convert the boolean to numeric:\n\nTRUE + 6\n\n[1] 7\n\n\nSome operations can’t be performed on certain types of values, e.g. you can’t multiply two strings. If R encoutners two types it can’t work with it will throw an error.\n\n6 * 'this is gonna throw an error'\n\nError in 6 * \"this is gonna throw an error\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWhoops, we got our first error. An error means that R was not able to execute your command and stopped. You won’t get results of the operation if you get an error (which is different from getting a warning! A warning means that something happened that R wants to tell you about: e.g. it encountered something unusual and had to deal with it in a certain way. Warnings will often start with the word Warning. Sometimes it may be a bit confusing because depending on you color settings errors and warnings may be displayed in the same color and it can be a bit scary in the beginning if you get flashing red letter saying something went wrong but don’t worry).\nGenerally an error message will tell you 2 things:\n\nWhere the error happened (in our case in the 6 * 'this is gonna throw an error' line)\nWhat the error is (in our case non-numeric argument to binary operator which just means we used something that is not a number in an operation that requires numbers).\n\nHow exactly an error message is structured largely depends on how well the functions were prepared. If the functions you use are well documented then the error messages should be pretty clear and understandable (which unfortunately is not always the case. This is another reason to document functions well when you start writing your own functions. Be nice to people who start using them!)."
  },
  {
    "objectID": "02basic_data.html",
    "href": "02basic_data.html",
    "title": "Basic objects",
    "section": "",
    "text": "Now we have a very fancy calculator lets see what R can actually be more useful for. One of the most basic but at the same time useful things is the ability to save objects in R. An object can be anything with something assigned to it. You can store a number, text or whole datasets in an object. Another way to think about objects is that they are like pointers. You assign a name to an object which then points to something. Whenever you call the name of the object it refers to whatever it points to. This means that you can easily create objects and then make calculations using the objects. These calculations can then be easily reused no matter what is inside the object as long as you don’t get any errors.\n\n\nCreating an object in R is actually very simple. You just assign whatever you want to store in the object to a name you want to use to refer to your object. There are a few ways in which you can make the assignment: &lt;-, = or sometimes -&gt;. After creating an object it should appear in your global environment. When you want to access whatever is stored in your object you can just use the name of the object.\n\n my_first_object &lt;- 1\n my_first_object\n\n[1] 1\n\n\n\n\n\nYou can also easily update/change an object though as a general rule in R you can’t change an object once you created it. The way to actually change it is to recreate an object by assigning a new value to the same name.\n\nmy_object &lt;- 5\nmy_object\n\n[1] 5\n\nmy_object &lt;- 10\nmy_object\n\n[1] 10\n\n\n\n\n\n\n\nYou can perform basic mathematical operations on objects just like you would make them on values. You can also assign such result to a new object\n\nnumber_1 &lt;- 5\nnumber_2 &lt;- 3\nsum_of_numbers &lt;- number_1 + number_2\nsum_of_numbers\n\n[1] 8\n\n\nIf we now update one of the objects and repeat the same command we will get updated result.\n\nnumber_2 &lt;- 1\nsum_of_numbers &lt;- number_1 + number_2\nsum_of_numbers\n\n[1] 6\n\n\n\n\n\nYou can also make a number of logical operations. These will generally either be comparisons (is something larger than something else?) or logical operation (just like logic 101 operations like and, or, not). These operations result in a boolean value.\nComparisons are fairly straightforward. You make them with &gt;, &lt; and =. One thing to remember is that to test equality you need double equality sign: ==. A single = is used for assigning objects:\n\nsmall_object &lt;- 5\nlarge_object &lt;- 10\nsmall_object &lt;= large_object\n\n[1] TRUE\n\nsmall_object == large_object\n\n[1] FALSE\n\n\nThere are a few things to keep in mind when making comparisons, especially of other types of objects. If you compare a true/false value to a number, R will convert the boolean to 0 or 1. If you compare strings R will start with the first letter and if they are the same it will move on to the second letter and so on. Alphabetical order determines which string is ‘larger’. Note that for some special signs (like polish signs etc) this might get weird. Another thing to note is that R is case sensitive - ‘A’ is not the same as ‘a’.\n\n'hello' &lt; 'world'\n\n[1] TRUE\n\n\nThe other type of logical operations allow you to create more complicated comparisons. Generally & evaluates as TRUE only if both parts of the expression are TRUE. | (or) evaluates to true only if at least one part of the expression is TRUE. The last operator is the not one: !. It turns TRUE into FALSE and FALSE into TRUE.\n\nsomething_true &lt;- TRUE\nsomething_false &lt;- FALSE\n\nsomething_true & something_false\n\n[1] FALSE\n\nsomething_true | something_false\n\n[1] TRUE\n\n!something_true\n\n[1] FALSE\n\n\nThese operation will be especially useful when we will be filtering datasets (e.g. we want only those observations that have age higher than some value and come from a given region).\n\n\n\nOne might think that perhaps you can add strings together? Ultimately maybe something will come out of it? Lets test it out:\n\n'hello' + 'world'\n\nError in \"hello\" + \"world\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nYou can’t add strings together the same way you would with numbers. WWorking with strings will require some functions. We’ll dig into more details on functions in the future but for now you can think about them as operations that take some input, do something with it and produce some output. Using function generally looks like this function_name(arguments). Arguments of the functions are the inputs.\nYou can put together two strings using paste0() and cat() functions. If you want to learn more about them you can look up their documentation with ?paste0 and ?cat. Notice that cat() inserts a space between the words and paaste0 does not.\n\ncat('Hello', 'World')\n\nHello World\n\npaste0('Hello', 'World')\n\n[1] \"HelloWorld\"\n\n\n\n\n\n\nIf you are not sure what type of value you are working with you can use class() to check what type it is.\n\nquote &lt;- 'Me, poor man, my library/was dukedom large enough'\nclass(quote)\n\n[1] \"character\"\n\n\nThere are situations in which you might want to change the class of the object you are working with. A common situation is when you load a dataset and a variable that should be numeric is loaded as character. It is possible to convert one type into another (but remember about the hierarchy, not everything can be converted to any other type). As a general rule all functions for converting types have the form as. so e.g. as.numeric() will convert a value to a numeric one.\n\nmessy_type &lt;- '1'\ncorrect_type &lt;- as.numeric(messy_type)\nclass(correct_type)\n\n[1] \"numeric\"\n\n\nHowever, be mindful that if something can’t be converted to the desired type R will try to coerce it anyway and will produce missing values (coded in R as NA).\n\nmessy_type &lt;- 'Sweet lord, you play me false'\ncorrect_type &lt;- as.logical(messy_type)\ncorrect_type\n\n[1] NA\n\nclass(correct_type)\n\n[1] \"logical\"\n\n\nNotice that the code above does produce an object of class logical but it stores only NA. This can be sometimes tricky because you might not know that something went wrongSome other things will work however - you can convert numeric values into logical ones. 0 will be converted to FALSE and everything else into TRUE.\n\n\n\n\nTest if the expression below is a tautology (true no matter what the truth value of p and q):\n!(p & q) | !(p | !q)\nCreate 5 objects: a, b, c, d, e and assign them values 1, 15, 3, 4.5, 6. Calculate the mean of all these elements and then calculate the sum of squared differences of each value from the mean.\nWhat is the result of comparison TRUE == \"TRUE\"? Why?"
  },
  {
    "objectID": "02basic_data.html#creating-objects",
    "href": "02basic_data.html#creating-objects",
    "title": "Basic objects",
    "section": "",
    "text": "Creating an object in R is actually very simple. You just assign whatever you want to store in the object to a name you want to use to refer to your object. There are a few ways in which you can make the assignment: &lt;-, = or sometimes -&gt;. After creating an object it should appear in your global environment. When you want to access whatever is stored in your object you can just use the name of the object.\n\n my_first_object &lt;- 1\n my_first_object\n\n[1] 1"
  },
  {
    "objectID": "02basic_data.html#updating-objects",
    "href": "02basic_data.html#updating-objects",
    "title": "Basic objects",
    "section": "",
    "text": "You can also easily update/change an object though as a general rule in R you can’t change an object once you created it. The way to actually change it is to recreate an object by assigning a new value to the same name.\n\nmy_object &lt;- 5\nmy_object\n\n[1] 5\n\nmy_object &lt;- 10\nmy_object\n\n[1] 10"
  },
  {
    "objectID": "02basic_data.html#basic-operations-on-objects",
    "href": "02basic_data.html#basic-operations-on-objects",
    "title": "Basic objects",
    "section": "",
    "text": "You can perform basic mathematical operations on objects just like you would make them on values. You can also assign such result to a new object\n\nnumber_1 &lt;- 5\nnumber_2 &lt;- 3\nsum_of_numbers &lt;- number_1 + number_2\nsum_of_numbers\n\n[1] 8\n\n\nIf we now update one of the objects and repeat the same command we will get updated result.\n\nnumber_2 &lt;- 1\nsum_of_numbers &lt;- number_1 + number_2\nsum_of_numbers\n\n[1] 6\n\n\n\n\n\nYou can also make a number of logical operations. These will generally either be comparisons (is something larger than something else?) or logical operation (just like logic 101 operations like and, or, not). These operations result in a boolean value.\nComparisons are fairly straightforward. You make them with &gt;, &lt; and =. One thing to remember is that to test equality you need double equality sign: ==. A single = is used for assigning objects:\n\nsmall_object &lt;- 5\nlarge_object &lt;- 10\nsmall_object &lt;= large_object\n\n[1] TRUE\n\nsmall_object == large_object\n\n[1] FALSE\n\n\nThere are a few things to keep in mind when making comparisons, especially of other types of objects. If you compare a true/false value to a number, R will convert the boolean to 0 or 1. If you compare strings R will start with the first letter and if they are the same it will move on to the second letter and so on. Alphabetical order determines which string is ‘larger’. Note that for some special signs (like polish signs etc) this might get weird. Another thing to note is that R is case sensitive - ‘A’ is not the same as ‘a’.\n\n'hello' &lt; 'world'\n\n[1] TRUE\n\n\nThe other type of logical operations allow you to create more complicated comparisons. Generally & evaluates as TRUE only if both parts of the expression are TRUE. | (or) evaluates to true only if at least one part of the expression is TRUE. The last operator is the not one: !. It turns TRUE into FALSE and FALSE into TRUE.\n\nsomething_true &lt;- TRUE\nsomething_false &lt;- FALSE\n\nsomething_true & something_false\n\n[1] FALSE\n\nsomething_true | something_false\n\n[1] TRUE\n\n!something_true\n\n[1] FALSE\n\n\nThese operation will be especially useful when we will be filtering datasets (e.g. we want only those observations that have age higher than some value and come from a given region).\n\n\n\nOne might think that perhaps you can add strings together? Ultimately maybe something will come out of it? Lets test it out:\n\n'hello' + 'world'\n\nError in \"hello\" + \"world\": argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nYou can’t add strings together the same way you would with numbers. WWorking with strings will require some functions. We’ll dig into more details on functions in the future but for now you can think about them as operations that take some input, do something with it and produce some output. Using function generally looks like this function_name(arguments). Arguments of the functions are the inputs.\nYou can put together two strings using paste0() and cat() functions. If you want to learn more about them you can look up their documentation with ?paste0 and ?cat. Notice that cat() inserts a space between the words and paaste0 does not.\n\ncat('Hello', 'World')\n\nHello World\n\npaste0('Hello', 'World')\n\n[1] \"HelloWorld\""
  },
  {
    "objectID": "02basic_data.html#changing-types",
    "href": "02basic_data.html#changing-types",
    "title": "Basic objects",
    "section": "",
    "text": "If you are not sure what type of value you are working with you can use class() to check what type it is.\n\nquote &lt;- 'Me, poor man, my library/was dukedom large enough'\nclass(quote)\n\n[1] \"character\"\n\n\nThere are situations in which you might want to change the class of the object you are working with. A common situation is when you load a dataset and a variable that should be numeric is loaded as character. It is possible to convert one type into another (but remember about the hierarchy, not everything can be converted to any other type). As a general rule all functions for converting types have the form as. so e.g. as.numeric() will convert a value to a numeric one.\n\nmessy_type &lt;- '1'\ncorrect_type &lt;- as.numeric(messy_type)\nclass(correct_type)\n\n[1] \"numeric\"\n\n\nHowever, be mindful that if something can’t be converted to the desired type R will try to coerce it anyway and will produce missing values (coded in R as NA).\n\nmessy_type &lt;- 'Sweet lord, you play me false'\ncorrect_type &lt;- as.logical(messy_type)\ncorrect_type\n\n[1] NA\n\nclass(correct_type)\n\n[1] \"logical\"\n\n\nNotice that the code above does produce an object of class logical but it stores only NA. This can be sometimes tricky because you might not know that something went wrongSome other things will work however - you can convert numeric values into logical ones. 0 will be converted to FALSE and everything else into TRUE."
  },
  {
    "objectID": "03data_types.html",
    "href": "03data_types.html",
    "title": "Types of data",
    "section": "",
    "text": "So far we have worked only with single values or objects storing only one value. However, usually you want to work with whole sets of values like variables or whole datasets. There is a number of types of data you can encounter in R which allow you to do that. A fairly easy way to orient yourself in the different types of data is:"
  },
  {
    "objectID": "03data_types.html#vectors",
    "href": "03data_types.html#vectors",
    "title": "Types of data",
    "section": "Vectors",
    "text": "Vectors\nThe most basic type of data is a vector. Vectors can store any number of values of the same type in 1 dimension. You can create a vector using c() function.\n\nmy_very_first_vector &lt;- c(1,2,3)\nmy_very_first_vector\n\n[1] 1 2 3\n\n\nVectors are indexed: they have a first, second value etc. This means that you can access part of a vector -subset them. Subsetting is accomplished with []. You can also subset a range of values from a vector wirh [:]:\n\nlong_vector &lt;- c(1,2,3,4,5,6,7,8,9,10)\nlong_vector[3:5]\n\n[1] 3 4 5\n\n\nIf you try to put different types of values into one vector R will convert the types to a matching one. This is especially important when due to some mistake/error a single value of a different type gets lost in some other variable. Just a single value will trigger the whole variable to be converted!\n\nmy_vector &lt;- c(1, TRUE, 'some text')\nmy_vector\n\n[1] \"1\"         \"TRUE\"      \"some text\"\n\nclass(my_vector)\n\n[1] \"character\"\n\n\nYou can get a brief summary of a given vector with summary(). It will give slightly different information depending on what type of values is stored in a given vector:\n\nsummary(long_vector)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00 \n\nsummary(my_vector)\n\n   Length     Class      Mode \n        3 character character \n\n\nYou can make pretty much the same operations on vectors as on single values. One of the great features of R is that by default it will make operations element wise - if you try to add two vectors together then the first element from vector 1 will be added to first element of vector 2 and so on (the fancy name for this is vectorization).\n\nnumbers &lt;- c(1,2,3,4,5)\nnumbers2 &lt;- c(6,7,8,9,10)\nnumbers + numbers2\n\n[1]  7  9 11 13 15\n\n\nIf the vectors have different lengths then R will start to recycle values from the shorter vectors. But it will output a warning if the length of one vector is not a multiple of the other vector.\n\nshort_v &lt;- c(1,2,3)\nlong_v &lt;- c(1,2,3,4,5)\nshort_v + long_v\n\nWarning in short_v + long_v: długość dłuszego obiektu nie jest wielokrotnością\ndługości krótszego obiektu\n\n\n[1] 2 4 6 5 7\n\n\nIf you want to join two vectors together you can do it in 2 ways: the first one is with c() just like creating a new vector (and in fact it will simply create a new vector!). The other one is with append(). The first argument is the vector you want to append to and the second argument is the vector you want to append. append() also allows you to specify where to append the second vector with after argument that requires an index so you can put it e.g. inside the first vector rather than at the end\n\nappend(long_v, short_v, after = 2)\n\n[1] 1 2 1 2 3 3 4 5\n\n\nAnother thing about vectors is that they can be named: each element can have a name. This can be especially useful e.g. when the vector is a result of some statistical operations and you want to make it easier to understand which number means what (e.g. you want to put together the mean, median and mode in 1 vector). You can add names to elements in a vector simply with a =:\n\nnamed_vector &lt;- c(\"element one\" = 1, \"element two\" = 2)\nnamed_vector\n\nelement one element two \n          1           2 \n\n\nBefore we move on to factors lets introduce a few functions that can be useful for creating vectors:\n\nrep() function allows you to repeat a given value or a vector n times. You can create large vectors with it easily:\n\nlong_v &lt;- rep(c(1,2,3), 50)\nsummary(long_v)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       2       2       3       3 \n\n\nseq() allows you to create a sequence of numbers from some number to some number. You can either specify how long the sequence is to be and R will figure out the distances between numbers (lenght.out argument) or you can specify the distances between numbers with by argument and R will figure out the length of a resulting vector.\n\nseq_v &lt;- seq(0,1, length.out = 100)\nsummary(seq_v)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.25    0.50    0.50    0.75    1.00 \n\n\nrnorm() allows you to draw random numbers from a normal distribution with a specified mean and standard deviation (there is actually a whole family of drawing numbers from different distributions e.g. rbinom() for drawing from binomial distribution or rbeta() for drawing from beta distribution):\n\nnorm_v &lt;- rnorm(100, mean = 5, sd = 2)\nsummary(norm_v)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.9532  3.6011  5.1754  5.0126  6.4056 10.0213"
  },
  {
    "objectID": "03data_types.html#factors",
    "href": "03data_types.html#factors",
    "title": "Types of data",
    "section": "Factors",
    "text": "Factors\nFactors are much like vectors except that they are used for storing categorical values - they have levels. You can store variables such as country or experimental condition of participants in a factor. You can create a factor by calling factor() and passing it a vector as an argument.\n\nmy_vector &lt;- c('a', 'b', 'a', 'b')\nmy_factor &lt;- factor(my_vector)\nmy_factor\n\n[1] a b a b\nLevels: a b\n\n\nFactors can also have ordered levels. You can make an ordered factor by setting ordered = T argument when creating a factor. Notice how the output looks different now: it includes information on the order.\n\nordered_vec &lt;- c('low', 'high', 'high', 'low', 'low')\nordered_fac &lt;- factor(ordered_vec, ordered = T)\nordered_fac\n\n[1] low  high high low  low \nLevels: high &lt; low\n\n\nYou can also manually set the levels of a factor. You can do it when creating the factor. Notice that for ordered factor the order in which you pass the levels will determine the order of levels in the factor.\n\nordered_vec &lt;- c('low', 'high', 'high', 'low', 'low')\nordered_fac &lt;- factor(ordered_vec, ordered = T, levels = c('low', 'high', 'medium'))\nordered_fac\n\n[1] low  high high low  low \nLevels: low &lt; high &lt; medium"
  },
  {
    "objectID": "03data_types.html#matrices",
    "href": "03data_types.html#matrices",
    "title": "Types of data",
    "section": "Matrices",
    "text": "Matrices\nMatrices are a bit like vectors but they have two dimensions. They have rows and columns but treat them in the same way. Because of this they can store only one type of values (much like vectors). You can create a matrix from scratch with the matrix() function. This function takes a vectors of values as its input (these are the values we will fill our matrix with) and additional information on how the matrix has to look - how many columns and rows it should have and whether to fill the matrix with values by rows or columns\n\nnumbered_vector &lt;- c(1,2,3,4,5,6,7,8,9)\nmy_matrix &lt;- matrix(numbered_vector, nrow = 3, ncol = 3)\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nYou can also create a matrix by ‘glueing’ vectors together. You can bind them either as rows (rbind() function) or by columns (cbind() function). Notice that the names of the vectors will be used either as names of rows or columns.\n\nvec1 &lt;- c(1,2,3)\nvec2 &lt;- c(4,5,6)\ncbind(vec1, vec2)\n\n     vec1 vec2\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nSince we have two dimensions subsetting matrices can work both on rows and columns. The general idea is still the same but we have to specify whether we are subsetting rows or columns. Rows always come first, columns second separated by a comma like this matrix[rows,columns]. You can select ranges of rows or columns just like in a vector.\n\nmy_matrix[2,2]\n\n[1] 5\n\n\nIf you want to select all rows or columns you can leave the space blank. Remember to keep the comma though!\n\nmy_matrix[,3]\n\n[1] 7 8 9\n\n\nOperations on matrices follow similar rules like operations on vectors - they are element-wise by default (note that they are not your classical linear algebra operations!). E.g. if you multiply a matrix by a vector each row from the matrix will be multiplied by a given element from the vector (1st row by 1st value etc):\n\nmy_matrix * vec1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    4   10   16\n[3,]    9   18   27"
  },
  {
    "objectID": "03data_types.html#data-frames",
    "href": "03data_types.html#data-frames",
    "title": "Types of data",
    "section": "Data frames",
    "text": "Data frames\nIn a day to day analysis you will likely work with data frames most of the time. A data frame is like a matrix in that it has rows and columns but can store different types of values in each column (so that e.g. you can have some variables that are numeric and others that are text). A different way of thinking about data frames is as a list of vectors of the same length with each vector representing a different variable. Each row represents a different observation (e.g. participant).\nYou can create a data frame with data.frame() function passing all the variables as arguments. Lets create 3 vectors: author, title and year.\n\nauthor &lt;- c('Allport', 'Heider', 'Lewin', 'Allport', 'Heider')\ntitle &lt;- c('Nature of Prejudice', 'Psychology of interpersonal relations',\n           'Principles of Topological Psychology', 'Psychology of Rumor', 'The life of a psychologist: An autobiography')\nyear &lt;- c(1954, 1958, 1936, 1947, 1983)\n\npsych_books &lt;- data.frame(author, title, year)\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n\n\nSubsetting data frames works the same way as matrices. You can subset both on rows and columns. An important thing to remember (and one of the reasons a lot of people switch to tibbles which are kind of data frames+. We’ll get to tibbles some time in the future) is that if you subset a single column the result will be a vector and not a dataframe. This sometimes is annoying if you are designing something that is supposed to work on data frames specifically.\nThere is one additional way of subsetting a data frame. Subsetting variables based on their position is tiresome because we rarely remember the order of all the columns (especially as our data frames get bigger). You can select a single variable using $:\n\npsych_books$author\n\n[1] \"Allport\" \"Heider\"  \"Lewin\"   \"Allport\" \"Heider\" \n\n\nUsing the $ operator reflects a way of thinking about datasets that is pretty common: data frames are ordered collections of variables and each variable has its name. You can also use $ to create new variables. Just assign a vector of values to a new name in your dataframe:\n\npsych_books$discipline &lt;- c('intergroup relations', 'social psychology',\n                            'general psychology', 'social psychology',\n                            'biography')\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n            discipline\n1 intergroup relations\n2    social psychology\n3   general psychology\n4    social psychology\n5            biography\n\n\nRemember how we talked about element-wise operations on vectors? You can leverage it to easily create new variables that are results of operations on other variables. Thanks to this you can add a whole new variable that is a result of a mathematical operation in just one line (imagine adding a variable that is a sum of all points from a quiz for each student). Here’s an example if we wanted to calculate how many years ago each book from our data frame was published:\n\npsych_books$book_age &lt;- 2022 - psych_books$year\npsych_books\n\n   author                                        title year\n1 Allport                          Nature of Prejudice 1954\n2  Heider        Psychology of interpersonal relations 1958\n3   Lewin         Principles of Topological Psychology 1936\n4 Allport                          Psychology of Rumor 1947\n5  Heider The life of a psychologist: An autobiography 1983\n            discipline book_age\n1 intergroup relations       68\n2    social psychology       64\n3   general psychology       86\n4    social psychology       75\n5            biography       39\n\n\nYou can also subset data frames based on condition. Lets say we want to find out which psychology books in our dataset are really old, say above 70. We can subset psych_books using [] but we need to add one more thing to specify our condition - we need to tell R which rows are the ones that fulfill our condition. We can do it with which(). It needs the condition as argument and will return numbers of rows from the data frame that fulfill the condition. We just need to put which() inside our subsetting to get the rows we want:\n\npsych_books[which(psych_books$book_age &gt; 70),]\n\n   author                                title year         discipline book_age\n3   Lewin Principles of Topological Psychology 1936 general psychology       86\n4 Allport                  Psychology of Rumor 1947  social psychology       75"
  },
  {
    "objectID": "03data_types.html#lists",
    "href": "03data_types.html#lists",
    "title": "Types of data",
    "section": "Lists",
    "text": "Lists\nLists are the final type of basic data in R we will discuss here. They are the most versatile ones - they can store anything inside of them: single values, vectors, matrices, dataframes or even other lists! So they have 1 dimension but can store anything inside them. One important feature of lists is that they are ordered: you can access their elements by position. So you can think of lists as collections of objects but there aren’t really limits to what these objects are (in fact data frames are very specific lists: they are collections of variables that have the same length and form a nice rectangular table). Lists are created with list(). Lets create a list of the plants in a house along with a value storing information on how many days ago did we last water them:\n\nlist_of_objects &lt;- list(\n  plants = c('Calathea', 'Chamedora', 'Pilea', 'Philodendron'),\n  days_since_watering = 5\n)\nlist_of_objects\n\n$plants\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\n$days_since_watering\n[1] 5\n\n\nYou might encounter lists if you need to store a number of different things together. E.g. results of many statistical analyses are stored in lists because they might contain both information about the model, data and results. In fact as you dive deeper into R you will start to encounter more and more lists because they are very versatile.\nYou can access objects stored in lists in a few ways. You can use the [] you used for all other types of data. An important feature of this type of subsetting is that the result will always be a list (even if it has only 1 element). The other option is to use double square brackets [[]]. This will extract the object inside a list so the result won’t be a list (you can think of it as ‘getting deeper’ into the list to extract the exact element you want).\n\nlist_of_objects[1]\n\n$plants\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\nlist_of_objects[[1]]\n\n[1] \"Calathea\"     \"Chamedora\"    \"Pilea\"        \"Philodendron\"\n\n\nIf the elements in your list are named you can also use $ to extract them.\n\nlist_of_objects$days_since_watering\n\n[1] 5"
  },
  {
    "objectID": "04loops_conditionals.html",
    "href": "04loops_conditionals.html",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Now you know how to create different kinds of objects and how to perform simple operations with them. However, very often you want to add more control over how operations are ran in R. You might want to execute a command only if a condition is satisfied. Or you might want to make the same operations for a number of elements. These are the kinds of situations for which you want to use flow control. What this refers to is basically altering how the code is executed. In a regular situations all commands from your script are executed from the first line all the way down to the last line. Flow control alters that either by specifying condiotinal statements that tell R to execute a given chunk of code only if a condition is met or by using loops that repeat a given chunk of code.\n\n\nAnother way conditional statements are referred to which may be more intuitive are if else statements. They allow you to tell R to execute given chunk of code if a condition is met and to do something else if the condition is not met.\nThe general logic of conditional statements looks like this:\n\nif (condition) {\n  Do this\n  And do this\n}\n\nA single if statement can have multiple conditions chained together with | and & operators. So, for example\n\nx &lt;- 5\ny &lt;- -5\n\nif (x &gt; 0 & y &lt; 0) {\n  print(\"Hooray!\")\n}\n\n[1] \"Hooray!\"\n\n\nIn many situations you want to state what is to be done if a condition is met and what to do otherwise. This turns your statement into an if else one. The only difference is that after the if statement you add else and specify what to do then in curly brackets. With this knowledge you can already create the rules for a simple game like paper, rock, scissors!\n\n#set the choice for each player\nplayer1 &lt;- 'scissors'\nplayer2 &lt;- 'rock'\n\n#define an if statement that outputs the result of the game\nif (player1 == player2) {\n  print('draw')\n} else if ((player1 == 'scissors' & player2 == 'paper') |\n           (player1 == 'paper' & player2 == 'rock') |\n           (player1 == 'rock' & player2 == 'scissors')) {\n  print('player 1 wins')\n} else if ((player2 == 'scissors' & player1 == 'paper') |\n           (player2 == 'paper' & player1 == 'rock') |\n           (player2 == 'rock' & player1 == 'scissors')) {\n  print('player 2 wins')\n} else {\n  print('these are not allowed moves')\n}\n\n[1] \"player 2 wins\"\n\n\nTake a moment to study the code above. Notice what kinds of conditions are included in that statement. When writing an if statement it’s a good idea to consider all possible situations and how your if statement maps to them. In a paper, rock, scissors game you can have 3 outcomes: both players choose the same option (a draw), player 1 wins or player 2 wins. Notice that the code above includes also a fourth options specified in the last else statement. What if someone makes a typo and writes rook instead of rock? That last else statement safeguards us for such situations. If we didn’t include it and someone made a type then our if else statement wouldn’t produce anything. You can play around with different values of player1 and player2 to see the results.\nOne more thing about if else statements: in many situations it is a good idea to give some thought to what exactly a given statement is supposed to do and how large the statement needs to be. A good example is an if statement that is supposed to run some check (e.g. make sure that we are working with a numeric value) and stop execution if it detects a problem. Imagine a situation in which we want to do some calculations on numbers and want to make sure that we are indeed working with numeric values. you could design an if else statement that would do it:\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n} else {\n  x + y\n}\n\nError in eval(expr, envir, enclos): This is not a number!\n\n\nTake a moment to look at the code above. Do you think it is good? It certainly gets the job done. Do you think it could be simplified?\nIn fact the else part is redundant in this case. The if statement runs the check on x and y and stops execution of the code if any of them is not numeric. If both values are numeric the execution of code simply proceeds. In this case adding an else statement makes the code harder to read (and imagine what would happen if we had to perform a number of checks like this! We would need a lot of if else statements that would make everything even less clear). The code below does the same thing as the if else statement above but is more clear.\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n}\n## Error in eval(expr, envir, enclos): This is not a number!\nx + y\n## Error in x + y: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\n\n\nAnother way of controlling the flow of your code is by repeating a given chunk of code. There are two basic ways to do that: repeat something a number of times or keep repeating until some condition is met. The first way is called a for loop and the second one a while loop.\n\n\nBefore we make our first for loop lets take a moment to see when a for loop is not needed. Recall again that a lot of things in R are vectorized. This means that operations on vectors are conducted element-wise. Thanks to this if you want to e.g. add 5 to each value stored in a numeric vector (in the language of a for loop: for every element of a vector, add 5 to it) you can just write vector_name + 5. No need for more complicated, explicit repetition. However, there are situations in which you have to make an explicit for loop to repeat something n times. The general structure of a for loops looks like this:\n\nfor (i in object) {\n  Do this to each element\n}\n\nIt’s worth keeping in mind what the i in the for loop is. In the example above i will be every consecutive element of object. However we could do a similar thing with:\n\nfor (i in 1:length(object)) {\n  do this to object[i]\n}\n\nNow each i is a number from 1 to thew length of object and we access each element of object by using a proper (ith) index. Which way of running a for loop you choose might depend on the context. looping explicitly over elements of an object rather than indexes can be more intuitive but imagine you don’t want to do something to every element of an object but only to to a subset (e.g. from 3rd inwards). Doing it with indexes is easier. Generally the best approach is to think what you need first and write code code second, not the other way around.\nLets say we want to get a geometric sequence which starts from 1 and in which each next number is the previous number times 1.5. We can easily create the first 20 numbers from that sequence with a for loop:\n\nv &lt;- c(1)\n\nfor (i in 1:20)  {\n  v[i+1] &lt;- v[i]*1.5\n}\nv\n\n [1]    1.00000    1.50000    2.25000    3.37500    5.06250    7.59375\n [7]   11.39062   17.08594   25.62891   38.44336   57.66504   86.49756\n[13]  129.74634  194.61951  291.92926  437.89389  656.84084  985.26125\n[19] 1477.89188 2216.83782 3325.25673\n\n\nLets look at another example. sThere is a dataset available in R on airquality in New York City called airquality. It stores information on ozone, sun, wind and temperature from 5 months. One of the things that might be of interest when looking at the dataset is what was the average value of each of the variables informing on airquality. Lets first look at the dataset:\n\ndata(\"airquality\")\nd &lt;- airquality\nd &lt;- na.omit(d)\n\nhead(d)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n\nThe first 4 columns are of interest here. We can loop over these columns and for each calculate their mean and append it to the vector of results (which we need to create before starting the loop). Just to be sure which value is the mean of which variable we can name the elements of the resulting vector with names of the columns after the for loop:\n\nresults &lt;- c()\n\nfor (v in d[,1:4]) {\n  m &lt;- mean(v)\n  results &lt;- c(results, m)\n}\nnames(results) &lt;- colnames(d[,1:4])\nresults\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\n\n\n\nWhile loops will keep executing a given chunk of code as long as some condition is met. They aren’t very common in R, at least not until you start building your own algorithms or simulations from scratch. However, it’s worth knowing what they are in case you encounter them.\nWe can use a while loop to make a very simple simulation. Lets say we want to see how temperatures change from a given temperature (lets say 20 degrees Celsius) across time and that we represent time by some random change from each previous temperature. We can create a vector with such predicted temperatures and see how long it takes for it to reach a certain level (lets say 30 degrees Celsius). We represent the change by adding a random value from a normal distribution with mean = .05 and standard deviation = .5 (this is what the rnorm(1,.05,.5) does). The while loop would look something like this: We first create the initial value and a vector to store all temperatures and next we keep adding the random value to our temperature and storing all temperatures until it reaches 30. The last line tells R to plot all the temperatures as a line plot. This is of course a very, very, very simplistic simulation (temperatures don’t change in such a simple way) but it works to show you the idea behind while loops. We can then calculate e.g. how long it took for the temperature to reach a certain level.\n\nC &lt;- 20\nresults &lt;- c(20)\nwhile (C &lt; 30) {\n  C &lt;- C + rnorm(1,.05,.5)\n  results &lt;- c(results, C)\n}\n\nplot(results, type = 'line', lwd = 2, col=4, xlab = \"days\", ylab = \"temperature\")\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\n\n\n\n\nBecause while loops do not have a fixed number of iteration they can potentially run infinitely. This is usually not something we want so it’s a good idea to make sure that your while loop eventually stops. In case you do get stuck in an infinite loop you can press Esc in your console and this should make RStudio stop the loop by force.\nTruth is while loops are not common in R. You will rarely find yourself in situation where you need to perform some actions while a given condition is true (e.g. keep a program running until a user presses exit; keep displaying a board of a game until a player makes a move). However, it’s still good to know what while loops are so that you will know one when you see it.\n\n\n\n\nThere is a special family of functions in R that makes working with for loops a bit easier. These functions let you specify what to loop over and what function to apply to each element but in a function rather than a whole loop with all the curly brackets and stuff.\nThe reason why this is a whole family of functions is that you can iterate in various ways and you can get the output in different formats. There are more functions in the family but the general ones are:\n\nlapply() - loops over elements of a vector/list and returns a list\nsapply() - same as lapply but tries to simplify the result to a vector or matrix\napply() - used for looping over 2 dimensional structures - it lets you specify if you want to loop over rows or columns\ntapply() - same as apply but lets you split the object you are looping over based on some factor (e.g. imagine you want to calculate the mean value of your dependent variable for each experimental condition).\n\nLets see some of these in action.\n\n\nImagine you are working with a list in R. You want to get information on how many elements each object in the list has. sapply makes it very easy:\n\nmy_list &lt;- list(\n  1:50,\n  sample(300, 5),\n  c(\"random\", \"vector\")\n)\n\nsapply(my_list, length)\n\n[1] 50  5  2\n\n\n\n\n\nRemember the airquality dataset and calculating the mean of each numeric variable? We can achieve the same thing with apply() instead of a for loop:\n\napply(d[,1:4], 2, mean)\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\nNotice that the means calculated above are global means from the entire dataset. What is probably much more sensible is a mean for each month. There is one additional trick needed here. Tapply won’t allow us to split a number of columns by some vector and perform a given operation on each of the columns. That’s because tapply works on vectors. In order to get monthly means for all 4 columns we need to combine apply with tapply. What we need to do is start with apply and loop over the 4 columns of interest and for each of them use tapply that will split a given column by month and calculate the means. Combining functions can get us really far if only we give some thought to what each function does (including what are its inputs and outputs) and what we really need to do. notice that tapply() is preceded by function(x). We’ll learn more about them when looking at functions but for now think about it as an expression that allows us to specify some function that takes x as its main argument (in this case these are going to be columns from the airquality dataset) and apply some custom function to it.\n\napply(d[,1:4], 2, function(x) tapply(x, d$Month, mean))\n\n     Ozone  Solar.R      Wind     Temp\n5 24.12500 182.0417 11.504167 66.45833\n6 29.44444 184.2222 12.177778 78.22222\n7 59.11538 216.4231  8.523077 83.88462\n8 60.00000 173.0870  8.860870 83.69565\n9 31.44828 168.2069 10.075862 76.89655\n\n\nOne important thing about apply functions is that they are generally faster than explicit for loops. Same thing goes for vectorized code as well - it’s faster than a for loop. We can make a simple comparison by using the microbenchmark package to make the same thing with a for loop and apply() function. We’ll save it as bench\nNow if we look at the results:\n\nbench\n\nUnit: milliseconds\n                                               expr    min      lq     mean\n for (i in 1:ncol(df)) {     print(mean(df[, i])) } 3.1195 3.53700 4.370040\n                                 apply(df, 2, mean) 3.8517 4.69315 7.278746\n  median      uq     max neval\n 3.82005 5.28135  7.1903   100\n 5.29420 7.06480 69.9919   100\n\n\nYou can see that apply() is much faster than a for loop.\n\n\n\n\n\nTry to code the logic of assigning points to players of a prisoner dilemma with a given matrix:\n\n\n\nexercise 1\n#' we need to specify outcomes based on Prisoner 1 and prisoner 2 choices.\n#' We'll need a series of ifelse statements to code each of the 4 possible outcomes.\n#' Lets define the choices as choice1 and choice2 and store infoirmation on which choice each on of these is.\n#' Next lets define some choices by the prisoners.\n#' We need to define if statements that will cover all possible combinations of choices for prisoners 1 and 2.\n#' We can do it by:\n#' 1. Creating an if else statement for the choice of prisoner 1 \n#' 2. inside that if else statement create if else statements for prisoner 2.\n#' 3. Assign vector of results depending on ech combination of prisoner 1 and 2 choices.\n#' You can copy this code and change values of Prisoner1 and Prisoner2 to choice1 or choice2 to see if the results change.\n\nchoice1 &lt;- \"Stay silent\"\nchoice2 &lt;- \"Betray and testify\"\nPrisoner1 &lt;- choice1\nPrisoner2 &lt;- choice2\n\nif(Prisoner1 == choice1) {\n  if(Prisoner2 == choice1) {\n    result &lt;-  c(P1_result = 1, P2_result = 1)\n  } else {\n    result &lt;-  c(P1_result = 0, P2_result = 10)\n  }\n} else {\n  if(Prisoner2 == choice1) {\n    result &lt;-  c(P1_result = 10, P2_result = 0)\n  } else {\n    result &lt;-  c(P1_result = 6, P2_result = 6)\n  }\n}\nresult\n\n\nCreate a for loop that will print out the first 50 numbers from the Fibonacci sequence\n\n\nexercise 2\n#' The fibonacchi sequence atarts with a 0 and 1 and then each next number is the \n#' sum of previous 2 numbers. This means we will need to first define the 2 first numbers\n#' and then run a loop that for every iteration i will print the sum of ith-1 and ith-2\n#' numbers from the sequence. Since the first 2 numbers are defined \n#' we can loop from 3 to 50 (that's hoew many numbers we are to print)\n\nx &lt;- c(0,1)\n\nfor(i in 3:50) {\n  x_i &lt;- sum(x[i-1], x[i-2])\n  x &lt;- c(x, x_i)\n  print(x_i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 5\n[1] 8\n[1] 13\n[1] 21\n[1] 34\n[1] 55\n[1] 89\n[1] 144\n[1] 233\n[1] 377\n[1] 610\n[1] 987\n[1] 1597\n[1] 2584\n[1] 4181\n[1] 6765\n[1] 10946\n[1] 17711\n[1] 28657\n[1] 46368\n[1] 75025\n[1] 121393\n[1] 196418\n[1] 317811\n[1] 514229\n[1] 832040\n[1] 1346269\n[1] 2178309\n[1] 3524578\n[1] 5702887\n[1] 9227465\n[1] 14930352\n[1] 24157817\n[1] 39088169\n[1] 63245986\n[1] 102334155\n[1] 165580141\n[1] 267914296\n[1] 433494437\n[1] 701408733\n[1] 1134903170\n[1] 1836311903\n[1] 2971215073\n[1] 4807526976\n[1] 7778742049\n\n\nGiven the iris dataframe (you can load it with data(\"iris\") loop over all of its columns and calculate the mean of every numeric column\n\n\nexercise 3\n#' What we are supposed to do is:\n#' 1. load the iris dataset\n#' 2. loop over numeric columns (there is 1 column that is not numeric)\n#' 3. for each column calculate its mean\n\ndata(\"iris\")\n\nresults &lt;- c()\nfor(i in 1:ncol(iris)) {\n  if(is.numeric(iris[,i])) {\n    results &lt;- c(results, mean(iris[,i]))\n  }\n}\n\n#if we want to format the names of values in the vector we can extract the names of numeric columns.\n#The sapply function returns a vector of logical values with TRUE for every numeric column\n#in the dataset. We can use that vector to subset the dataset and get only numeric columns.\nnames(results) &lt;- colnames(iris[,sapply(iris, is.numeric)])\nresults\n\n\n#We can also shorten the for loop and use apply instead. \n#THis time we'll need to loop over the dataset with numeric columns selected\napply(iris[,sapply(iris, is.numeric)], 2, mean)\n\n\nCreate a vector of numbers randomly drawn from a normal distribution with a mean of 2 and standard deviation 0.5. Loop over a sequence of 100 numbers from the minimum to the maximum of the vector you created. For each value in the sequence calculate the sum of squared differences from each value in the vector. Store all the sums in a new vector. Which value in the sequence minimizes the sum of squared distances?\n\n\nexercise 4\n#' What the exercise asks us to do is:\n#' 1. Create a vector of numbers drawn from normal distribution wiht mean = 2 and sd = .5. We can use rnorm() for that.\n#' 2. loop over a sequence of numbers from the minimum to maximum of our vector and for each value calculate the sum of squared distances of each number from a given number in the sequence. This will require:\n#' 2a. Define a vector to store the sums of squared distances\n#' 2b. Start the for loop over seq(min(v1), max(v1), .1). You can change the distance in seq from .1 to something else if you want to see with different resolution\n#' 2c. For each i in the sequence calculate the sum of squared differences and append it to vector\n#' 3. Preferrably name the vector with results\n#' 4. Find the minimum number\n\n#the number we get as mimizing the sum of squared differences is certainly \n#going to be the one closest to the mean (you can check it with mean(v1) if you want)!\n\nv1 &lt;- rnorm(1000, 2, .5)\nsums_squared &lt;- c()\n\nfor(i in seq(min(v1), max(v1), .1)) {\n  diff_i &lt;- sum((i - v1)^2)\n  sums_squared &lt;- c(sums_squared, diff_i)\n}\n\nto_name &lt;- seq(min(v1), max(v1), .1)\nnames(sums_squared) &lt;- to_name\n\nsums_squared[which.min(sums_squared)]"
  },
  {
    "objectID": "04loops_conditionals.html#conditional-statements",
    "href": "04loops_conditionals.html#conditional-statements",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Another way conditional statements are referred to which may be more intuitive are if else statements. They allow you to tell R to execute given chunk of code if a condition is met and to do something else if the condition is not met.\nThe general logic of conditional statements looks like this:\n\nif (condition) {\n  Do this\n  And do this\n}\n\nA single if statement can have multiple conditions chained together with | and & operators. So, for example\n\nx &lt;- 5\ny &lt;- -5\n\nif (x &gt; 0 & y &lt; 0) {\n  print(\"Hooray!\")\n}\n\n[1] \"Hooray!\"\n\n\nIn many situations you want to state what is to be done if a condition is met and what to do otherwise. This turns your statement into an if else one. The only difference is that after the if statement you add else and specify what to do then in curly brackets. With this knowledge you can already create the rules for a simple game like paper, rock, scissors!\n\n#set the choice for each player\nplayer1 &lt;- 'scissors'\nplayer2 &lt;- 'rock'\n\n#define an if statement that outputs the result of the game\nif (player1 == player2) {\n  print('draw')\n} else if ((player1 == 'scissors' & player2 == 'paper') |\n           (player1 == 'paper' & player2 == 'rock') |\n           (player1 == 'rock' & player2 == 'scissors')) {\n  print('player 1 wins')\n} else if ((player2 == 'scissors' & player1 == 'paper') |\n           (player2 == 'paper' & player1 == 'rock') |\n           (player2 == 'rock' & player1 == 'scissors')) {\n  print('player 2 wins')\n} else {\n  print('these are not allowed moves')\n}\n\n[1] \"player 2 wins\"\n\n\nTake a moment to study the code above. Notice what kinds of conditions are included in that statement. When writing an if statement it’s a good idea to consider all possible situations and how your if statement maps to them. In a paper, rock, scissors game you can have 3 outcomes: both players choose the same option (a draw), player 1 wins or player 2 wins. Notice that the code above includes also a fourth options specified in the last else statement. What if someone makes a typo and writes rook instead of rock? That last else statement safeguards us for such situations. If we didn’t include it and someone made a type then our if else statement wouldn’t produce anything. You can play around with different values of player1 and player2 to see the results.\nOne more thing about if else statements: in many situations it is a good idea to give some thought to what exactly a given statement is supposed to do and how large the statement needs to be. A good example is an if statement that is supposed to run some check (e.g. make sure that we are working with a numeric value) and stop execution if it detects a problem. Imagine a situation in which we want to do some calculations on numbers and want to make sure that we are indeed working with numeric values. you could design an if else statement that would do it:\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n} else {\n  x + y\n}\n\nError in eval(expr, envir, enclos): This is not a number!\n\n\nTake a moment to look at the code above. Do you think it is good? It certainly gets the job done. Do you think it could be simplified?\nIn fact the else part is redundant in this case. The if statement runs the check on x and y and stops execution of the code if any of them is not numeric. If both values are numeric the execution of code simply proceeds. In this case adding an else statement makes the code harder to read (and imagine what would happen if we had to perform a number of checks like this! We would need a lot of if else statements that would make everything even less clear). The code below does the same thing as the if else statement above but is more clear.\n\nx &lt;- 'not a number'\ny &lt;- 3\nif ((class(x) != 'numeric') | (class(y) != 'numeric')) {\n  stop('This is not a number!')\n}\n## Error in eval(expr, envir, enclos): This is not a number!\nx + y\n## Error in x + y: argument nieliczbowy przekazany do operatora dwuargumentowego"
  },
  {
    "objectID": "04loops_conditionals.html#loops",
    "href": "04loops_conditionals.html#loops",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Another way of controlling the flow of your code is by repeating a given chunk of code. There are two basic ways to do that: repeat something a number of times or keep repeating until some condition is met. The first way is called a for loop and the second one a while loop.\n\n\nBefore we make our first for loop lets take a moment to see when a for loop is not needed. Recall again that a lot of things in R are vectorized. This means that operations on vectors are conducted element-wise. Thanks to this if you want to e.g. add 5 to each value stored in a numeric vector (in the language of a for loop: for every element of a vector, add 5 to it) you can just write vector_name + 5. No need for more complicated, explicit repetition. However, there are situations in which you have to make an explicit for loop to repeat something n times. The general structure of a for loops looks like this:\n\nfor (i in object) {\n  Do this to each element\n}\n\nIt’s worth keeping in mind what the i in the for loop is. In the example above i will be every consecutive element of object. However we could do a similar thing with:\n\nfor (i in 1:length(object)) {\n  do this to object[i]\n}\n\nNow each i is a number from 1 to thew length of object and we access each element of object by using a proper (ith) index. Which way of running a for loop you choose might depend on the context. looping explicitly over elements of an object rather than indexes can be more intuitive but imagine you don’t want to do something to every element of an object but only to to a subset (e.g. from 3rd inwards). Doing it with indexes is easier. Generally the best approach is to think what you need first and write code code second, not the other way around.\nLets say we want to get a geometric sequence which starts from 1 and in which each next number is the previous number times 1.5. We can easily create the first 20 numbers from that sequence with a for loop:\n\nv &lt;- c(1)\n\nfor (i in 1:20)  {\n  v[i+1] &lt;- v[i]*1.5\n}\nv\n\n [1]    1.00000    1.50000    2.25000    3.37500    5.06250    7.59375\n [7]   11.39062   17.08594   25.62891   38.44336   57.66504   86.49756\n[13]  129.74634  194.61951  291.92926  437.89389  656.84084  985.26125\n[19] 1477.89188 2216.83782 3325.25673\n\n\nLets look at another example. sThere is a dataset available in R on airquality in New York City called airquality. It stores information on ozone, sun, wind and temperature from 5 months. One of the things that might be of interest when looking at the dataset is what was the average value of each of the variables informing on airquality. Lets first look at the dataset:\n\ndata(\"airquality\")\nd &lt;- airquality\nd &lt;- na.omit(d)\n\nhead(d)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n7    23     299  8.6   65     5   7\n8    19      99 13.8   59     5   8\n\n\nThe first 4 columns are of interest here. We can loop over these columns and for each calculate their mean and append it to the vector of results (which we need to create before starting the loop). Just to be sure which value is the mean of which variable we can name the elements of the resulting vector with names of the columns after the for loop:\n\nresults &lt;- c()\n\nfor (v in d[,1:4]) {\n  m &lt;- mean(v)\n  results &lt;- c(results, m)\n}\nnames(results) &lt;- colnames(d[,1:4])\nresults\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\n\n\n\nWhile loops will keep executing a given chunk of code as long as some condition is met. They aren’t very common in R, at least not until you start building your own algorithms or simulations from scratch. However, it’s worth knowing what they are in case you encounter them.\nWe can use a while loop to make a very simple simulation. Lets say we want to see how temperatures change from a given temperature (lets say 20 degrees Celsius) across time and that we represent time by some random change from each previous temperature. We can create a vector with such predicted temperatures and see how long it takes for it to reach a certain level (lets say 30 degrees Celsius). We represent the change by adding a random value from a normal distribution with mean = .05 and standard deviation = .5 (this is what the rnorm(1,.05,.5) does). The while loop would look something like this: We first create the initial value and a vector to store all temperatures and next we keep adding the random value to our temperature and storing all temperatures until it reaches 30. The last line tells R to plot all the temperatures as a line plot. This is of course a very, very, very simplistic simulation (temperatures don’t change in such a simple way) but it works to show you the idea behind while loops. We can then calculate e.g. how long it took for the temperature to reach a certain level.\n\nC &lt;- 20\nresults &lt;- c(20)\nwhile (C &lt; 30) {\n  C &lt;- C + rnorm(1,.05,.5)\n  results &lt;- c(results, C)\n}\n\nplot(results, type = 'line', lwd = 2, col=4, xlab = \"days\", ylab = \"temperature\")\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\n\n\n\n\nBecause while loops do not have a fixed number of iteration they can potentially run infinitely. This is usually not something we want so it’s a good idea to make sure that your while loop eventually stops. In case you do get stuck in an infinite loop you can press Esc in your console and this should make RStudio stop the loop by force.\nTruth is while loops are not common in R. You will rarely find yourself in situation where you need to perform some actions while a given condition is true (e.g. keep a program running until a user presses exit; keep displaying a board of a game until a player makes a move). However, it’s still good to know what while loops are so that you will know one when you see it."
  },
  {
    "objectID": "04loops_conditionals.html#apply-family",
    "href": "04loops_conditionals.html#apply-family",
    "title": "Loops and conditionals",
    "section": "",
    "text": "There is a special family of functions in R that makes working with for loops a bit easier. These functions let you specify what to loop over and what function to apply to each element but in a function rather than a whole loop with all the curly brackets and stuff.\nThe reason why this is a whole family of functions is that you can iterate in various ways and you can get the output in different formats. There are more functions in the family but the general ones are:\n\nlapply() - loops over elements of a vector/list and returns a list\nsapply() - same as lapply but tries to simplify the result to a vector or matrix\napply() - used for looping over 2 dimensional structures - it lets you specify if you want to loop over rows or columns\ntapply() - same as apply but lets you split the object you are looping over based on some factor (e.g. imagine you want to calculate the mean value of your dependent variable for each experimental condition).\n\nLets see some of these in action.\n\n\nImagine you are working with a list in R. You want to get information on how many elements each object in the list has. sapply makes it very easy:\n\nmy_list &lt;- list(\n  1:50,\n  sample(300, 5),\n  c(\"random\", \"vector\")\n)\n\nsapply(my_list, length)\n\n[1] 50  5  2\n\n\n\n\n\nRemember the airquality dataset and calculating the mean of each numeric variable? We can achieve the same thing with apply() instead of a for loop:\n\napply(d[,1:4], 2, mean)\n\n    Ozone   Solar.R      Wind      Temp \n 42.09910 184.80180   9.93964  77.79279 \n\n\nNotice that the means calculated above are global means from the entire dataset. What is probably much more sensible is a mean for each month. There is one additional trick needed here. Tapply won’t allow us to split a number of columns by some vector and perform a given operation on each of the columns. That’s because tapply works on vectors. In order to get monthly means for all 4 columns we need to combine apply with tapply. What we need to do is start with apply and loop over the 4 columns of interest and for each of them use tapply that will split a given column by month and calculate the means. Combining functions can get us really far if only we give some thought to what each function does (including what are its inputs and outputs) and what we really need to do. notice that tapply() is preceded by function(x). We’ll learn more about them when looking at functions but for now think about it as an expression that allows us to specify some function that takes x as its main argument (in this case these are going to be columns from the airquality dataset) and apply some custom function to it.\n\napply(d[,1:4], 2, function(x) tapply(x, d$Month, mean))\n\n     Ozone  Solar.R      Wind     Temp\n5 24.12500 182.0417 11.504167 66.45833\n6 29.44444 184.2222 12.177778 78.22222\n7 59.11538 216.4231  8.523077 83.88462\n8 60.00000 173.0870  8.860870 83.69565\n9 31.44828 168.2069 10.075862 76.89655\n\n\nOne important thing about apply functions is that they are generally faster than explicit for loops. Same thing goes for vectorized code as well - it’s faster than a for loop. We can make a simple comparison by using the microbenchmark package to make the same thing with a for loop and apply() function. We’ll save it as bench\nNow if we look at the results:\n\nbench\n\nUnit: milliseconds\n                                               expr    min      lq     mean\n for (i in 1:ncol(df)) {     print(mean(df[, i])) } 3.1195 3.53700 4.370040\n                                 apply(df, 2, mean) 3.8517 4.69315 7.278746\n  median      uq     max neval\n 3.82005 5.28135  7.1903   100\n 5.29420 7.06480 69.9919   100\n\n\nYou can see that apply() is much faster than a for loop."
  },
  {
    "objectID": "04loops_conditionals.html#exercises",
    "href": "04loops_conditionals.html#exercises",
    "title": "Loops and conditionals",
    "section": "",
    "text": "Try to code the logic of assigning points to players of a prisoner dilemma with a given matrix:\n\n\n\nexercise 1\n#' we need to specify outcomes based on Prisoner 1 and prisoner 2 choices.\n#' We'll need a series of ifelse statements to code each of the 4 possible outcomes.\n#' Lets define the choices as choice1 and choice2 and store infoirmation on which choice each on of these is.\n#' Next lets define some choices by the prisoners.\n#' We need to define if statements that will cover all possible combinations of choices for prisoners 1 and 2.\n#' We can do it by:\n#' 1. Creating an if else statement for the choice of prisoner 1 \n#' 2. inside that if else statement create if else statements for prisoner 2.\n#' 3. Assign vector of results depending on ech combination of prisoner 1 and 2 choices.\n#' You can copy this code and change values of Prisoner1 and Prisoner2 to choice1 or choice2 to see if the results change.\n\nchoice1 &lt;- \"Stay silent\"\nchoice2 &lt;- \"Betray and testify\"\nPrisoner1 &lt;- choice1\nPrisoner2 &lt;- choice2\n\nif(Prisoner1 == choice1) {\n  if(Prisoner2 == choice1) {\n    result &lt;-  c(P1_result = 1, P2_result = 1)\n  } else {\n    result &lt;-  c(P1_result = 0, P2_result = 10)\n  }\n} else {\n  if(Prisoner2 == choice1) {\n    result &lt;-  c(P1_result = 10, P2_result = 0)\n  } else {\n    result &lt;-  c(P1_result = 6, P2_result = 6)\n  }\n}\nresult\n\n\nCreate a for loop that will print out the first 50 numbers from the Fibonacci sequence\n\n\nexercise 2\n#' The fibonacchi sequence atarts with a 0 and 1 and then each next number is the \n#' sum of previous 2 numbers. This means we will need to first define the 2 first numbers\n#' and then run a loop that for every iteration i will print the sum of ith-1 and ith-2\n#' numbers from the sequence. Since the first 2 numbers are defined \n#' we can loop from 3 to 50 (that's hoew many numbers we are to print)\n\nx &lt;- c(0,1)\n\nfor(i in 3:50) {\n  x_i &lt;- sum(x[i-1], x[i-2])\n  x &lt;- c(x, x_i)\n  print(x_i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 5\n[1] 8\n[1] 13\n[1] 21\n[1] 34\n[1] 55\n[1] 89\n[1] 144\n[1] 233\n[1] 377\n[1] 610\n[1] 987\n[1] 1597\n[1] 2584\n[1] 4181\n[1] 6765\n[1] 10946\n[1] 17711\n[1] 28657\n[1] 46368\n[1] 75025\n[1] 121393\n[1] 196418\n[1] 317811\n[1] 514229\n[1] 832040\n[1] 1346269\n[1] 2178309\n[1] 3524578\n[1] 5702887\n[1] 9227465\n[1] 14930352\n[1] 24157817\n[1] 39088169\n[1] 63245986\n[1] 102334155\n[1] 165580141\n[1] 267914296\n[1] 433494437\n[1] 701408733\n[1] 1134903170\n[1] 1836311903\n[1] 2971215073\n[1] 4807526976\n[1] 7778742049\n\n\nGiven the iris dataframe (you can load it with data(\"iris\") loop over all of its columns and calculate the mean of every numeric column\n\n\nexercise 3\n#' What we are supposed to do is:\n#' 1. load the iris dataset\n#' 2. loop over numeric columns (there is 1 column that is not numeric)\n#' 3. for each column calculate its mean\n\ndata(\"iris\")\n\nresults &lt;- c()\nfor(i in 1:ncol(iris)) {\n  if(is.numeric(iris[,i])) {\n    results &lt;- c(results, mean(iris[,i]))\n  }\n}\n\n#if we want to format the names of values in the vector we can extract the names of numeric columns.\n#The sapply function returns a vector of logical values with TRUE for every numeric column\n#in the dataset. We can use that vector to subset the dataset and get only numeric columns.\nnames(results) &lt;- colnames(iris[,sapply(iris, is.numeric)])\nresults\n\n\n#We can also shorten the for loop and use apply instead. \n#THis time we'll need to loop over the dataset with numeric columns selected\napply(iris[,sapply(iris, is.numeric)], 2, mean)\n\n\nCreate a vector of numbers randomly drawn from a normal distribution with a mean of 2 and standard deviation 0.5. Loop over a sequence of 100 numbers from the minimum to the maximum of the vector you created. For each value in the sequence calculate the sum of squared differences from each value in the vector. Store all the sums in a new vector. Which value in the sequence minimizes the sum of squared distances?\n\n\nexercise 4\n#' What the exercise asks us to do is:\n#' 1. Create a vector of numbers drawn from normal distribution wiht mean = 2 and sd = .5. We can use rnorm() for that.\n#' 2. loop over a sequence of numbers from the minimum to maximum of our vector and for each value calculate the sum of squared distances of each number from a given number in the sequence. This will require:\n#' 2a. Define a vector to store the sums of squared distances\n#' 2b. Start the for loop over seq(min(v1), max(v1), .1). You can change the distance in seq from .1 to something else if you want to see with different resolution\n#' 2c. For each i in the sequence calculate the sum of squared differences and append it to vector\n#' 3. Preferrably name the vector with results\n#' 4. Find the minimum number\n\n#the number we get as mimizing the sum of squared differences is certainly \n#going to be the one closest to the mean (you can check it with mean(v1) if you want)!\n\nv1 &lt;- rnorm(1000, 2, .5)\nsums_squared &lt;- c()\n\nfor(i in seq(min(v1), max(v1), .1)) {\n  diff_i &lt;- sum((i - v1)^2)\n  sums_squared &lt;- c(sums_squared, diff_i)\n}\n\nto_name &lt;- seq(min(v1), max(v1), .1)\nnames(sums_squared) &lt;- to_name\n\nsums_squared[which.min(sums_squared)]"
  },
  {
    "objectID": "05functions.html",
    "href": "05functions.html",
    "title": "Functions",
    "section": "",
    "text": "Most of the things we do in R doesn’t have to be written from scratch. We have many tools available to get what we want. These tools are functions.\n\n\nThe first approximation to how a function is built is to think of it as a kind of machine. The machine takes some inputs, processes them in some way and returns outputs. The inputs are the arguments you provide to a function like a vector or a dataset. The result is the output. Very often the insides of a function, the machinery within it that is responsible for getting from the input to the output is a black box to us. We have no clue how exactly a function arrives at its result. Sometimes we don’t need to know it but in many situations at least some knowledge is necessary to be certain that the function does exactly what we need it to do and won’t surprise us (an annoying example we will get to later is silent dropping of missing values by some functions).\n\n\n\nLets focus on the inputs. There are a few kinds of them. The most basic ones are input arguments - this is what you put into the machine. Apart from it there are a few other types of arguments that can allow you to have more control over the behavior of functions. They are a bit like toggles and switches on a machine that change how it operates.\nDefault arguments: arguments that are set to some default value. This value will be used unless specified otherwise. An example is the na.rm argument from mean or sum. This argument is set to FALSE by default so that the function will return an error if there are any missing values in the input argument. This is such a good example because it also stresses why choosing proper defaults is really important when writing functions. A lot of people, when they first encounter functions like mean or sum, are surprised or even annoyed. Why in the world set defaults that are more likely to produce errors? We are often fixated on avoiding errors in code but this is not always the way to go in data analysis. We often want functions to operate smoothly and seamlessly. But that is false peace. Smooth behavior is not always what we need from functions. Clunky functions are often good in data analysis because they force us to be explicit with what we do with data. Even if they make climbing the hill a bit more steep they are sure to lead us on the right path to the top.\nYou can also encounter alternative arguments. These arguments have a prespecified set of possible values (usually defined as a vector).= For example the table() function that can give us a frequency table of a factor has a useNA argument that specifies whether to use NA values.It can take three different valus that specify possible behaviour of the function. You can read more onwhat they do inthe documentation of the function.\nThe final type of argument is the … argument. It is a placeholder for any number and kind of arguments that will later on be passed inside the function usually as arguments in some internal function. Take lapply as an example. Apart from the argument X and FUN which specify what to loop over and what function to apply to each element of X it also has the … argument. It’s there because the function you want to apply to every element of X might take some additional arguments. How many and what kind of arguments these are might vary from function to function and the … argument allows us to handle this. Any arguments passed in the … will be used as argument of the function specified in the FUN argument of lapply.\n\n\n\nWhy spend time building your own functions? There are a few general cases. The first and probably most obvious one is when there is no available function that would do what you need. For example there is no available function to find a mode of a vector in R. If you want to find it you need to build your own function. Finding a mode is a simple example but there may be cases where you need to do something more complex or customize the behavior of an already existing function. Second reason is to avoid repetition. If you do similar operations a number of times (e.g. only the dataset or vriables change but all the rest stays the same) then copying and pasting code will soon become problematic. It makes code less readable, longer and more difficult to manage. Imagine you need to change one thing in that code. You’ll need to change it in every place where it was pasted. Writing a function instead means you can just change how you define the function.\nThe general logic for defining a function is as follows:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nTurning a chunk of code into a function can be done quite easily in a few steps. Imagine we want to see what is the probability that a random number drawn from one vector will be larger than the mean of another vector (we will simulate a few variables with rnorm() which draws random numbers from a normal distribution with a given mean and standard deviation):\n\n#simulate vectors\nvar1 &lt;-rnorm(100, 0, 1)\nvar2 &lt;- rnorm(100, 1, 3)\nvar3 &lt;- rnorm(100, .5, 2)\nvar4 &lt;- rnorm(100, 0, 3)\nvar5 &lt;- rnorm(100, 0.1, .2)\n\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n\n[1] 0.57\n\n\n\nBuild the scaffolding of the function. This is exactly what is in the code chunk above:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nPaste the code you want to turn into a function\n\nmy_function &lt;- function(arguments) {\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\nIdentify all the “moving parts”: What will change? Each of these things has to get its own argument (all the moving parts are marked on the right side of the code chunk):\n\nmy_function &lt;- function(arguments) {\n#calculate mean\n1mean_1 &lt;- mean(var1)\n\n#calculate lenght\n2length_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\n3n_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n1\n\nvar_1\n\n2\n\nvar_2\n\n3\n\nvar_2\n\n\n\n\nChange each of the “moving parts” in the code chunk into appropriate argument\n\nmy_function &lt;- function(x, y) {\n#calculate mean\nmean_1 &lt;- mean(y)\n\n#calculate lenght\nlength_2 &lt;- length(x)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(x[x &gt; mean_2])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n\n\nWhen building your own functions, especially if they are going to be used by other people, it’s a good idea to consider potential weird things that could happen. When first creating a function we usually have its typical behaviour in mind because we just want our function to work. However, there’s a whole bunch of weird stuff that might happen if you don’t prepare for it in advance. For example, imagine you want to create a function from scratch that will output the mean of a numeric vector. You could try to do something like a for loop (yes this is slow and inefficient but it’s just for the purpose of demonstration):\n\nmy_mean &lt;- function(x) {\n  sum &lt;- 0\n  for(i in x){\n    sum &lt;- sum + i\n  }\n  result &lt;- sum/length(x)\n  return(result)\n}\n\nPretty straightforward right? Now lets see our function in action on some typical use case and compare its results to the built-in mean() function:\n\nv &lt;- c(1,2,3,4,5,6,7,8,9)\n\nmy_mean(v)\n\n[1] 5\n\nmean(v)\n\n[1] 5\n\n\nYay, we get the same result! Seems like our function works! But before we call it a day and start using our own mean function lets see some less typical cases. E.g. What will happen if the vector has some missing values? Or if its an empty vector? Or if it is not a numeric vector? Lets see:\n\nv_na &lt;- c(1,2,3,NA,5)\nv_empty &lt;- c()\nv_char &lt;- c(\"A\", \"B\", \"C\")\n\nmy_mean(v_na)\n\n[1] NA\n\nmy_mean(v_empty)\n\n[1] NaN\n\nmy_mean(v_char)\n\nError in sum + i: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWe get some weird behaviour. Each of these calls to my_mean() function returned something different. First we got NA when the vector had NAs in it. Passing an empty vector resulted in NaN - short for Not a Number. Finally, passing a character vector gave us an error.. Notice that only the last case gave us an error so if we then implemented our functions in some calculations we might not even notice something is wrong - for example imagine we calculated a mean with our function from a vector with missiing values (e.g we asked a bunch of participants about their mood 5 times a day and now we want to calculate daily average mood and see how it relates to some variables of interest) and then tried to use its output in some other function that had na.rm argument set to TRUE. We’d lose a bunch of information without so much as a warning! That`s why considering possible but unusual cases for a new function is important. It allows us to prepare for possible future problems.\n\n\n\n\nThere are situations in which you might want to use a custom function but not necessarily save it with a name for future use. In such situations we often use what is called an anonymous function (sometimes you can also encounter the term lambda functions). The general way these functions are constructed in R is as follows:\n\n(function(x) WHAT THE FUCNTION DOES)(arguments)\n\nA pretty common situation where you can also encounter these functions is inside iterations like with apply:\n\nv1 &lt;- c(1,2,3,4)\nv2 &lt;- c(3,4,5,6,8)\nv3 &lt;- c(-1,4,3,2)\nv_list &lt;- list(v1,v2,v3)\n\nlapply(v_list, function(x) {x+x})\n\n[[1]]\n[1] 2 4 6 8\n\n[[2]]\n[1]  6  8 10 12 16\n\n[[3]]\n[1] -2  8  6  4\n\n\n\n\n\nOk, so we wrote our super cool new function. We tested it and are pretty confident it works properly. Can we finally call it a day? Again, not so fast. We need one more thing. Imagine you take a long holiday and get back towork after a month or two, How confident are you you will remember how exactly our new function works? Or imagine you share the function you ccreated with other people .Of course you (or others) can read the code of the function to learn that again but that is tedious. That`s why it’s so important to document code. This goes for functions but is just as true for any code that will be used by others or you in the future. Treat yourself in the future like you would treat another person. Documentation is super important! For a lot of people writing (or reading!) documentation is seen as tedious and redundant task. I guarantee you that if you don’t document your own functions you will regret this sooner or later (probably sooner). Very few functions are self explanatory enough to not need any form of documentation. In many cases simply using comments in code with # will be enough. Sometimes building vignettes that shows how to use some functions can be a better idea. Remember to always leave some description of what given code is about and what it does.\n\n\n\nSince R has a huge community people are constantly developing new things you can do in R. You don’t have to define everything from scratch. Usually if you need a function for some statistical procedure or e.g. for plotting some package out there already has it. There is no need to reinvent the wheel.\nIn order to use functions from packages you need to first install the package on your computer. You can do it by calling install.packages(\"PACKAGENAME\") functon. You need to do it only once on a given device (unless you want to update the package or you are using renv but more on that later). Once the package has been installed you can load it in a given R session by calling library(PACKAGENAME). Remember you need to run it every time you open a new R session. Alternatively, after installing a package you can call a function from it directly without loading the package first by using PACKAGENAME::function_name().\nAnother thing to know about functions from packages is name conflict. Since R is open source and most of the packages are developed and maintained by the community it is not so uncommon that two different packages have a function with the same name. You might wonder what will happen if you load both packages and then call this function? Generally, the last package loaded is going to mask previous packages. However this can be problematic e.g. if you are sharing scripts (and someone changes the order of loading packages) or if you actually want to use the function from the first package.\nThere are at least two ways of dealing with this problem. The first one is to be explicit. Above we described a second way of calling a function from a package: PACKAGENAME::function_name(). This way you explicitly state which package the function is from so you shield yourself from name conflict. The second way is by specifying additional arguments to the library() function. If you look up its documentation you can see that it has two optional arguments: exlcude and include.only. They allow you to load a package without some function or to load only some functions from a package. This is useful in situations where you want to load 2 packages with conflicting functions but you know you want to use the conflicting function from only one of them.\n\n\n\n\nRemember the for loop that generated n first numbers from Fibonacci sequence from the class on loops? Now turn it into a function that will return from ith to jth Fibonacci number. Document the function properly so it is clear what it does\n\n\nexercise 1\n#' Recall our code for generating 50 numbers from Fibonacci sequence:\n\nx &lt;- c(0,1)\n\nfor(i in 3:50) {\n  x[i] &lt;- x[i-1] + x[i-2]\n}\n\n#' What we need to do now is wrap this in a function that will generate numbers from Fibonacci sequence and select from ith to jth number.\n#' We'll need 2 arguments: starting and ending number\n#' Next we'll need to generate numbers from Fibonacci sequence up to jth number\n#' Finally we'll need to subset the resulting vector from ith number\n#' We also need one more modification: the loop own't give us onyl the first or second number\n#' because the subsetting won't work there. We just need an if else statement that will test if the jth number is 1 or 2 and return proper result if yes.\n\nget_fibonacci &lt;- function(i,j) {\n  x &lt;- c(0,1)\n\n  if (j == 1) {\n    result &lt;- x[1]\n    return(result)\n  } else if (j == 2) {\n    result &lt;- x\n    return(result[i:j])\n  }\n\n\n  for(k in 3:j) {\n  x[k] &lt;- x[k-1] + x[k-2]\n}\n  result &lt;- x[i:j]\n  return(result)\n}\n\n\nCreate a function that calculates a mode of a vector. Consider potential edge cases and provide tests that show your function behaves properly\n\n\nexercise 2\n#' A mode of a vector is its most common value.\n#' One way to try and get a mode of a vector would be to get frequencies of every value in the vector,\n#' sort them in descending order and then extract the first value. We can do it with table() to get counts\n#' sort() with decreasing = TRUE to sort. There are just two problems, one small and another bigger:\n#' 1. table() will return a named vector of values - what we'll get is the highest count and not really\n#' the value with highest counts. We can fix it by calling names() on the result of table() and sort()\n#' to get the values rather than counts.\n#' 2. What if there are two modes? In such cases extracting the highest value won't work because we'll\n#' only get one mode. We'll need to adjust for this somehow. One way to do it would be to first\n#' extract the first highest value and then loop over the remaining values. In each iteration of the loop\n#'we want to test if the ith value is equal to the highest value. If yes - we have another mode and we\n#' append it to results. If no - we got all modes and we can close the loop. One more thing to consider\n#' here is whether this loop should run every time? What if we get a vector with just one type of values\n#' e.g. c(1,1,1,1)? Or an empty vector? The result of table() will have length of 1 so we can't loop over\n#' all the other elements except for the mos common one because there is just one element.\n#'  We need to test if length of the sorted counts have at least 2 elements and run the loop if yes.\n#' \n#' Other things we might want to test are: what if we get missing values? \n#' Table() will remove them by default and not return counts of NAs. What if we provide an empty vector?\n#' Again, table() has a default behavior - it will return an empty table that will be turned to NULL when\n#' we try to get the names. Are these the behaviors we want for our function?\n#' If not then we need to adjust accordingly.\n#' There might be other edge cases but these should suffice to get you thinking about what you might encounter when designing functions\n\n\n\nmode &lt;- function(factor){\n  #get sorted counts of factor\n  sorted &lt;- sort(table(factor), decreasing = T)\n\n  #extract highest counts\n  largest &lt;- sorted[1]\n\n  if (length(sorted) &gt;= 2) {\n     #loop over remaining values\n  for (i in 2:length(sorted)) {\n\n    #test if next count is equal to highest count\n    if(sorted[i] == largest[1]) {\n      #append next mode\n      largest &lt;- c(largest, sorted[i])\n    } else {\n      #if next count is not equal - end the loop\n      break\n    }\n  }\n  }\n\n\n  #get names of all the modes to return modes rather than their counts\n  largest &lt;- names(sorted[1:length(largest)])\n  #return results\n  return(largest)\n}\n\n#some checks: simple case, two modes, empty vector, missing values\nsimple_v &lt;- c(1,1,1,2,3,2)\ntwo_modes_v &lt;- c(1,1,1,2,2,2)\nempty_v &lt;- c()\nmissing_v &lt;- c(1,2,1,NA,NA,NA)\none_type_v &lt;- c(\"a\", \"a\")\n\nmode(simple_v)\nmode(two_modes_v)\nmode(empty_v)\nmode(missing_v)\nmode(one_type_v)"
  },
  {
    "objectID": "05functions.html#how-is-a-function-built",
    "href": "05functions.html#how-is-a-function-built",
    "title": "Functions",
    "section": "",
    "text": "The first approximation to how a function is built is to think of it as a kind of machine. The machine takes some inputs, processes them in some way and returns outputs. The inputs are the arguments you provide to a function like a vector or a dataset. The result is the output. Very often the insides of a function, the machinery within it that is responsible for getting from the input to the output is a black box to us. We have no clue how exactly a function arrives at its result. Sometimes we don’t need to know it but in many situations at least some knowledge is necessary to be certain that the function does exactly what we need it to do and won’t surprise us (an annoying example we will get to later is silent dropping of missing values by some functions)."
  },
  {
    "objectID": "05functions.html#types-of-arguments",
    "href": "05functions.html#types-of-arguments",
    "title": "Functions",
    "section": "",
    "text": "Lets focus on the inputs. There are a few kinds of them. The most basic ones are input arguments - this is what you put into the machine. Apart from it there are a few other types of arguments that can allow you to have more control over the behavior of functions. They are a bit like toggles and switches on a machine that change how it operates.\nDefault arguments: arguments that are set to some default value. This value will be used unless specified otherwise. An example is the na.rm argument from mean or sum. This argument is set to FALSE by default so that the function will return an error if there are any missing values in the input argument. This is such a good example because it also stresses why choosing proper defaults is really important when writing functions. A lot of people, when they first encounter functions like mean or sum, are surprised or even annoyed. Why in the world set defaults that are more likely to produce errors? We are often fixated on avoiding errors in code but this is not always the way to go in data analysis. We often want functions to operate smoothly and seamlessly. But that is false peace. Smooth behavior is not always what we need from functions. Clunky functions are often good in data analysis because they force us to be explicit with what we do with data. Even if they make climbing the hill a bit more steep they are sure to lead us on the right path to the top.\nYou can also encounter alternative arguments. These arguments have a prespecified set of possible values (usually defined as a vector).= For example the table() function that can give us a frequency table of a factor has a useNA argument that specifies whether to use NA values.It can take three different valus that specify possible behaviour of the function. You can read more onwhat they do inthe documentation of the function.\nThe final type of argument is the … argument. It is a placeholder for any number and kind of arguments that will later on be passed inside the function usually as arguments in some internal function. Take lapply as an example. Apart from the argument X and FUN which specify what to loop over and what function to apply to each element of X it also has the … argument. It’s there because the function you want to apply to every element of X might take some additional arguments. How many and what kind of arguments these are might vary from function to function and the … argument allows us to handle this. Any arguments passed in the … will be used as argument of the function specified in the FUN argument of lapply."
  },
  {
    "objectID": "05functions.html#building-your-own-functions",
    "href": "05functions.html#building-your-own-functions",
    "title": "Functions",
    "section": "",
    "text": "Why spend time building your own functions? There are a few general cases. The first and probably most obvious one is when there is no available function that would do what you need. For example there is no available function to find a mode of a vector in R. If you want to find it you need to build your own function. Finding a mode is a simple example but there may be cases where you need to do something more complex or customize the behavior of an already existing function. Second reason is to avoid repetition. If you do similar operations a number of times (e.g. only the dataset or vriables change but all the rest stays the same) then copying and pasting code will soon become problematic. It makes code less readable, longer and more difficult to manage. Imagine you need to change one thing in that code. You’ll need to change it in every place where it was pasted. Writing a function instead means you can just change how you define the function.\nThe general logic for defining a function is as follows:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nTurning a chunk of code into a function can be done quite easily in a few steps. Imagine we want to see what is the probability that a random number drawn from one vector will be larger than the mean of another vector (we will simulate a few variables with rnorm() which draws random numbers from a normal distribution with a given mean and standard deviation):\n\n#simulate vectors\nvar1 &lt;-rnorm(100, 0, 1)\nvar2 &lt;- rnorm(100, 1, 3)\nvar3 &lt;- rnorm(100, .5, 2)\nvar4 &lt;- rnorm(100, 0, 3)\nvar5 &lt;- rnorm(100, 0.1, .2)\n\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n\n[1] 0.57\n\n\n\nBuild the scaffolding of the function. This is exactly what is in the code chunk above:\n\nmy_function &lt;- function(arguments) {\n  #what the function does\n}\n\nPaste the code you want to turn into a function\n\nmy_function &lt;- function(arguments) {\n#calculate mean\nmean_1 &lt;- mean(var1)\n\n#calculate lenght\nlength_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\nIdentify all the “moving parts”: What will change? Each of these things has to get its own argument (all the moving parts are marked on the right side of the code chunk):\n\nmy_function &lt;- function(arguments) {\n#calculate mean\n1mean_1 &lt;- mean(var1)\n\n#calculate lenght\n2length_2 &lt;- length(var2)\n\n#calculate how many values in var2 are larger than mean_1\n\n3n_larger &lt;- length(var2[var2 &gt; mean_1])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n1\n\nvar_1\n\n2\n\nvar_2\n\n3\n\nvar_2\n\n\n\n\nChange each of the “moving parts” in the code chunk into appropriate argument\n\nmy_function &lt;- function(x, y) {\n#calculate mean\nmean_1 &lt;- mean(y)\n\n#calculate lenght\nlength_2 &lt;- length(x)\n\n#calculate how many values in var2 are larger than mean_1\n\nn_larger &lt;- length(x[x &gt; mean_2])\n\n#get the proportion\n\nn_larger/length_2\n}\n\n\n\n\nWhen building your own functions, especially if they are going to be used by other people, it’s a good idea to consider potential weird things that could happen. When first creating a function we usually have its typical behaviour in mind because we just want our function to work. However, there’s a whole bunch of weird stuff that might happen if you don’t prepare for it in advance. For example, imagine you want to create a function from scratch that will output the mean of a numeric vector. You could try to do something like a for loop (yes this is slow and inefficient but it’s just for the purpose of demonstration):\n\nmy_mean &lt;- function(x) {\n  sum &lt;- 0\n  for(i in x){\n    sum &lt;- sum + i\n  }\n  result &lt;- sum/length(x)\n  return(result)\n}\n\nPretty straightforward right? Now lets see our function in action on some typical use case and compare its results to the built-in mean() function:\n\nv &lt;- c(1,2,3,4,5,6,7,8,9)\n\nmy_mean(v)\n\n[1] 5\n\nmean(v)\n\n[1] 5\n\n\nYay, we get the same result! Seems like our function works! But before we call it a day and start using our own mean function lets see some less typical cases. E.g. What will happen if the vector has some missing values? Or if its an empty vector? Or if it is not a numeric vector? Lets see:\n\nv_na &lt;- c(1,2,3,NA,5)\nv_empty &lt;- c()\nv_char &lt;- c(\"A\", \"B\", \"C\")\n\nmy_mean(v_na)\n\n[1] NA\n\nmy_mean(v_empty)\n\n[1] NaN\n\nmy_mean(v_char)\n\nError in sum + i: argument nieliczbowy przekazany do operatora dwuargumentowego\n\n\nWe get some weird behaviour. Each of these calls to my_mean() function returned something different. First we got NA when the vector had NAs in it. Passing an empty vector resulted in NaN - short for Not a Number. Finally, passing a character vector gave us an error.. Notice that only the last case gave us an error so if we then implemented our functions in some calculations we might not even notice something is wrong - for example imagine we calculated a mean with our function from a vector with missiing values (e.g we asked a bunch of participants about their mood 5 times a day and now we want to calculate daily average mood and see how it relates to some variables of interest) and then tried to use its output in some other function that had na.rm argument set to TRUE. We’d lose a bunch of information without so much as a warning! That`s why considering possible but unusual cases for a new function is important. It allows us to prepare for possible future problems."
  },
  {
    "objectID": "05functions.html#anonymous-functions",
    "href": "05functions.html#anonymous-functions",
    "title": "Functions",
    "section": "",
    "text": "There are situations in which you might want to use a custom function but not necessarily save it with a name for future use. In such situations we often use what is called an anonymous function (sometimes you can also encounter the term lambda functions). The general way these functions are constructed in R is as follows:\n\n(function(x) WHAT THE FUCNTION DOES)(arguments)\n\nA pretty common situation where you can also encounter these functions is inside iterations like with apply:\n\nv1 &lt;- c(1,2,3,4)\nv2 &lt;- c(3,4,5,6,8)\nv3 &lt;- c(-1,4,3,2)\nv_list &lt;- list(v1,v2,v3)\n\nlapply(v_list, function(x) {x+x})\n\n[[1]]\n[1] 2 4 6 8\n\n[[2]]\n[1]  6  8 10 12 16\n\n[[3]]\n[1] -2  8  6  4"
  },
  {
    "objectID": "05functions.html#documentation",
    "href": "05functions.html#documentation",
    "title": "Functions",
    "section": "",
    "text": "Ok, so we wrote our super cool new function. We tested it and are pretty confident it works properly. Can we finally call it a day? Again, not so fast. We need one more thing. Imagine you take a long holiday and get back towork after a month or two, How confident are you you will remember how exactly our new function works? Or imagine you share the function you ccreated with other people .Of course you (or others) can read the code of the function to learn that again but that is tedious. That`s why it’s so important to document code. This goes for functions but is just as true for any code that will be used by others or you in the future. Treat yourself in the future like you would treat another person. Documentation is super important! For a lot of people writing (or reading!) documentation is seen as tedious and redundant task. I guarantee you that if you don’t document your own functions you will regret this sooner or later (probably sooner). Very few functions are self explanatory enough to not need any form of documentation. In many cases simply using comments in code with # will be enough. Sometimes building vignettes that shows how to use some functions can be a better idea. Remember to always leave some description of what given code is about and what it does."
  },
  {
    "objectID": "05functions.html#functions-from-packages",
    "href": "05functions.html#functions-from-packages",
    "title": "Functions",
    "section": "",
    "text": "Since R has a huge community people are constantly developing new things you can do in R. You don’t have to define everything from scratch. Usually if you need a function for some statistical procedure or e.g. for plotting some package out there already has it. There is no need to reinvent the wheel.\nIn order to use functions from packages you need to first install the package on your computer. You can do it by calling install.packages(\"PACKAGENAME\") functon. You need to do it only once on a given device (unless you want to update the package or you are using renv but more on that later). Once the package has been installed you can load it in a given R session by calling library(PACKAGENAME). Remember you need to run it every time you open a new R session. Alternatively, after installing a package you can call a function from it directly without loading the package first by using PACKAGENAME::function_name().\nAnother thing to know about functions from packages is name conflict. Since R is open source and most of the packages are developed and maintained by the community it is not so uncommon that two different packages have a function with the same name. You might wonder what will happen if you load both packages and then call this function? Generally, the last package loaded is going to mask previous packages. However this can be problematic e.g. if you are sharing scripts (and someone changes the order of loading packages) or if you actually want to use the function from the first package.\nThere are at least two ways of dealing with this problem. The first one is to be explicit. Above we described a second way of calling a function from a package: PACKAGENAME::function_name(). This way you explicitly state which package the function is from so you shield yourself from name conflict. The second way is by specifying additional arguments to the library() function. If you look up its documentation you can see that it has two optional arguments: exlcude and include.only. They allow you to load a package without some function or to load only some functions from a package. This is useful in situations where you want to load 2 packages with conflicting functions but you know you want to use the conflicting function from only one of them."
  },
  {
    "objectID": "05functions.html#exercises",
    "href": "05functions.html#exercises",
    "title": "Functions",
    "section": "",
    "text": "Remember the for loop that generated n first numbers from Fibonacci sequence from the class on loops? Now turn it into a function that will return from ith to jth Fibonacci number. Document the function properly so it is clear what it does\n\n\nexercise 1\n#' Recall our code for generating 50 numbers from Fibonacci sequence:\n\nx &lt;- c(0,1)\n\nfor(i in 3:50) {\n  x[i] &lt;- x[i-1] + x[i-2]\n}\n\n#' What we need to do now is wrap this in a function that will generate numbers from Fibonacci sequence and select from ith to jth number.\n#' We'll need 2 arguments: starting and ending number\n#' Next we'll need to generate numbers from Fibonacci sequence up to jth number\n#' Finally we'll need to subset the resulting vector from ith number\n#' We also need one more modification: the loop own't give us onyl the first or second number\n#' because the subsetting won't work there. We just need an if else statement that will test if the jth number is 1 or 2 and return proper result if yes.\n\nget_fibonacci &lt;- function(i,j) {\n  x &lt;- c(0,1)\n\n  if (j == 1) {\n    result &lt;- x[1]\n    return(result)\n  } else if (j == 2) {\n    result &lt;- x\n    return(result[i:j])\n  }\n\n\n  for(k in 3:j) {\n  x[k] &lt;- x[k-1] + x[k-2]\n}\n  result &lt;- x[i:j]\n  return(result)\n}\n\n\nCreate a function that calculates a mode of a vector. Consider potential edge cases and provide tests that show your function behaves properly\n\n\nexercise 2\n#' A mode of a vector is its most common value.\n#' One way to try and get a mode of a vector would be to get frequencies of every value in the vector,\n#' sort them in descending order and then extract the first value. We can do it with table() to get counts\n#' sort() with decreasing = TRUE to sort. There are just two problems, one small and another bigger:\n#' 1. table() will return a named vector of values - what we'll get is the highest count and not really\n#' the value with highest counts. We can fix it by calling names() on the result of table() and sort()\n#' to get the values rather than counts.\n#' 2. What if there are two modes? In such cases extracting the highest value won't work because we'll\n#' only get one mode. We'll need to adjust for this somehow. One way to do it would be to first\n#' extract the first highest value and then loop over the remaining values. In each iteration of the loop\n#'we want to test if the ith value is equal to the highest value. If yes - we have another mode and we\n#' append it to results. If no - we got all modes and we can close the loop. One more thing to consider\n#' here is whether this loop should run every time? What if we get a vector with just one type of values\n#' e.g. c(1,1,1,1)? Or an empty vector? The result of table() will have length of 1 so we can't loop over\n#' all the other elements except for the mos common one because there is just one element.\n#'  We need to test if length of the sorted counts have at least 2 elements and run the loop if yes.\n#' \n#' Other things we might want to test are: what if we get missing values? \n#' Table() will remove them by default and not return counts of NAs. What if we provide an empty vector?\n#' Again, table() has a default behavior - it will return an empty table that will be turned to NULL when\n#' we try to get the names. Are these the behaviors we want for our function?\n#' If not then we need to adjust accordingly.\n#' There might be other edge cases but these should suffice to get you thinking about what you might encounter when designing functions\n\n\n\nmode &lt;- function(factor){\n  #get sorted counts of factor\n  sorted &lt;- sort(table(factor), decreasing = T)\n\n  #extract highest counts\n  largest &lt;- sorted[1]\n\n  if (length(sorted) &gt;= 2) {\n     #loop over remaining values\n  for (i in 2:length(sorted)) {\n\n    #test if next count is equal to highest count\n    if(sorted[i] == largest[1]) {\n      #append next mode\n      largest &lt;- c(largest, sorted[i])\n    } else {\n      #if next count is not equal - end the loop\n      break\n    }\n  }\n  }\n\n\n  #get names of all the modes to return modes rather than their counts\n  largest &lt;- names(sorted[1:length(largest)])\n  #return results\n  return(largest)\n}\n\n#some checks: simple case, two modes, empty vector, missing values\nsimple_v &lt;- c(1,1,1,2,3,2)\ntwo_modes_v &lt;- c(1,1,1,2,2,2)\nempty_v &lt;- c()\nmissing_v &lt;- c(1,2,1,NA,NA,NA)\none_type_v &lt;- c(\"a\", \"a\")\n\nmode(simple_v)\nmode(two_modes_v)\nmode(empty_v)\nmode(missing_v)\nmode(one_type_v)"
  },
  {
    "objectID": "06loading_data.html",
    "href": "06loading_data.html",
    "title": "Loading data",
    "section": "",
    "text": "So far we’ve been creating the object to work with ourselves. Usually you will work with already existing datasets though (like a file from an experiment, survey etc.). The first thing you need to do to start working on your file is to load it into R session. There are many ways of loading a dataset or file into your R session so that you can use it. How to do it depends mainly on how the data is stored.\nWhen loading a dataset there are generally a few things to consider:\n\nExtension: data can be stored in different ways. This can also be related to what kind of information is stored in a given file. Depending on how a file is stored you might need to load it differently because it stores different information. For example excel files can have multiple sheets while spss files can store labels attached to variables. When loading a file you always have to provide files with extensions.\nfile path: files are stored in different places on your computer (or on the web). In order to load it you need to tell R exactly where to look for the file. A file path speifies exactly where exactly a file is stored e.g. C:/Users/User/Documents specifies the path to the documents folder. A file path can be absolute like the one shown above - its unequivocal. There are also relative file paths that specifies the location of a file relative to current directory. By default R will look in what is called working directory. For simple scripts this by default is set to the documents folder. You can check the current working directory with getwd() function. If you want to change the working directory you can do it with setwd() and pass a string with new directory as argument. This way of working with directories will do for now but it’s not really a good way of managing directories. If you move your script to another folder or share the data and script with other people using setwd() will fail you. Because of this you should avoid manually setting working directory.. One way to deal with this problem is to use here() function from the here package. It uses some heuristics to determine the directory of your script. It can solve the problem of moving or sharing files but remember it uses heuristics so it might not always work. The other solution is to use R projects which automatically set the working directory to the project directory. We’ll learn about them later on."
  },
  {
    "objectID": "06loading_data.html#loading-data-in-r",
    "href": "06loading_data.html#loading-data-in-r",
    "title": "Loading data",
    "section": "",
    "text": "So far we’ve been creating the object to work with ourselves. Usually you will work with already existing datasets though (like a file from an experiment, survey etc.). The first thing you need to do to start working on your file is to load it into R session. There are many ways of loading a dataset or file into your R session so that you can use it. How to do it depends mainly on how the data is stored.\nWhen loading a dataset there are generally a few things to consider:\n\nExtension: data can be stored in different ways. This can also be related to what kind of information is stored in a given file. Depending on how a file is stored you might need to load it differently because it stores different information. For example excel files can have multiple sheets while spss files can store labels attached to variables. When loading a file you always have to provide files with extensions.\nfile path: files are stored in different places on your computer (or on the web). In order to load it you need to tell R exactly where to look for the file. A file path speifies exactly where exactly a file is stored e.g. C:/Users/User/Documents specifies the path to the documents folder. A file path can be absolute like the one shown above - its unequivocal. There are also relative file paths that specifies the location of a file relative to current directory. By default R will look in what is called working directory. For simple scripts this by default is set to the documents folder. You can check the current working directory with getwd() function. If you want to change the working directory you can do it with setwd() and pass a string with new directory as argument. This way of working with directories will do for now but it’s not really a good way of managing directories. If you move your script to another folder or share the data and script with other people using setwd() will fail you. Because of this you should avoid manually setting working directory.. One way to deal with this problem is to use here() function from the here package. It uses some heuristics to determine the directory of your script. It can solve the problem of moving or sharing files but remember it uses heuristics so it might not always work. The other solution is to use R projects which automatically set the working directory to the project directory. We’ll learn about them later on."
  },
  {
    "objectID": "06loading_data.html#loading-flat-files",
    "href": "06loading_data.html#loading-flat-files",
    "title": "Loading data",
    "section": "Loading flat files",
    "text": "Loading flat files\nWe’ll start with loading what are called flat files: csv (short for comma separated values) and tsv (tab separated values). Both of these are basically plain text files just structures in a ver specific wayThe name kind of gives away how these formats store data: in csv columns are separated by commas and in tsv columns are separated by tabs. You can see an example of a csv file below:\n\nNot very readable right? There are 2 packages with similar functions for loading .csv and .tsv files: utils and readr. We`ll load the gapminder population data saved as a .csv file:\n\ndf1 &lt;- read.csv(\"data/pop.csv\")\nhead(df)\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\nUsing the readr package is very similar:\n\nlibrary(readr)\ndf2 &lt;- read_csv(\"data/pop.csv\")\n\nRows: 197 Columns: 302\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (302): country, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 18...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df2)\n\n# A tibble: 6 × 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 Afghani… 3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M \n2 Angola   1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M \n3 Albania  400k   402k   404k   405k   407k   409k   411k   413k   414k   416k  \n4 Andorra  2650   2650   2650   2650   2650   2650   2650   2650   2650   2650  \n5 UAE      40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k \n6 Argenti… 534k   520k   506k   492k   479k   466k   453k   441k   429k   417k  \n# ℹ 291 more variables: `1810` &lt;chr&gt;, `1811` &lt;chr&gt;, `1812` &lt;chr&gt;, `1813` &lt;chr&gt;,\n#   `1814` &lt;chr&gt;, `1815` &lt;chr&gt;, `1816` &lt;chr&gt;, `1817` &lt;chr&gt;, `1818` &lt;chr&gt;,\n#   `1819` &lt;chr&gt;, `1820` &lt;chr&gt;, `1821` &lt;chr&gt;, `1822` &lt;chr&gt;, `1823` &lt;chr&gt;,\n#   `1824` &lt;chr&gt;, `1825` &lt;chr&gt;, `1826` &lt;chr&gt;, `1827` &lt;chr&gt;, `1828` &lt;chr&gt;,\n#   `1829` &lt;chr&gt;, `1830` &lt;chr&gt;, `1831` &lt;chr&gt;, `1832` &lt;chr&gt;, `1833` &lt;chr&gt;,\n#   `1834` &lt;chr&gt;, `1835` &lt;chr&gt;, `1836` &lt;chr&gt;, `1837` &lt;chr&gt;, `1838` &lt;chr&gt;,\n#   `1839` &lt;chr&gt;, `1840` &lt;chr&gt;, `1841` &lt;chr&gt;, `1842` &lt;chr&gt;, `1843` &lt;chr&gt;, …\n\n\nOne big difference is that readr by default gives a message about number of rows, columns and types of variables that were loaded. Notice that it loaded every variable as character. That’s because the original file has “k” and “M” to indicate thousands and millions which is interpreted by R as text. We would need to convert those into proper numbers. Loading .tsv files is exactly the same, the only difference is that functions to do it are named read.tsv() and read_tsv().\nThere’s a bunch of things we can customize when loading data into R. Maybe you don’t need the entire dataset but only a subset of columns? Or you want to manually specify what types of variables you want? Or maybe the dataset you are using does not have variable names? You can specify all of these things as arguments to the loading functions. You can look at the documentation to lean more about them."
  },
  {
    "objectID": "06loading_data.html#loading-data-from-spss",
    "href": "06loading_data.html#loading-data-from-spss",
    "title": "Loading data",
    "section": "Loading data from SPSS",
    "text": "Loading data from SPSS\nData coming from statistical software can’t be loaded in such a simple way as above. Mainly because it stores more information. Here we’ll focus on .sav format which is used by SPSS. Apart from rows and columns (observations and variables) .sav format stores additional information e.g. on value labels - it is able to attach labels to numbers (like in Likert scales 1 can refer to ‘strongly disagree’). SPSS also allows to specify user-defined missing values (a common practice is to e.g. code missing values with 99).\nThis means we have to somehow deal with this additional information when loading .sav files. There are essentially two ways to go about it: reduce the amount of information stored or introduce a new type of values that can store this additional information. This first approach is taken by the foreign package. The second one is taken by haven package. Each of these approaches has some advantages and drawbacks. Stripping labels from values is generally easier and keeps consistency in terms of types of objects you are dealing with. You just keep working with numeric, integer, character or boolean values. The downside is that you lose some information and when e.g. saving a data file back to .sav format you can’t restore them. It might also cause problems if some variables need to be reversed beause you will lose information about that. Using haven allows you to keep all the labels but this is achieved by creating a new type of object: haven-labelled. Thanks to this you can keep all the information e.g. when saving back to .sav format. Most of the time you shouldn’t have problems with this new type of object but some statistical procedures from other packages might not like this and refuse to work. If that happens you can strip the labels using zap_labels().\n\nloading data with foreign\nThe main functiom in foreign is read.spss(). By default foregin will load data into a list rather than a dataframe. You can load into a data frame by setting the argument to.data.frame to TRUE. Another useful argument is use.value.labels which, if set to TRUE will convert the numerical values stored in .sav into their corresponding labels. This is the way foreign deals with labelled values: you can use either numeric values or their labels. In the documentation of the function you can read about additional arguments that control handling of labels.\nPlease notice though that the documentation for read.spss() function in foreign states that it was originally developed in 2000 and does not guarantee compatibility with newer versions of SPSS (whcih hasn’t changed much since but still. At least it doesn’t look like windows xp anymore).\nTo look at loading .sav files we’ll use a trimmed version (the whole file is huge and we don’t need it) of World Value Survey wave 6:\n\nlibrary(foreign)\ndf_foreign &lt;- read.spss(\"data/WVS6.sav\", to.data.frame = TRUE)\n\nOnce we have the data loaded lets see what types of values we have.\n\nstr(df_foreign[,1:3])\n\n'data.frame':   89565 obs. of  3 variables:\n $ V1 : Factor w/ 7 levels \"1981-1984\",\"1989-1993\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ V2 : Factor w/ 61 levels \"Not asked in survey\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ V2A: Factor w/ 61 levels \"Algeria\",\"Azerbaijan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nloading data with haven\nHaven package deals differently with loading labeled variables. It introduces a new type of variable: haven-labelled data. It is capable of storing both numeric values and labels attached to it by adding an attribute to the variable with labels. You can load a file in haven using read_spss() (notice the underscore).\n\nlibrary(haven)\ndf_haven &lt;- read_spss(\"data/WVS6.sav\")\n\nNow that we have loaded the dataset, lets look at the types of variables we have.\n\nstr(df_haven[,1:3])\n\ntibble [89,565 × 3] (S3: tbl_df/tbl/data.frame)\n $ V1 : dbl+lbl [1:89565] 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6...\n   ..@ label      : chr \"Wave\"\n   ..@ format.spss: chr \"F2.0\"\n   ..@ labels     : Named num [1:7] 1 2 3 4 5 6 7\n   .. ..- attr(*, \"names\")= chr [1:7] \"1981-1984\" \"1989-1993\" \"1994-1998\" \"1999-2004\" ...\n $ V2 : dbl+lbl [1:89565] 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...\n   ..@ label      : chr \"Country/region\"\n   ..@ format.spss: chr \"F4.0\"\n   ..@ labels     : Named num [1:61] -4 12 31 32 36 51 76 112 152 156 ...\n   .. ..- attr(*, \"names\")= chr [1:61] \"Not asked in survey\" \"Algeria\" \"Azerbaijan\" \"Argentina\" ...\n $ V2A: dbl+lbl [1:89565] 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...\n   ..@ label      : chr \"Country/regions [with split ups]\"\n   ..@ format.spss: chr \"F3.0\"\n   ..@ labels     : Named num [1:61] 12 31 32 36 51 76 112 152 156 158 ...\n   .. ..- attr(*, \"names\")= chr [1:61] \"Algeria\" \"Azerbaijan\" \"Argentina\" \"Australia\" ...\n - attr(*, \"label\")= chr \"filelabel\"\n\n\nAnd lets see what happens if we zap the labels.\n\ndf_nolab &lt;- zap_labels(df_haven)\nstr(df_nolab)\n\ntibble [89,565 × 8] (S3: tbl_df/tbl/data.frame)\n $ V1             : num [1:89565] 6 6 6 6 6 6 6 6 6 6 ...\n  ..- attr(*, \"label\")= chr \"Wave\"\n  ..- attr(*, \"format.spss\")= chr \"F2.0\"\n $ V2             : num [1:89565] 12 12 12 12 12 12 12 12 12 12 ...\n  ..- attr(*, \"label\")= chr \"Country/region\"\n  ..- attr(*, \"format.spss\")= chr \"F4.0\"\n $ V2A            : num [1:89565] 12 12 12 12 12 12 12 12 12 12 ...\n  ..- attr(*, \"label\")= chr \"Country/regions [with split ups]\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n $ COW            : num [1:89565] 615 615 615 615 615 615 615 615 615 615 ...\n  ..- attr(*, \"label\")= chr \"COW Country Code\"\n  ..- attr(*, \"format.spss\")= chr \"F2.0\"\n $ C_COW_ALPHA    : chr [1:89565] \"ALG\" \"ALG\" \"ALG\" \"ALG\" ...\n  ..- attr(*, \"label\")= chr \"Country code CoW alpha\"\n  ..- attr(*, \"format.spss\")= chr \"A8\"\n $ B_COUNTRY_ALPHA: chr [1:89565] \"DZA\" \"DZA\" \"DZA\" \"DZA\" ...\n  ..- attr(*, \"label\")= chr \"Country code ISO 3166 alpha\"\n  ..- attr(*, \"format.spss\")= chr \"A8\"\n $ V10            : num [1:89565] 2 2 2 2 1 2 2 2 2 1 ...\n  ..- attr(*, \"label\")= chr \"Feeling of happiness\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n $ V11            : num [1:89565] 1 2 2 1 3 1 2 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"State of health (subjective)\"\n  ..- attr(*, \"format.spss\")= chr \"F3.0\"\n - attr(*, \"label\")= chr \"filelabel\"\n\n\nOne last thing about the haven package. SPSS has the habit of storing categorical variables as labelled values (e.g. 1 - blue, 2 - brown). This can be a big problem because R will try to treat them as numeric values. This can lead to some ridiculous errors in analysis. To change a variable from haven-labelled to a factors with values taken from labels you can use as_factor() (again notice the underscore instead of dot)."
  },
  {
    "objectID": "06loading_data.html#loading-excel-files",
    "href": "06loading_data.html#loading-excel-files",
    "title": "Loading data",
    "section": "Loading excel files",
    "text": "Loading excel files\nOne additional thing you have to take into account when loading data from excel is that it can store a number of sheets in a single file. This has to be taken into account when loading such file into R.\nOne of the packages available for loading excel data is readxl.\nThe main difference in excel files is that they can store multiple sheets in one file. Because of that we need to specify which sheet we want to load. You can inspect the names of sheets with excel_sheets().\n\nlibrary(readxl)\n\nexcel_sheets(\"data/pop.xlsx\")\n\n[1] \"pop\"\n\n\nOnce we know the sheet names we can load the one that interests us. We need to specify the path to the file and which sheet we want to load. There are of course many other arguments that give additional control over what is loaded but they are quite similar to other loading functions. You can read more on them in the documentation of the function. If you need to load multiple sheets you can save the sheet names as an object and then loop over them using lapply().\n\ndf_excel &lt;- read_excel(\"data/pop.xlsx\", sheet = \"pop\")\n\nNow we can look at the top of the file:\n\nhead(df_excel)\n\n# A tibble: 6 × 302\n  country  `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809`\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 Afghani… 3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M  3.28M \n2 Angola   1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M  1.57M \n3 Albania  400k   402k   404k   405k   407k   409k   411k   413k   414k   416k  \n4 Andorra  2650   2650   2650   2650   2650   2650   2650   2650   2650   2650  \n5 UAE      40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k  40.2k \n6 Argenti… 534k   520k   506k   492k   479k   466k   453k   441k   429k   417k  \n# ℹ 291 more variables: `1810` &lt;chr&gt;, `1811` &lt;chr&gt;, `1812` &lt;chr&gt;, `1813` &lt;chr&gt;,\n#   `1814` &lt;chr&gt;, `1815` &lt;chr&gt;, `1816` &lt;chr&gt;, `1817` &lt;chr&gt;, `1818` &lt;chr&gt;,\n#   `1819` &lt;chr&gt;, `1820` &lt;chr&gt;, `1821` &lt;chr&gt;, `1822` &lt;chr&gt;, `1823` &lt;chr&gt;,\n#   `1824` &lt;chr&gt;, `1825` &lt;chr&gt;, `1826` &lt;chr&gt;, `1827` &lt;chr&gt;, `1828` &lt;chr&gt;,\n#   `1829` &lt;chr&gt;, `1830` &lt;chr&gt;, `1831` &lt;chr&gt;, `1832` &lt;chr&gt;, `1833` &lt;chr&gt;,\n#   `1834` &lt;chr&gt;, `1835` &lt;chr&gt;, `1836` &lt;chr&gt;, `1837` &lt;chr&gt;, `1838` &lt;chr&gt;,\n#   `1839` &lt;chr&gt;, `1840` &lt;chr&gt;, `1841` &lt;chr&gt;, `1842` &lt;chr&gt;, `1843` &lt;chr&gt;, …"
  },
  {
    "objectID": "06loading_data.html#loading-jsons",
    "href": "06loading_data.html#loading-jsons",
    "title": "Loading data",
    "section": "Loading jsons",
    "text": "Loading jsons\nThere are situations in which you might work with data that does not come from simple tables but is stored in completely different way. One example that we’ll introduce here is the json format. Json is short for Javascript Object Notation and is a common way of storing data in the web.\nJson stores data as key - value pairs. These might not approximate tabular format and can be nested and fairly complicated. This type of data is especially common when downloading data directly from the web (e.g. social media data) or from APIs. You can imagine a file that stores information on each user of a website: their username, password and all posts that they have created along with information on each post like their creation date. It might look something like this:\n\n\n\n\n\nBecause json can be a complicated and nested structure the type of data that best approximates it in R is a list. There are ways to ask R to try and handle such list structure and try to convert it into a data frame but it does not always work. Cleaning an unevenly nested json can be a real pain sometimes!\nWe’ll look at an example of a NASA API that stores information on the number of people currently present on space stations. The package we’ll use to load the data is jsonlite.\n\nlibrary(jsonlite)\n\nnasa &lt;- fromJSON(\"http://api.open-notify.org/astros.json\")\n\nNotice how the loaded object looks like. It is a list with 3 elements: number of people as an integer, a dataframe of astronauts and a string “success”.\n\nnasa\n\n$message\n[1] \"success\"\n\n$people\n                name craft\n1    Jasmin Moghbeli   ISS\n2   Andreas Mogensen   ISS\n3   Satoshi Furukawa   ISS\n4 Konstantin Borisov   ISS\n5     Oleg Kononenko   ISS\n6       Nikolai Chub   ISS\n7       Loral O'Hara   ISS\n\n$number\n[1] 7"
  },
  {
    "objectID": "06loading_data.html#exercises",
    "href": "06loading_data.html#exercises",
    "title": "Loading data",
    "section": "exercises",
    "text": "exercises\n\nDownload the data from the link here. Load the dataset, make sure that each variable is loaded with the proper format. Next, calculate the summary statistics (means and standard deviations) of attention variable in each experimental condition.\nDownload an excel file from here. Load it and put all sheets together into one data frame. Can you automate the process e.g. using a loop?\nUse the The Metropolitan Museum of Art API to load a json that stores information on IDs of all works by Magdalena Abakanowicz available in the API. The API documentation can be found here: https://metmuseum.github.io/. Next use the extracted IDs to load information on each of Abakanowicz’s artworks."
  },
  {
    "objectID": "07data_wrangling.html",
    "href": "07data_wrangling.html",
    "title": "data wrangling",
    "section": "",
    "text": "Now that we have our dataset loaded we can finally get to work with it! We’ll start with the basics of data wrangling: subsetting datasets, sorting variables, changing variables and getting basic summaries. These are the standard things that you might want to do before any statistical modelling.\n\n\nFor data wrangling we’ll be working within tidyverse throughout this course. Tidyverse is a set of packages designed for working with data in a clean, readable way. A huge advantage (apart from readability) is that all packages in tidyverse are designed to be compatible with each other and share common “grammar” and way of doing things. This way it is easy to combine them to do a lot of different things with your data. We’ve already met one package from this collection: readr. Other packages include:\n\ndplyr: package for data wrangling. We’ll focus on it in this class\ntidyr: package for tidying data and reshaping it. We’ll look at it in the next class\nggplot2: the go to package for data visualization in R. Absolutely the best of the best when it comes to plotting.\nforcats: package for working with factors\nstrings: package for working with text data\npurrr: functional programming stuff in R like easier iteration within tidyverse\ntibble: package that introduces slightly altered data frames\n\nTidyverse is not the only way in R for data wrangling (other package often used is data.table, a new alternative is polars). If you don’t want additional packages you don’t even need them, you can do almost everything in base R if you want to. So why choose tidyverse? First of all it’s extremely intuitive. Writing and reading code in tidyverse feels almost like writing plain text of what you want to do with your data. Thanks to pipes it also made code much more readable (more on that in a moment although there are now pipe alternatives). One downside of tidyverse is that it is significantly slower than other packages. If speed is paramount you might want to consider switching to data.table or polars.\n\n\n\nSo far if we wanted to use multiple functions in a single call we had to wrap one function inside another e.g. if we wanted to take a list of vectors, calculate the mean of each vector and then find out the highest mean we could do something like this:\n\nmax(sapply(list(c(1,2,3), c(4,5,6), c(6,7,8)), mean))\n\n[1] 7\n\n\nIt’s not the easiest code to read, right? When combining functions this way you need to read them inside out. This is not how people read. It would e much easier if we could read code more linearly e.g. from left to right and top to bottom. Enter the pipe! The pipe operator allows you to chain together functions in a readable way. The basic idea (there’s more to pipes though) is to take what is on the left hand side of the pipe and pass it as the first argument of whatever is on the right hand side of the pipe. This changes the inside-out into left-to-right code. There are 2 pipes in R. The first one comes from the magrittr package and this is the one used in tidyvese. This pipe looks like this: %&gt;%. If we wanted to rewrite the code above using this pipe it would look like this:\n\nlibrary(magrittr)\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) %&gt;%\n  sapply(mean) %&gt;%\n  max()\n\n[1] 7\n\n\nIt’s much easier to understand what this code does right? An alternative introduced in R 4.1 is the native pipe: |&gt;. The basic functionality is pretty much the same as in the magrittr pipe but you don’t need to load any packages to use it (you might need to enable native pipe in the global options in Tools bar in RStudio). The same code as above but with native pipe looks like this:\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) |&gt;\n  sapply(mean) |&gt;\n  max()\n\n[1] 7\n\n\nYou might wonder why have two kinds of pipes one of which needs loading a new package? The first reason is very simple: magrittr pipe is older. There are however a few differences. You can read about the details here. Remember that pipe automatically passes what is on the left as the first argument to whatever is on the right of the pipe? What if you need to pass it not as the first but second or third argument? Both pipe operators have a placeholder argument that can be used in such situations. %&gt;% has the . operator and |&gt; has _. The difference between them is that _ can only be used once and has to be used with named arguments. Here’s an example of how placeholder argument can work: append() allows you to join two vectors together. The vector passed as the second argument is appended to the one passed as the first argument:\n\nx &lt;- c(1,2,3)\ny &lt;- c(4,5,6)\nx %&gt;%\n  append(y, .)\n\n[1] 4 5 6 1 2 3\n\n\nGenerally, the differences boil down to simplicity: native pipe was deliberately created to be a simpler operator with less functionality. Most of the time you won’t notice much difference (maybe except for how the placeholder argument works).\n\n\n\nNow we can get to the basics of data wrangling in dplyr package. We’ll look at the storms dataset in the dplyr package. It stores information on date, place, status and some other things about storms from 1975 to 2021. The dataset stores multiple observations from each storm because measurements were made every few hours. Before we move one to working with data lets introduce one function: glimpse(). It’s a bit like str() but is a bit more readable for dataframes. This function can give you a concise look at what variables you have in your dataset. Lets load tidyverse, our dataset and look at it:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"storms\")\n\nglimpse(storms)\n\nRows: 19,066\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe dataset has 19066 rows and 13 columns. We can also see that we have various types of variables: many numeric, one factor and one character.\n\n\nYou can subset a dataframe either by columns or by rows. If you want to extract a subset of rows based on some logical conditions you can use filter(). Lets say we want only storms from 2020:\n\nstorms %&gt;%\n  filter(year == 2020)\n\n# A tibble: 863 × 13\n   name    year month   day  hour   lat  long status     category  wind pressure\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Arthur  2020     5    16    18  28   -78.7 tropical …       NA    30     1008\n 2 Arthur  2020     5    17     0  28.9 -78   tropical …       NA    35     1006\n 3 Arthur  2020     5    17     6  29.6 -77.6 tropical …       NA    35     1004\n 4 Arthur  2020     5    17    12  30.3 -77.5 tropical …       NA    35     1003\n 5 Arthur  2020     5    17    18  31   -77.3 tropical …       NA    40     1003\n 6 Arthur  2020     5    18     0  31.9 -77   tropical …       NA    40     1003\n 7 Arthur  2020     5    18     6  33.1 -76.7 tropical …       NA    40     1002\n 8 Arthur  2020     5    18    12  34.4 -75.9 tropical …       NA    45     1000\n 9 Arthur  2020     5    18    18  35.5 -74.7 tropical …       NA    45      993\n10 Arthur  2020     5    19     0  36.2 -73.1 tropical …       NA    50      991\n# ℹ 853 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nWe can also filter based on multiple conditions. This works exactly like all the logical operations we’ve seen previously. One difference is that you can use , instead of &. Lets say we want to get all storms from June 2020:\n\nstorms %&gt;%\n  filter(year == 2020, month == 6)\n\n# A tibble: 57 × 13\n   name       year month   day  hour   lat  long status  category  wind pressure\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Cristobal  2020     6     1    18  19.4 -90.9 tropic…       NA    25     1006\n 2 Cristobal  2020     6     2     0  19.6 -91.6 tropic…       NA    25     1005\n 3 Cristobal  2020     6     2     6  19.6 -92.1 tropic…       NA    30     1005\n 4 Cristobal  2020     6     2    12  19.5 -92.5 tropic…       NA    35     1004\n 5 Cristobal  2020     6     2    18  19.2 -92.6 tropic…       NA    40     1001\n 6 Cristobal  2020     6     3     0  19   -92.5 tropic…       NA    45      996\n 7 Cristobal  2020     6     3     6  18.9 -92.3 tropic…       NA    50      994\n 8 Cristobal  2020     6     3    12  18.8 -92.2 tropic…       NA    50      993\n 9 Cristobal  2020     6     3    13  18.7 -92.1 tropic…       NA    50      993\n10 Cristobal  2020     6     3    18  18.5 -91.9 tropic…       NA    45      994\n# ℹ 47 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nThere were 57 storms recorded in June 2020.\nIf you want to select only certain columns from a dataset you can use select(). E.g. if we want only the name, latitude and longitude of the storm we can do it like this:\n\nstorms %&gt;%\n  select(name, lat, long)\n\n# A tibble: 19,066 × 3\n   name    lat  long\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amy    27.5 -79  \n 2 Amy    28.5 -79  \n 3 Amy    29.5 -79  \n 4 Amy    30.5 -79  \n 5 Amy    31.5 -78.8\n 6 Amy    32.4 -78.7\n 7 Amy    33.3 -78  \n 8 Amy    34   -77  \n 9 Amy    34.4 -75.8\n10 Amy    34   -74.8\n# ℹ 19,056 more rows\n\n\nIf you want to select a range of columns you can use :\n\nstorms %&gt;%\n  select(year:hour)\n\n# A tibble: 19,066 × 4\n    year month   day  hour\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  1975     6    27     0\n 2  1975     6    27     6\n 3  1975     6    27    12\n 4  1975     6    27    18\n 5  1975     6    28     0\n 6  1975     6    28     6\n 7  1975     6    28    12\n 8  1975     6    28    18\n 9  1975     6    29     0\n10  1975     6    29     6\n# ℹ 19,056 more rows\n\n\nA particular situation in which you might want to subset a dataset is to get the rows with highest/lowest values of a variable, get the first/last rows or draw a random sample from the dataset. All of these can be achieved by different versions of slice(). slice_sample() will draw a random sample from the dataset (either by number or proportion). You can also specify if you want to draw with replacements:\n\nstorms %&gt;%\n  slice_sample(n = 100)\n\n# A tibble: 100 × 13\n   name      year month   day  hour   lat  long status   category  wind pressure\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 AL021999  1999     7     3     4  20.5 -97   tropica…       NA    30     1004\n 2 Wilfred   2020     9    20     6  14.9 -41.5 tropica…       NA    35     1007\n 3 Leslie    2018     9    23    12  33.2 -46.5 subtrop…       NA    35     1002\n 4 Emily     1987     9    24     6  23.2 -73   tropica…       NA    45     1001\n 5 Edouard   2002     9     5     0  29.4 -81.1 tropica…       NA    35     1009\n 6 Nana      2020     9     3     6  16.8 -88.3 hurrica…        1    65      994\n 7 Marilyn   1995     9    18    18  27.2 -69.3 hurrica…        1    80      966\n 8 Barry     2013     6    16     0  12.4 -81.5 other l…       NA    20     1009\n 9 Joaquin   2015     9    27     0  26.9 -68.6 other l…       NA    20     1011\n10 Leslie    2012     8    31     0  14.7 -45.8 tropica…       NA    45     1002\n# ℹ 90 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nslice_min() and slice_max() allow you to get rows with highest values on a variable.\n\nstorms %&gt;%\n  slice_max(wind, n = 10)\n\n# A tibble: 24 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 14 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nFinally slice_head() and slice_tail() allow you to get n first or last rows from a dataset.\n\nstorms %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 13\n  name   year month   day  hour   lat  long status       category  wind pressure\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1 Amy    1975     6    27     0  27.5 -79   tropical de…       NA    25     1013\n2 Amy    1975     6    27     6  28.5 -79   tropical de…       NA    25     1013\n3 Amy    1975     6    27    12  29.5 -79   tropical de…       NA    25     1013\n4 Amy    1975     6    27    18  30.5 -79   tropical de…       NA    25     1013\n5 Amy    1975     6    28     0  31.5 -78.8 tropical de…       NA    25     1012\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nOne last thing about filtering. Sometimes you want to filter all unique values of a variable. In order to do it you can use distinct(). It will extract all unique values of a variable. By default it will return only the column with distinct values and drop all the other columns from the dataframe. If you want to keep all the other variables (though remember that it will probably keep only the first entry for each unique value!) you can set the .keep_all argument to TRUE.\n\nstorms %&gt;%\n  distinct(category)\n\n# A tibble: 6 × 1\n  category\n     &lt;dbl&gt;\n1       NA\n2        1\n3        3\n4        2\n5        4\n6        5\n\n\n\n\n\nSorting datasets based on variables is super simple. You can use the arrange() function and if you need to sort in descending order use desc() inside it. Lets say we want to find the storm with the strongest wind:\n\nstorms %&gt;%\n  arrange(desc(wind))\n\n# A tibble: 19,066 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 19,056 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nLooks like Allen from 1980 was the strongest storm.\n\n\n\nCounting values in variables is very simple, just use count(). One thing to remember is that count() will returned a different dataframe. Unless you specify anything additional it will return a dataframe with 2 columns: one will contain all unique values of the variable you counted and the other one, named n will contain their counts (you can specify the name variable to change that to something else). Setting sort argument to TRUE will sort the counts in descending order. E.g. if we want to find out which year had the most measurements of storms (and not the number of storms! Remember that each row is 1 measurement of 1 storm) we can do it with:\n\nstorms %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 47 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2005   873\n 2  2020   863\n 3  1995   762\n 4  2010   663\n 5  2012   654\n 6  2017   610\n 7  2018   608\n 8  2004   604\n 9  2003   593\n10  2021   592\n# ℹ 37 more rows\n\n\n\n\n\nA common task when wrangling data is creating new variables in an already existing dataset. You can do it by using mutate(). Lets say we want to create a new variable that stores information on whether the storm was during summer (June, July, August) or not:\n\nstorms %&gt;%\n  mutate(summer = ifelse(month == 6 | month == 7 | month == 8, TRUE, FALSE))\n\n# A tibble: 19,066 × 14\n   name   year month   day  hour   lat  long status      category  wind pressure\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n# ℹ 19,056 more rows\n# ℹ 3 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;, summer &lt;lgl&gt;\n\n\nRemember that if you want to keep the variable you need to assign the new dataset to an object.\n\n\n\nAnother extremely common task is to get summaries about our dataset. We can do it with summarise() function. e.g. what if we want to see the mean and standard deviation of strength of wind (for now across all measurements):\n\nstorms %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T),\n            sd_wind = sd(wind, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_wind sd_wind\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      50.0    25.5\n\n\nNotice that the shape of the dataset has changed now. The columns are now the summaries and not the original variables.\n\n\n\nSo far we have been calculating things on entire datasets. In many situations you want to calculate something separately for each level of a categorical variable (much like tapply() earlier). To group a dataset we can use group_by(). You can also group by multiple variables at once by separating them by a coma. R will group by the first variable and then by the second etc. This is especially useful for creating summaries. E.g. if we want to get average wind speed for each storm we can easily do it:\n\nstorms %&gt;%\n  group_by(name) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n# A tibble: 258 × 2\n   name     mean_wind\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 AL011993      29.5\n 2 AL012000      25  \n 3 AL021992      29  \n 4 AL021994      24.2\n 5 AL021999      28.8\n 6 AL022000      29.2\n 7 AL022001      25  \n 8 AL022003      30  \n 9 AL022006      31.5\n10 AL031987      21.2\n# ℹ 248 more rows\n\n\nSimilarly if we want to calculate average wind speed from all measurements for each year and month:\n\nstorms %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 253 × 3\n# Groups:   year [47]\n    year month mean_wind\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1  1975     6      37.5\n 2  1975     7      49.7\n 3  1975     8      44.5\n 4  1975     9      55.9\n 5  1975    10      62.4\n 6  1976     8      55.9\n 7  1976     9      61.3\n 8  1976    10      51.0\n 9  1977     8      53  \n10  1977     9      50.6\n# ℹ 243 more rows\n\n\nActually the summarise() function has its own argument for calculating grouped summaries. You can specify .by argument inside the summarise() function. If you use group_by() then summarise() will by default drop the last level of grouping.\nOne more important thing about the group_by() function is that it works by adding an attribute to the dataframe. This means that after a group_by() all subsequent operations will be conducted on the grouped dataframe. E.g. if you sort after grouping then sorting will be conducted within each group separately. You can drop the grouping with ungroup(). There is one last special kind of grouping you might want to use - sometimes you want to perform some operation separately on each row (e.g. calculate the average of a multi item scale for each participant). You can do it with rowwise().\n\n\n\nThere are situations when you want to perform the same operation on multiple columns (e.g. calculate the mean and standard deviation of multiple variables). You can do it by hand but this can be tedious. To simplify it you can use across() inside mutate() or summarise(). the syntax of across() is as follows: the first argument, .cols specifies which columns to perform operations on. The second argument .fns specifies which functions to apply. You have to provide it a list of functions (preferably named list) or a formula. Finally the .names argument specifies how to automatically assign new variable names. E.g. “{.col}_{.fn}” will create variables with column name, underscore and function name (that’s why named list is useful here). If we want to get the mean and standard deviation of wind speed and pressure in each category of storms we could do it in a few lines of code (by the way, notice how group_by() by default includes NA as a separate category):\n\nstorms %&gt;%\n  group_by(category) %&gt;%\n  summarise(across(.cols = c(\"wind\", \"pressure\"), .fns = list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 6 × 5\n  category wind_mean wind_sd pressure_mean pressure_sd\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1        1      71.0    5.55          981.        9.21\n2        2      89.5    3.77          967.        8.94\n3        3     104.     4.18          955.        8.91\n4        4     122.     6.39          940.        9.43\n5        5     147.     6.22          918.       12.0 \n6       NA      38.1   12.0          1002.        9.24\n\n\n\n\n\nOne more useful thing for data wrangling is a set of functions making it easier to select multiple columns based on some condition. There are a few helper functions you can use to do that. They do basically what their names suggest. These verbs are as follows: starts_with(), ends_with() and contains(). This way you don’t have to manually type all the names if they have something in common (e.g. they are items from the same scale so they are named ``scale_1, scale_2 etc.). E.g. lets say we want to get all the columns that end with “diameter”:\n\nstorms %&gt;%\n  select(ends_with(\"diameter\"))\n\n# A tibble: 19,066 × 2\n   tropicalstorm_force_diameter hurricane_force_diameter\n                          &lt;int&gt;                    &lt;int&gt;\n 1                           NA                       NA\n 2                           NA                       NA\n 3                           NA                       NA\n 4                           NA                       NA\n 5                           NA                       NA\n 6                           NA                       NA\n 7                           NA                       NA\n 8                           NA                       NA\n 9                           NA                       NA\n10                           NA                       NA\n# ℹ 19,056 more rows\n\n\nNice! These verbs also work nicely inside across(). One thing to be aware of: if no column matches what you ask for you won’t get an error but a dataframe with all the rows but 0 columns:\n\nstorms %&gt;%\n  select(contains(\"some_weird_name\"))\n\n# A tibble: 19,066 × 0\n\n\n\n\n\n\nWhere tidyverse really shines is in combining multiple functions together with pipes. Through different orders of the functions described above we can get a ton of things out of our dataset. This already gives us the ability to anwser a number of questions that might be very interesting for analysis.\n\nExample 1: Lets say we want to find out the name of storm from each category that had the highest average pressure in 1989.\n\nstorms %&gt;%\n  filter(year == 1989) %&gt;%\n  group_by(category, name) %&gt;%\n  summarise(mean_pressure = mean(pressure, na.rm = T)) %&gt;%\n  slice_max(mean_pressure, n = 1)\n\n`summarise()` has grouped output by 'category'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   category [6]\n  category name      mean_pressure\n     &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1        1 Chantal            987 \n2        2 Dean               972.\n3        3 Hugo               953.\n4        4 Gabrielle          943.\n5        5 Hugo               918 \n6       NA Barry             1012.\n\n\nExample 2: Lets say we want to find the average wind speed at each hour of the day but we want that in kilometers per hours rather than knots (as is in the database). 1 knot is around 1.852 km/h.\n\nstorms %&gt;%\n  mutate(wind_km = wind*1.852) %&gt;%\n  group_by(hour) %&gt;%\n  summarise(mean_wind_km = mean(wind, na.rm = T))\n\n# A tibble: 24 × 2\n    hour mean_wind_km\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1     0         49.6\n 2     1         76.2\n 3     2         64.4\n 4     3         76.7\n 5     4         70.6\n 6     5         76.2\n 7     6         49.7\n 8     7         80.8\n 9     8         68.2\n10     9         60.9\n# ℹ 14 more rows\n\n\n\n\n\n\nThroughout the exercises you’ll still work with the storms dataset\n\nFind out which year had the most storms (remember that each row is 1 measurement of 1 storm. You have to find the year with the most storms and not the most measurements!)\nFind the average wind speed for each storm in 2018 and then sort from the highest to the lowest.\nCalculate the mean and standard deviation of measurement pressure for each month of the year"
  },
  {
    "objectID": "07data_wrangling.html#tidyverse",
    "href": "07data_wrangling.html#tidyverse",
    "title": "data wrangling",
    "section": "",
    "text": "For data wrangling we’ll be working within tidyverse throughout this course. Tidyverse is a set of packages designed for working with data in a clean, readable way. A huge advantage (apart from readability) is that all packages in tidyverse are designed to be compatible with each other and share common “grammar” and way of doing things. This way it is easy to combine them to do a lot of different things with your data. We’ve already met one package from this collection: readr. Other packages include:\n\ndplyr: package for data wrangling. We’ll focus on it in this class\ntidyr: package for tidying data and reshaping it. We’ll look at it in the next class\nggplot2: the go to package for data visualization in R. Absolutely the best of the best when it comes to plotting.\nforcats: package for working with factors\nstrings: package for working with text data\npurrr: functional programming stuff in R like easier iteration within tidyverse\ntibble: package that introduces slightly altered data frames\n\nTidyverse is not the only way in R for data wrangling (other package often used is data.table, a new alternative is polars). If you don’t want additional packages you don’t even need them, you can do almost everything in base R if you want to. So why choose tidyverse? First of all it’s extremely intuitive. Writing and reading code in tidyverse feels almost like writing plain text of what you want to do with your data. Thanks to pipes it also made code much more readable (more on that in a moment although there are now pipe alternatives). One downside of tidyverse is that it is significantly slower than other packages. If speed is paramount you might want to consider switching to data.table or polars."
  },
  {
    "objectID": "07data_wrangling.html#the-pipe",
    "href": "07data_wrangling.html#the-pipe",
    "title": "data wrangling",
    "section": "",
    "text": "So far if we wanted to use multiple functions in a single call we had to wrap one function inside another e.g. if we wanted to take a list of vectors, calculate the mean of each vector and then find out the highest mean we could do something like this:\n\nmax(sapply(list(c(1,2,3), c(4,5,6), c(6,7,8)), mean))\n\n[1] 7\n\n\nIt’s not the easiest code to read, right? When combining functions this way you need to read them inside out. This is not how people read. It would e much easier if we could read code more linearly e.g. from left to right and top to bottom. Enter the pipe! The pipe operator allows you to chain together functions in a readable way. The basic idea (there’s more to pipes though) is to take what is on the left hand side of the pipe and pass it as the first argument of whatever is on the right hand side of the pipe. This changes the inside-out into left-to-right code. There are 2 pipes in R. The first one comes from the magrittr package and this is the one used in tidyvese. This pipe looks like this: %&gt;%. If we wanted to rewrite the code above using this pipe it would look like this:\n\nlibrary(magrittr)\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) %&gt;%\n  sapply(mean) %&gt;%\n  max()\n\n[1] 7\n\n\nIt’s much easier to understand what this code does right? An alternative introduced in R 4.1 is the native pipe: |&gt;. The basic functionality is pretty much the same as in the magrittr pipe but you don’t need to load any packages to use it (you might need to enable native pipe in the global options in Tools bar in RStudio). The same code as above but with native pipe looks like this:\n\nlist(c(1,2,3), c(4,5,6), c(6,7,8)) |&gt;\n  sapply(mean) |&gt;\n  max()\n\n[1] 7\n\n\nYou might wonder why have two kinds of pipes one of which needs loading a new package? The first reason is very simple: magrittr pipe is older. There are however a few differences. You can read about the details here. Remember that pipe automatically passes what is on the left as the first argument to whatever is on the right of the pipe? What if you need to pass it not as the first but second or third argument? Both pipe operators have a placeholder argument that can be used in such situations. %&gt;% has the . operator and |&gt; has _. The difference between them is that _ can only be used once and has to be used with named arguments. Here’s an example of how placeholder argument can work: append() allows you to join two vectors together. The vector passed as the second argument is appended to the one passed as the first argument:\n\nx &lt;- c(1,2,3)\ny &lt;- c(4,5,6)\nx %&gt;%\n  append(y, .)\n\n[1] 4 5 6 1 2 3\n\n\nGenerally, the differences boil down to simplicity: native pipe was deliberately created to be a simpler operator with less functionality. Most of the time you won’t notice much difference (maybe except for how the placeholder argument works)."
  },
  {
    "objectID": "07data_wrangling.html#the-basic-dplyr-function",
    "href": "07data_wrangling.html#the-basic-dplyr-function",
    "title": "data wrangling",
    "section": "",
    "text": "Now we can get to the basics of data wrangling in dplyr package. We’ll look at the storms dataset in the dplyr package. It stores information on date, place, status and some other things about storms from 1975 to 2021. The dataset stores multiple observations from each storm because measurements were made every few hours. Before we move one to working with data lets introduce one function: glimpse(). It’s a bit like str() but is a bit more readable for dataframes. This function can give you a concise look at what variables you have in your dataset. Lets load tidyverse, our dataset and look at it:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"storms\")\n\nglimpse(storms)\n\nRows: 19,066\nColumns: 13\n$ name                         &lt;chr&gt; \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\", \"Amy\",…\n$ year                         &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975,…\n$ month                        &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ day                          &lt;int&gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 2…\n$ hour                         &lt;dbl&gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18,…\n$ lat                          &lt;dbl&gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3,…\n$ long                         &lt;dbl&gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7,…\n$ status                       &lt;fct&gt; tropical depression, tropical depression,…\n$ category                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wind                         &lt;int&gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 4…\n$ pressure                     &lt;int&gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011,…\n$ tropicalstorm_force_diameter &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hurricane_force_diameter     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThe dataset has 19066 rows and 13 columns. We can also see that we have various types of variables: many numeric, one factor and one character.\n\n\nYou can subset a dataframe either by columns or by rows. If you want to extract a subset of rows based on some logical conditions you can use filter(). Lets say we want only storms from 2020:\n\nstorms %&gt;%\n  filter(year == 2020)\n\n# A tibble: 863 × 13\n   name    year month   day  hour   lat  long status     category  wind pressure\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Arthur  2020     5    16    18  28   -78.7 tropical …       NA    30     1008\n 2 Arthur  2020     5    17     0  28.9 -78   tropical …       NA    35     1006\n 3 Arthur  2020     5    17     6  29.6 -77.6 tropical …       NA    35     1004\n 4 Arthur  2020     5    17    12  30.3 -77.5 tropical …       NA    35     1003\n 5 Arthur  2020     5    17    18  31   -77.3 tropical …       NA    40     1003\n 6 Arthur  2020     5    18     0  31.9 -77   tropical …       NA    40     1003\n 7 Arthur  2020     5    18     6  33.1 -76.7 tropical …       NA    40     1002\n 8 Arthur  2020     5    18    12  34.4 -75.9 tropical …       NA    45     1000\n 9 Arthur  2020     5    18    18  35.5 -74.7 tropical …       NA    45      993\n10 Arthur  2020     5    19     0  36.2 -73.1 tropical …       NA    50      991\n# ℹ 853 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nWe can also filter based on multiple conditions. This works exactly like all the logical operations we’ve seen previously. One difference is that you can use , instead of &. Lets say we want to get all storms from June 2020:\n\nstorms %&gt;%\n  filter(year == 2020, month == 6)\n\n# A tibble: 57 × 13\n   name       year month   day  hour   lat  long status  category  wind pressure\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Cristobal  2020     6     1    18  19.4 -90.9 tropic…       NA    25     1006\n 2 Cristobal  2020     6     2     0  19.6 -91.6 tropic…       NA    25     1005\n 3 Cristobal  2020     6     2     6  19.6 -92.1 tropic…       NA    30     1005\n 4 Cristobal  2020     6     2    12  19.5 -92.5 tropic…       NA    35     1004\n 5 Cristobal  2020     6     2    18  19.2 -92.6 tropic…       NA    40     1001\n 6 Cristobal  2020     6     3     0  19   -92.5 tropic…       NA    45      996\n 7 Cristobal  2020     6     3     6  18.9 -92.3 tropic…       NA    50      994\n 8 Cristobal  2020     6     3    12  18.8 -92.2 tropic…       NA    50      993\n 9 Cristobal  2020     6     3    13  18.7 -92.1 tropic…       NA    50      993\n10 Cristobal  2020     6     3    18  18.5 -91.9 tropic…       NA    45      994\n# ℹ 47 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nThere were 57 storms recorded in June 2020.\nIf you want to select only certain columns from a dataset you can use select(). E.g. if we want only the name, latitude and longitude of the storm we can do it like this:\n\nstorms %&gt;%\n  select(name, lat, long)\n\n# A tibble: 19,066 × 3\n   name    lat  long\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Amy    27.5 -79  \n 2 Amy    28.5 -79  \n 3 Amy    29.5 -79  \n 4 Amy    30.5 -79  \n 5 Amy    31.5 -78.8\n 6 Amy    32.4 -78.7\n 7 Amy    33.3 -78  \n 8 Amy    34   -77  \n 9 Amy    34.4 -75.8\n10 Amy    34   -74.8\n# ℹ 19,056 more rows\n\n\nIf you want to select a range of columns you can use :\n\nstorms %&gt;%\n  select(year:hour)\n\n# A tibble: 19,066 × 4\n    year month   day  hour\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  1975     6    27     0\n 2  1975     6    27     6\n 3  1975     6    27    12\n 4  1975     6    27    18\n 5  1975     6    28     0\n 6  1975     6    28     6\n 7  1975     6    28    12\n 8  1975     6    28    18\n 9  1975     6    29     0\n10  1975     6    29     6\n# ℹ 19,056 more rows\n\n\nA particular situation in which you might want to subset a dataset is to get the rows with highest/lowest values of a variable, get the first/last rows or draw a random sample from the dataset. All of these can be achieved by different versions of slice(). slice_sample() will draw a random sample from the dataset (either by number or proportion). You can also specify if you want to draw with replacements:\n\nstorms %&gt;%\n  slice_sample(n = 100)\n\n# A tibble: 100 × 13\n   name      year month   day  hour   lat  long status   category  wind pressure\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 AL021999  1999     7     3     4  20.5 -97   tropica…       NA    30     1004\n 2 Wilfred   2020     9    20     6  14.9 -41.5 tropica…       NA    35     1007\n 3 Leslie    2018     9    23    12  33.2 -46.5 subtrop…       NA    35     1002\n 4 Emily     1987     9    24     6  23.2 -73   tropica…       NA    45     1001\n 5 Edouard   2002     9     5     0  29.4 -81.1 tropica…       NA    35     1009\n 6 Nana      2020     9     3     6  16.8 -88.3 hurrica…        1    65      994\n 7 Marilyn   1995     9    18    18  27.2 -69.3 hurrica…        1    80      966\n 8 Barry     2013     6    16     0  12.4 -81.5 other l…       NA    20     1009\n 9 Joaquin   2015     9    27     0  26.9 -68.6 other l…       NA    20     1011\n10 Leslie    2012     8    31     0  14.7 -45.8 tropica…       NA    45     1002\n# ℹ 90 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nslice_min() and slice_max() allow you to get rows with highest values on a variable.\n\nstorms %&gt;%\n  slice_max(wind, n = 10)\n\n# A tibble: 24 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 14 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nFinally slice_head() and slice_tail() allow you to get n first or last rows from a dataset.\n\nstorms %&gt;%\n  slice_head(n = 5)\n\n# A tibble: 5 × 13\n  name   year month   day  hour   lat  long status       category  wind pressure\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1 Amy    1975     6    27     0  27.5 -79   tropical de…       NA    25     1013\n2 Amy    1975     6    27     6  28.5 -79   tropical de…       NA    25     1013\n3 Amy    1975     6    27    12  29.5 -79   tropical de…       NA    25     1013\n4 Amy    1975     6    27    18  30.5 -79   tropical de…       NA    25     1013\n5 Amy    1975     6    28     0  31.5 -78.8 tropical de…       NA    25     1012\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nOne last thing about filtering. Sometimes you want to filter all unique values of a variable. In order to do it you can use distinct(). It will extract all unique values of a variable. By default it will return only the column with distinct values and drop all the other columns from the dataframe. If you want to keep all the other variables (though remember that it will probably keep only the first entry for each unique value!) you can set the .keep_all argument to TRUE.\n\nstorms %&gt;%\n  distinct(category)\n\n# A tibble: 6 × 1\n  category\n     &lt;dbl&gt;\n1       NA\n2        1\n3        3\n4        2\n5        4\n6        5\n\n\n\n\n\nSorting datasets based on variables is super simple. You can use the arrange() function and if you need to sort in descending order use desc() inside it. Lets say we want to find the storm with the strongest wind:\n\nstorms %&gt;%\n  arrange(desc(wind))\n\n# A tibble: 19,066 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Allen    1980     8     7    18  21.8 -86.4 hurricane        5   165      899\n 2 Gilbert  1988     9    14     0  19.7 -83.8 hurricane        5   160      888\n 3 Wilma    2005    10    19    12  17.3 -82.8 hurricane        5   160      882\n 4 Dorian   2019     9     1    16  26.5 -77   hurricane        5   160      910\n 5 Dorian   2019     9     1    18  26.5 -77.1 hurricane        5   160      910\n 6 Allen    1980     8     5    12  15.9 -70.5 hurricane        5   155      932\n 7 Allen    1980     8     7    12  21   -84.8 hurricane        5   155      910\n 8 Allen    1980     8     8     0  22.2 -87.9 hurricane        5   155      920\n 9 Allen    1980     8     9     6  25   -94.2 hurricane        5   155      909\n10 Gilbert  1988     9    14     6  19.9 -85.3 hurricane        5   155      889\n# ℹ 19,056 more rows\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;\n\n\nLooks like Allen from 1980 was the strongest storm.\n\n\n\nCounting values in variables is very simple, just use count(). One thing to remember is that count() will returned a different dataframe. Unless you specify anything additional it will return a dataframe with 2 columns: one will contain all unique values of the variable you counted and the other one, named n will contain their counts (you can specify the name variable to change that to something else). Setting sort argument to TRUE will sort the counts in descending order. E.g. if we want to find out which year had the most measurements of storms (and not the number of storms! Remember that each row is 1 measurement of 1 storm) we can do it with:\n\nstorms %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 47 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2005   873\n 2  2020   863\n 3  1995   762\n 4  2010   663\n 5  2012   654\n 6  2017   610\n 7  2018   608\n 8  2004   604\n 9  2003   593\n10  2021   592\n# ℹ 37 more rows\n\n\n\n\n\nA common task when wrangling data is creating new variables in an already existing dataset. You can do it by using mutate(). Lets say we want to create a new variable that stores information on whether the storm was during summer (June, July, August) or not:\n\nstorms %&gt;%\n  mutate(summer = ifelse(month == 6 | month == 7 | month == 8, TRUE, FALSE))\n\n# A tibble: 19,066 × 14\n   name   year month   day  hour   lat  long status      category  wind pressure\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n# ℹ 19,056 more rows\n# ℹ 3 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;, summer &lt;lgl&gt;\n\n\nRemember that if you want to keep the variable you need to assign the new dataset to an object.\n\n\n\nAnother extremely common task is to get summaries about our dataset. We can do it with summarise() function. e.g. what if we want to see the mean and standard deviation of strength of wind (for now across all measurements):\n\nstorms %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T),\n            sd_wind = sd(wind, na.rm = T))\n\n# A tibble: 1 × 2\n  mean_wind sd_wind\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      50.0    25.5\n\n\nNotice that the shape of the dataset has changed now. The columns are now the summaries and not the original variables.\n\n\n\nSo far we have been calculating things on entire datasets. In many situations you want to calculate something separately for each level of a categorical variable (much like tapply() earlier). To group a dataset we can use group_by(). You can also group by multiple variables at once by separating them by a coma. R will group by the first variable and then by the second etc. This is especially useful for creating summaries. E.g. if we want to get average wind speed for each storm we can easily do it:\n\nstorms %&gt;%\n  group_by(name) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n# A tibble: 258 × 2\n   name     mean_wind\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 AL011993      29.5\n 2 AL012000      25  \n 3 AL021992      29  \n 4 AL021994      24.2\n 5 AL021999      28.8\n 6 AL022000      29.2\n 7 AL022001      25  \n 8 AL022003      30  \n 9 AL022006      31.5\n10 AL031987      21.2\n# ℹ 248 more rows\n\n\nSimilarly if we want to calculate average wind speed from all measurements for each year and month:\n\nstorms %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(mean_wind = mean(wind, na.rm = T))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 253 × 3\n# Groups:   year [47]\n    year month mean_wind\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1  1975     6      37.5\n 2  1975     7      49.7\n 3  1975     8      44.5\n 4  1975     9      55.9\n 5  1975    10      62.4\n 6  1976     8      55.9\n 7  1976     9      61.3\n 8  1976    10      51.0\n 9  1977     8      53  \n10  1977     9      50.6\n# ℹ 243 more rows\n\n\nActually the summarise() function has its own argument for calculating grouped summaries. You can specify .by argument inside the summarise() function. If you use group_by() then summarise() will by default drop the last level of grouping.\nOne more important thing about the group_by() function is that it works by adding an attribute to the dataframe. This means that after a group_by() all subsequent operations will be conducted on the grouped dataframe. E.g. if you sort after grouping then sorting will be conducted within each group separately. You can drop the grouping with ungroup(). There is one last special kind of grouping you might want to use - sometimes you want to perform some operation separately on each row (e.g. calculate the average of a multi item scale for each participant). You can do it with rowwise().\n\n\n\nThere are situations when you want to perform the same operation on multiple columns (e.g. calculate the mean and standard deviation of multiple variables). You can do it by hand but this can be tedious. To simplify it you can use across() inside mutate() or summarise(). the syntax of across() is as follows: the first argument, .cols specifies which columns to perform operations on. The second argument .fns specifies which functions to apply. You have to provide it a list of functions (preferably named list) or a formula. Finally the .names argument specifies how to automatically assign new variable names. E.g. “{.col}_{.fn}” will create variables with column name, underscore and function name (that’s why named list is useful here). If we want to get the mean and standard deviation of wind speed and pressure in each category of storms we could do it in a few lines of code (by the way, notice how group_by() by default includes NA as a separate category):\n\nstorms %&gt;%\n  group_by(category) %&gt;%\n  summarise(across(.cols = c(\"wind\", \"pressure\"), .fns = list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 6 × 5\n  category wind_mean wind_sd pressure_mean pressure_sd\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1        1      71.0    5.55          981.        9.21\n2        2      89.5    3.77          967.        8.94\n3        3     104.     4.18          955.        8.91\n4        4     122.     6.39          940.        9.43\n5        5     147.     6.22          918.       12.0 \n6       NA      38.1   12.0          1002.        9.24\n\n\n\n\n\nOne more useful thing for data wrangling is a set of functions making it easier to select multiple columns based on some condition. There are a few helper functions you can use to do that. They do basically what their names suggest. These verbs are as follows: starts_with(), ends_with() and contains(). This way you don’t have to manually type all the names if they have something in common (e.g. they are items from the same scale so they are named ``scale_1, scale_2 etc.). E.g. lets say we want to get all the columns that end with “diameter”:\n\nstorms %&gt;%\n  select(ends_with(\"diameter\"))\n\n# A tibble: 19,066 × 2\n   tropicalstorm_force_diameter hurricane_force_diameter\n                          &lt;int&gt;                    &lt;int&gt;\n 1                           NA                       NA\n 2                           NA                       NA\n 3                           NA                       NA\n 4                           NA                       NA\n 5                           NA                       NA\n 6                           NA                       NA\n 7                           NA                       NA\n 8                           NA                       NA\n 9                           NA                       NA\n10                           NA                       NA\n# ℹ 19,056 more rows\n\n\nNice! These verbs also work nicely inside across(). One thing to be aware of: if no column matches what you ask for you won’t get an error but a dataframe with all the rows but 0 columns:\n\nstorms %&gt;%\n  select(contains(\"some_weird_name\"))\n\n# A tibble: 19,066 × 0"
  },
  {
    "objectID": "07data_wrangling.html#combining-functions-together",
    "href": "07data_wrangling.html#combining-functions-together",
    "title": "data wrangling",
    "section": "",
    "text": "Where tidyverse really shines is in combining multiple functions together with pipes. Through different orders of the functions described above we can get a ton of things out of our dataset. This already gives us the ability to anwser a number of questions that might be very interesting for analysis.\n\nExample 1: Lets say we want to find out the name of storm from each category that had the highest average pressure in 1989.\n\nstorms %&gt;%\n  filter(year == 1989) %&gt;%\n  group_by(category, name) %&gt;%\n  summarise(mean_pressure = mean(pressure, na.rm = T)) %&gt;%\n  slice_max(mean_pressure, n = 1)\n\n`summarise()` has grouped output by 'category'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   category [6]\n  category name      mean_pressure\n     &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1        1 Chantal            987 \n2        2 Dean               972.\n3        3 Hugo               953.\n4        4 Gabrielle          943.\n5        5 Hugo               918 \n6       NA Barry             1012.\n\n\nExample 2: Lets say we want to find the average wind speed at each hour of the day but we want that in kilometers per hours rather than knots (as is in the database). 1 knot is around 1.852 km/h.\n\nstorms %&gt;%\n  mutate(wind_km = wind*1.852) %&gt;%\n  group_by(hour) %&gt;%\n  summarise(mean_wind_km = mean(wind, na.rm = T))\n\n# A tibble: 24 × 2\n    hour mean_wind_km\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1     0         49.6\n 2     1         76.2\n 3     2         64.4\n 4     3         76.7\n 5     4         70.6\n 6     5         76.2\n 7     6         49.7\n 8     7         80.8\n 9     8         68.2\n10     9         60.9\n# ℹ 14 more rows"
  },
  {
    "objectID": "07data_wrangling.html#exercises",
    "href": "07data_wrangling.html#exercises",
    "title": "data wrangling",
    "section": "",
    "text": "Throughout the exercises you’ll still work with the storms dataset\n\nFind out which year had the most storms (remember that each row is 1 measurement of 1 storm. You have to find the year with the most storms and not the most measurements!)\nFind the average wind speed for each storm in 2018 and then sort from the highest to the lowest.\nCalculate the mean and standard deviation of measurement pressure for each month of the year"
  },
  {
    "objectID": "08join_restructure.html",
    "href": "08join_restructure.html",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "We know a bit about wrangling data. Now we will deal with situations in which your data is not in the correct shape to allow you to calculate what you want. We will look at separating and uniting variables (e.g. what if your dataset has separate columns for year, month and day but you need those in 1 variable?), joining datasets (e.g. what if some information is in 1 dataset but other information you need is in another one?) and reshaping data (going from wide to long format and back again).\nWe’ll learn seaparting and uniting variables by looking at a dataset aboutScooby Doo episodes by plummye. The dataset is taken from Kaggle. This dataset has a loot of variables that are either generally about a given episode or about specific characters (like monsters or members of the scooby gang).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nscooby &lt;- read_csv(\"data/scooby.csv\")\n\nNew names:\nRows: 603 Columns: 76\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(60): series_name, network, season, title, imdb, engagement, format, mo... dbl\n(6): ...1, index, run_time, monster_amount, suspects_amount, culprit_a... lgl\n(9): unmask_other, caught_other, caught_not, door_gag, batman, scooby_... date\n(1): date_aired\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\n\nIn order to separate 1 variable into more you can use separate(). The opposite operation can be done with unite().When separating you need to specify which variable to split, what are the names of the new variables (passed as a character vector) and what is the separator which basically tells R where to “cut” the old variable into new ones. By default the old variable is removed from the dataset. Lets look at the date_aired variable. It stores the year, month and day of when each episode was aired. Lets say we wanted to split it into three variables: year, month and day. We can easily do it with separate():\n\nscooby_separated &lt;- scooby %&gt;% separate(date_aired, into = c(\"year\", \"month\", \"day\"), sep = \"-\")\n\nscooby_separated %&gt;%\n  select(year, month, day) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  year  month day  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 1969  09    13   \n2 1969  09    20   \n3 1969  09    27   \n4 1969  10    04   \n5 1969  10    11   \n6 1969  10    18   \n\n\nYay, we got 3 new variables just like we wanted! Uniting is very similar, it just has a reversed effect. You specify what should the name of the new variable be, what are the names of variables to unite and what R should use to separate the values from the old variables.\n\n#unite example\nscooby_united &lt;- unite(scooby_separated, col = \"date_aired\", year:day, sep = \"-\")\n\nhead(scooby_united$date_aired)\n\n[1] \"1969-09-13\" \"1969-09-20\" \"1969-09-27\" \"1969-10-04\" \"1969-10-11\"\n[6] \"1969-10-18\"\n\n\nA potential problem with separate() is when various rows have different number of values. Then you might get conflicting number of columns to create. For example the monster_type column stores information on the types of monsters that were present in a given episode. The problem is various episodes had different numbers of monsters and they’re all stored in a single column. They are all separated by commas. In order to use separate() we need to know how many columns to create. We can do it by finding out what is the maximum number of commas and adding 1. We can do it quickly with th str_count() function from stringr package which counts occurences of a string.\n\nmax(str_count(scooby$monster_type, \",\"))\n\n[1] 18\n\n\nOk, the maximum number of commas is 18 so there were maximum of 19 monster in a given episode. Now we’ll need to prepare the names for new variables as we’d rather not type 19 names by hand and then separate\n\na &lt;- \"monster_type_\"\nb &lt;- c(1:19)\nvars &lt;- paste0(a, b)\n\nscooby_separated &lt;- separate(scooby, monster_type, into = vars,  sep = \",\")\n\nWarning: Expected 19 pieces. Missing pieces filled with `NA` in 602 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\nscooby_separated %&gt;%\n  select(monster_type_1:monster_type_19) %&gt;%\n  head()\n\n# A tibble: 6 × 19\n  monster_type_1   monster_type_2 monster_type_3 monster_type_4 monster_type_5\n  &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;         \n1 Possessed Object &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n2 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n3 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n4 Ancient          &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n5 Ancient          &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n6 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n# ℹ 14 more variables: monster_type_6 &lt;chr&gt;, monster_type_7 &lt;chr&gt;,\n#   monster_type_8 &lt;chr&gt;, monster_type_9 &lt;chr&gt;, monster_type_10 &lt;chr&gt;,\n#   monster_type_11 &lt;chr&gt;, monster_type_12 &lt;chr&gt;, monster_type_13 &lt;chr&gt;,\n#   monster_type_14 &lt;chr&gt;, monster_type_15 &lt;chr&gt;, monster_type_16 &lt;chr&gt;,\n#   monster_type_17 &lt;chr&gt;, monster_type_18 &lt;chr&gt;, monster_type_19 &lt;chr&gt;\n\n\nNotice the warning that said for rows where there were not enough values to fill all 19 variables the rest was filled with missing values.\nAn alternative way to separate values is to create new rows rather than columns. This way you can avoid the problem with needing to know the number of columns to create. You can do it with separate_rows(). It will split a given column values into multiple rows and duplicate all the values from other columns:\n\nscooby_separated &lt;- separate_rows(scooby, monster_subtype, sep = \",\")\nnrow(scooby_separated)\n\n[1] 1148\n\n\nWe end up with a dataframe that has 1148 rows. Each row now corresponds to 1 monster per episode. If we count rows (titlevariable) and extract the highest value we should get 19:\n\ncount(scooby_separated, title) %&gt;%\n  slice_max(n, n = 1)\n\n# A tibble: 1 × 2\n  title                                      n\n  &lt;chr&gt;                                  &lt;int&gt;\n1 Scooby-Doo! and the Reluctant Werewolf    19\n\n\n\n\n\nIn many situations the information that you need is not stored in a single dataset but in multiple ones. For example you might be working on a longitudinal study and each wave is saved in a separate dataset. Another common situation in which data is stored in multiple files (or tables) is to reduce redundancies. Imagine you store information about book authors and the books they published. You have some information about each author like the date of birth, nationality, awards etc. and some information about each book like the title, release date and genre. If you wanted to store it all in one table keeping 1 book per row would probably be most natural. However, then you would need to duplicate information about authors for every book they published. If the dataset is big this might prove to be a real issue. It might be easier to keep 1 table with author information and another table with book information which would also include 1 column to map books to authors. When working on both datasets you might want to join these two tables.\nTo look at joining data we’ll use 3 datasets that contain information about United Nations Roll calls. The data comes from Harvard Dataverse.\n\nissues &lt;- read_csv(\"data/issues.csv\")\n\nRows: 5745 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): short_name, issue\ndbl (1): rcid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nroll_calls &lt;- read_csv(\"data/roll_calls.csv\")\n\nRows: 6202 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): unres, short, descr\ndbl  (5): rcid, session, importantvote, amend, para\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvotes &lt;- read_csv(\"data/unvotes.csv\")\n\nRows: 869937 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): country, country_code, vote\ndbl (1): rcid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe first dataset, issues stores information on the roll call id: rcid variable (what we will be joining on) and the name of the issue a given roll call was about:\n\nglimpse(issues)\n\nRows: 5,745\nColumns: 3\n$ rcid       &lt;dbl&gt; 77, 9001, 9002, 9003, 9004, 9005, 9006, 128, 129, 130, 131,…\n$ short_name &lt;chr&gt; \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\",…\n$ issue      &lt;chr&gt; \"Palestinian conflict\", \"Palestinian conflict\", \"Palestinia…\n\n\nThe second dataset contains information about specific roll calls like dates and descriptions:\n\nglimpse(roll_calls)\n\nRows: 6,202\nColumns: 9\n$ rcid          &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ session       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ importantvote &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date          &lt;date&gt; 1946-01-01, 1946-01-02, 1946-01-04, 1946-01-04, 1946-01…\n$ unres         &lt;chr&gt; \"R/1/66\", \"R/1/79\", \"R/1/98\", \"R/1/107\", \"R/1/295\", \"R/1…\n$ amend         &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,…\n$ para          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ short         &lt;chr&gt; \"AMENDMENTS, RULES OF PROCEDURE\", \"SECURITY COUNCIL ELEC…\n$ descr         &lt;chr&gt; \"TO ADOPT A CUBAN AMENDMENT TO THE UK PROPOSAL REFERRING…\n\n\nFinally, votes dataset contains roll call ids and information about how each country voted (yes, no or abstain):\n\nglimpse(votes)\n\nRows: 869,937\nColumns: 4\n$ rcid         &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ country      &lt;chr&gt; \"United States\", \"Canada\", \"Cuba\", \"Haiti\", \"Dominican Re…\n$ country_code &lt;chr&gt; \"US\", \"CA\", \"CU\", \"HT\", \"DO\", \"MX\", \"GT\", \"HN\", \"SV\", \"NI…\n$ vote         &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"y…\n\n\nImagine you want to analyze how many times each country voted in a specific way on each issue. In order to do that we need to join the issues and votes dataframes. All join functions in tidyr end with _join. They differ in what is kept or removed from the dataset when joining. In many situations not all records in two datasets will match (e.g. there is dropout between first and second wave of a longitudinal study so not every row in wave 1 will have a matching row in wave 2). You can deal with it in 3 ways: keep only the matching rows (inner_join() function), keep all rows from one dataset and remove non-matching rows from the other (right_join() and left_join() functions) or keep all the rows from both datasets (full_join() function). When performing a join you can also specify a by argument that controls which columns should be used for the join. If you don’t specify this argument R will automatically try to join on all columns that have the same names in both datasets.\nLets try and answer our question about how countries voted on specific issues:\n\nvotes %&gt;%\n  inner_join(issues, by = \"rcid\") %&gt;%\n  group_by(issue, country) %&gt;%\n  count(vote)\n\nWarning in inner_join(., issues, by = \"rcid\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 382 of `x` matches multiple rows in `y`.\nℹ Row 3009 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 3,469 × 4\n# Groups:   issue, country [1,195]\n   issue                        country     vote        n\n   &lt;chr&gt;                        &lt;chr&gt;       &lt;chr&gt;   &lt;int&gt;\n 1 Arms control and disarmament Afghanistan abstain    69\n 2 Arms control and disarmament Afghanistan no         19\n 3 Arms control and disarmament Afghanistan yes       939\n 4 Arms control and disarmament Albania     abstain   114\n 5 Arms control and disarmament Albania     no        168\n 6 Arms control and disarmament Albania     yes       468\n 7 Arms control and disarmament Algeria     abstain   109\n 8 Arms control and disarmament Algeria     no          2\n 9 Arms control and disarmament Algeria     yes       929\n10 Arms control and disarmament Andorra     abstain   112\n# ℹ 3,459 more rows\n\n\nNotice that we got a warning about a many-to-many relationship. That’s because in the votes dataset multiple rows have the same rcid value (multiple countries voted in the same roll calls) and in the issues datasets certain roll calls have more than one issue.\nApart from the classic joins there are also filtering joins. They don’t really join datasets but they filter the datasets to keep the rows that match (semi_join()) or don’t match (anti_join()). Using them we can e.g. find out for which roll calls we don’t have information on their issues:\n\nroll_calls %&gt;%\n  anti_join(issues, by = \"rcid\")\n\n# A tibble: 2,103 × 9\n    rcid session importantvote date       unres   amend  para short        descr\n   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;\n 1     3       1             0 1946-01-01 R/1/66      1     0 AMENDMENTS,… \"TO …\n 2     4       1             0 1946-01-02 R/1/79      0     0 SECURITY CO… \"TO …\n 3     5       1             0 1946-01-04 R/1/98      0     0 VOTING PROC… \"TO …\n 4     7       1             0 1946-01-02 R/1/295     1     0 GENERAL ASS… \"TO …\n 5     9       1             0 1946-02-05 R/1/329     0     0 POST-WAR RE… \"TO …\n 6    10       1             0 1946-02-05 R/1/361     1     1 U.N. MEMBER… \"TO …\n 7    12       1             0 1946-02-06 R/1/394     1     1 COUNCIL MEM… \"TO …\n 8    13       1             0 1946-02-01 R/1/434     1     1 PROPAGANDA,… \"TO …\n 9    14       1             0 1946-02-01 R/1/435     1     1 PERSONNEL, … \"TO …\n10    15       1             0 1946-02-01 R/1/435     1     1 QUISLINGS, … \"TO …\n# ℹ 2,093 more rows\n\n\nOne final note on joins: their behaviour might sometimes feel a bit unintuitive when dealing with duplicated values. Imagine two dataframe like below. What do you think will happen when we make an inner join on the id variable?\n\ntable1 &lt;- data.frame(id = c(1, 1),\n                     a = c(1, 2))\ntable2 &lt;- data.frame(id = c(1, 1),\n                     b = c(3, 4))\n\ntable1\n\n  id a\n1  1 1\n2  1 2\n\ntable2\n\n  id b\n1  1 3\n2  1 4\n\n\nWhat we get is a many-to-many mapping just like with the unvotes join we made:\n\ntable1 %&gt;%\n  inner_join(table2)\n\nJoining with `by = join_by(id)`\n\n\nWarning in inner_join(., table2): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n  id a b\n1  1 1 3\n2  1 1 4\n3  1 2 3\n4  1 2 4\n\n\nR automatically join every matching row from table 1 with every matching row from table 2. This might seem confusing but is actually a quite desirable behaviour. Remember the example with 2 tables on books: one with authors and one with books? When joining we want to match each author to every book they have written not just to the first or last one. If you want to have more control you can use the multiple or relationship arguments.\n\n\n\nThe last thing we’ll cover here is changing the format of your data from wide to long or vice versa. For many datasets there are 2 ways in which you can store information.\nIn wide format you generally store 1 observation in one row (e.g. one participant across all the waves of a study), In long format one row is one measurement for one observation (e.g. 1 wave for 1 participant). Lets look at the votes data frame. if we are interested in roll calls its currently in a long format - each row is 1 vote in a given roll call for one country. In wide format we could have 1 row per roll call and 1 column per country.\nYou can reshape the dataset into a wide format using pivot_wider(). You need to specify 3 things: from which column to derive values for new columns, (values_from) from which columns to use values for new variable names (names_from) and a set of observations that uniquely identify each observation. (id_cols). The last argument is needed so that we end up with 1 value per cell. We can reshape the votes data frame like this:\n\nvotes_wide &lt;- votes %&gt;%\n  pivot_wider(id_cols = \"rcid\", names_from = \"country\", values_from = \"vote\")\n\nvotes_wide\n\n# A tibble: 6,202 × 201\n    rcid `United States` Canada Cuba  Haiti `Dominican Republic` Mexico\n   &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt; \n 1     3 yes             no     yes   yes   yes                  yes   \n 2     4 no              no     no    no    no                   no    \n 3     5 no              no     yes   no    no                   yes   \n 4     6 no              no     yes   &lt;NA&gt;  abstain              yes   \n 5     7 no              no     yes   yes   yes                  yes   \n 6     8 no              yes    yes   &lt;NA&gt;  yes                  yes   \n 7     9 yes             yes    yes   yes   yes                  yes   \n 8    10 yes             yes    yes   yes   yes                  yes   \n 9    11 yes             yes    yes   &lt;NA&gt;  yes                  &lt;NA&gt;  \n10    12 yes             yes    yes   &lt;NA&gt;  yes                  no    \n# ℹ 6,192 more rows\n# ℹ 194 more variables: Guatemala &lt;chr&gt;, Honduras &lt;chr&gt;, `El Salvador` &lt;chr&gt;,\n#   Nicaragua &lt;chr&gt;, `Costa Rica` &lt;chr&gt;, Panama &lt;chr&gt;, Colombia &lt;chr&gt;,\n#   Venezuela &lt;chr&gt;, Ecuador &lt;chr&gt;, Peru &lt;chr&gt;, Brazil &lt;chr&gt;, Bolivia &lt;chr&gt;,\n#   Paraguay &lt;chr&gt;, Chile &lt;chr&gt;, Argentina &lt;chr&gt;, Uruguay &lt;chr&gt;,\n#   `United Kingdom` &lt;chr&gt;, Netherlands &lt;chr&gt;, Belgium &lt;chr&gt;, Luxembourg &lt;chr&gt;,\n#   France &lt;chr&gt;, Poland &lt;chr&gt;, Czechoslovakia &lt;chr&gt;, Yugoslavia &lt;chr&gt;, …\n\n\nYou could reshape the data in a different way so that each country is in 1 row and each column is a separate roll call:\n\nvotes_wide2 &lt;- votes %&gt;%\n  pivot_wider(id_cols = \"country\", names_from = \"rcid\", values_from = \"vote\")\n\nvotes_wide2\n\n# A tibble: 200 × 6,203\n   country     `3`   `4`   `5`   `6`   `7`   `8`   `9`   `10`  `11`  `12`  `13` \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 United Sta… yes   no    no    no    no    no    yes   yes   yes   yes   no   \n 2 Canada      no    no    no    no    no    yes   yes   yes   yes   yes   no   \n 3 Cuba        yes   no    yes   yes   yes   yes   yes   yes   yes   yes   no   \n 4 Haiti       yes   no    no    &lt;NA&gt;  yes   &lt;NA&gt;  yes   yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 5 Dominican … yes   no    no    abst… yes   yes   yes   yes   yes   yes   no   \n 6 Mexico      yes   no    yes   yes   yes   yes   yes   yes   &lt;NA&gt;  no    &lt;NA&gt; \n 7 Guatemala   yes   no    no    no    &lt;NA&gt;  yes   &lt;NA&gt;  yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 8 Honduras    yes   no    yes   yes   yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  yes   yes   no   \n 9 El Salvador yes   no    yes   abst… &lt;NA&gt;  &lt;NA&gt;  yes   no    &lt;NA&gt;  &lt;NA&gt;  no   \n10 Nicaragua   yes   no    yes   yes   yes   &lt;NA&gt;  &lt;NA&gt;  no    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 190 more rows\n# ℹ 6,191 more variables: `14` &lt;chr&gt;, `15` &lt;chr&gt;, `16` &lt;chr&gt;, `17` &lt;chr&gt;,\n#   `18` &lt;chr&gt;, `19` &lt;chr&gt;, `20` &lt;chr&gt;, `21` &lt;chr&gt;, `22` &lt;chr&gt;, `23` &lt;chr&gt;,\n#   `24` &lt;chr&gt;, `25` &lt;chr&gt;, `26` &lt;chr&gt;, `27` &lt;chr&gt;, `28` &lt;chr&gt;, `29` &lt;chr&gt;,\n#   `30` &lt;chr&gt;, `31` &lt;chr&gt;, `32` &lt;chr&gt;, `33` &lt;chr&gt;, `34` &lt;chr&gt;, `35` &lt;chr&gt;,\n#   `36` &lt;chr&gt;, `37` &lt;chr&gt;, `38` &lt;chr&gt;, `39` &lt;chr&gt;, `40` &lt;chr&gt;, `41` &lt;chr&gt;,\n#   `42` &lt;chr&gt;, `43` &lt;chr&gt;, `44` &lt;chr&gt;, `45` &lt;chr&gt;, `46` &lt;chr&gt;, `47` &lt;chr&gt;, …\n\n\nThere is no one correct way to shape your data. Instead the shape of your data should match the question you want to answer.\nYou can reshape the dataset into a long format using pivot_longer(). You nned to pass it a data frame, columns to reshape and how to name he new column with values and the new column with names. The latter will use variable names as its values. We can go back to the long format of votes:\n\nvotes_long &lt;- votes_wide %&gt;%\n  pivot_longer(cols = \"United States\":\"South Sudan\", names_to = \"country\", values_to = \"vote\")\n\nvotes_long\n\n# A tibble: 1,240,400 × 3\n    rcid country            vote \n   &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;\n 1     3 United States      yes  \n 2     3 Canada             no   \n 3     3 Cuba               yes  \n 4     3 Haiti              yes  \n 5     3 Dominican Republic yes  \n 6     3 Mexico             yes  \n 7     3 Guatemala          yes  \n 8     3 Honduras           yes  \n 9     3 El Salvador        yes  \n10     3 Nicaragua          yes  \n# ℹ 1,240,390 more rows\n\n\n\n\n\nUsing the datasets on UN roll calls. You can get the data by installing unvotes package.\n\nFind out which country voted the most in 1946 regardless of what kind of vote it was.\nFind out how many amendments each issue had across all roll calls\nFind out on which issue there was most agreement (highest percentage of the same votes) between USA and Poland"
  },
  {
    "objectID": "08join_restructure.html#separating-and-uniting-variables",
    "href": "08join_restructure.html#separating-and-uniting-variables",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "In order to separate 1 variable into more you can use separate(). The opposite operation can be done with unite().When separating you need to specify which variable to split, what are the names of the new variables (passed as a character vector) and what is the separator which basically tells R where to “cut” the old variable into new ones. By default the old variable is removed from the dataset. Lets look at the date_aired variable. It stores the year, month and day of when each episode was aired. Lets say we wanted to split it into three variables: year, month and day. We can easily do it with separate():\n\nscooby_separated &lt;- scooby %&gt;% separate(date_aired, into = c(\"year\", \"month\", \"day\"), sep = \"-\")\n\nscooby_separated %&gt;%\n  select(year, month, day) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  year  month day  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 1969  09    13   \n2 1969  09    20   \n3 1969  09    27   \n4 1969  10    04   \n5 1969  10    11   \n6 1969  10    18   \n\n\nYay, we got 3 new variables just like we wanted! Uniting is very similar, it just has a reversed effect. You specify what should the name of the new variable be, what are the names of variables to unite and what R should use to separate the values from the old variables.\n\n#unite example\nscooby_united &lt;- unite(scooby_separated, col = \"date_aired\", year:day, sep = \"-\")\n\nhead(scooby_united$date_aired)\n\n[1] \"1969-09-13\" \"1969-09-20\" \"1969-09-27\" \"1969-10-04\" \"1969-10-11\"\n[6] \"1969-10-18\"\n\n\nA potential problem with separate() is when various rows have different number of values. Then you might get conflicting number of columns to create. For example the monster_type column stores information on the types of monsters that were present in a given episode. The problem is various episodes had different numbers of monsters and they’re all stored in a single column. They are all separated by commas. In order to use separate() we need to know how many columns to create. We can do it by finding out what is the maximum number of commas and adding 1. We can do it quickly with th str_count() function from stringr package which counts occurences of a string.\n\nmax(str_count(scooby$monster_type, \",\"))\n\n[1] 18\n\n\nOk, the maximum number of commas is 18 so there were maximum of 19 monster in a given episode. Now we’ll need to prepare the names for new variables as we’d rather not type 19 names by hand and then separate\n\na &lt;- \"monster_type_\"\nb &lt;- c(1:19)\nvars &lt;- paste0(a, b)\n\nscooby_separated &lt;- separate(scooby, monster_type, into = vars,  sep = \",\")\n\nWarning: Expected 19 pieces. Missing pieces filled with `NA` in 602 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\nscooby_separated %&gt;%\n  select(monster_type_1:monster_type_19) %&gt;%\n  head()\n\n# A tibble: 6 × 19\n  monster_type_1   monster_type_2 monster_type_3 monster_type_4 monster_type_5\n  &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;         \n1 Possessed Object &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n2 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n3 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n4 Ancient          &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n5 Ancient          &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n6 Ghost            &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;           &lt;NA&gt;          \n# ℹ 14 more variables: monster_type_6 &lt;chr&gt;, monster_type_7 &lt;chr&gt;,\n#   monster_type_8 &lt;chr&gt;, monster_type_9 &lt;chr&gt;, monster_type_10 &lt;chr&gt;,\n#   monster_type_11 &lt;chr&gt;, monster_type_12 &lt;chr&gt;, monster_type_13 &lt;chr&gt;,\n#   monster_type_14 &lt;chr&gt;, monster_type_15 &lt;chr&gt;, monster_type_16 &lt;chr&gt;,\n#   monster_type_17 &lt;chr&gt;, monster_type_18 &lt;chr&gt;, monster_type_19 &lt;chr&gt;\n\n\nNotice the warning that said for rows where there were not enough values to fill all 19 variables the rest was filled with missing values.\nAn alternative way to separate values is to create new rows rather than columns. This way you can avoid the problem with needing to know the number of columns to create. You can do it with separate_rows(). It will split a given column values into multiple rows and duplicate all the values from other columns:\n\nscooby_separated &lt;- separate_rows(scooby, monster_subtype, sep = \",\")\nnrow(scooby_separated)\n\n[1] 1148\n\n\nWe end up with a dataframe that has 1148 rows. Each row now corresponds to 1 monster per episode. If we count rows (titlevariable) and extract the highest value we should get 19:\n\ncount(scooby_separated, title) %&gt;%\n  slice_max(n, n = 1)\n\n# A tibble: 1 × 2\n  title                                      n\n  &lt;chr&gt;                                  &lt;int&gt;\n1 Scooby-Doo! and the Reluctant Werewolf    19"
  },
  {
    "objectID": "08join_restructure.html#joining-data",
    "href": "08join_restructure.html#joining-data",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "In many situations the information that you need is not stored in a single dataset but in multiple ones. For example you might be working on a longitudinal study and each wave is saved in a separate dataset. Another common situation in which data is stored in multiple files (or tables) is to reduce redundancies. Imagine you store information about book authors and the books they published. You have some information about each author like the date of birth, nationality, awards etc. and some information about each book like the title, release date and genre. If you wanted to store it all in one table keeping 1 book per row would probably be most natural. However, then you would need to duplicate information about authors for every book they published. If the dataset is big this might prove to be a real issue. It might be easier to keep 1 table with author information and another table with book information which would also include 1 column to map books to authors. When working on both datasets you might want to join these two tables.\nTo look at joining data we’ll use 3 datasets that contain information about United Nations Roll calls. The data comes from Harvard Dataverse.\n\nissues &lt;- read_csv(\"data/issues.csv\")\n\nRows: 5745 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): short_name, issue\ndbl (1): rcid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nroll_calls &lt;- read_csv(\"data/roll_calls.csv\")\n\nRows: 6202 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): unres, short, descr\ndbl  (5): rcid, session, importantvote, amend, para\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvotes &lt;- read_csv(\"data/unvotes.csv\")\n\nRows: 869937 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): country, country_code, vote\ndbl (1): rcid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe first dataset, issues stores information on the roll call id: rcid variable (what we will be joining on) and the name of the issue a given roll call was about:\n\nglimpse(issues)\n\nRows: 5,745\nColumns: 3\n$ rcid       &lt;dbl&gt; 77, 9001, 9002, 9003, 9004, 9005, 9006, 128, 129, 130, 131,…\n$ short_name &lt;chr&gt; \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\", \"me\",…\n$ issue      &lt;chr&gt; \"Palestinian conflict\", \"Palestinian conflict\", \"Palestinia…\n\n\nThe second dataset contains information about specific roll calls like dates and descriptions:\n\nglimpse(roll_calls)\n\nRows: 6,202\nColumns: 9\n$ rcid          &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ session       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ importantvote &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date          &lt;date&gt; 1946-01-01, 1946-01-02, 1946-01-04, 1946-01-04, 1946-01…\n$ unres         &lt;chr&gt; \"R/1/66\", \"R/1/79\", \"R/1/98\", \"R/1/107\", \"R/1/295\", \"R/1…\n$ amend         &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,…\n$ para          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ short         &lt;chr&gt; \"AMENDMENTS, RULES OF PROCEDURE\", \"SECURITY COUNCIL ELEC…\n$ descr         &lt;chr&gt; \"TO ADOPT A CUBAN AMENDMENT TO THE UK PROPOSAL REFERRING…\n\n\nFinally, votes dataset contains roll call ids and information about how each country voted (yes, no or abstain):\n\nglimpse(votes)\n\nRows: 869,937\nColumns: 4\n$ rcid         &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ country      &lt;chr&gt; \"United States\", \"Canada\", \"Cuba\", \"Haiti\", \"Dominican Re…\n$ country_code &lt;chr&gt; \"US\", \"CA\", \"CU\", \"HT\", \"DO\", \"MX\", \"GT\", \"HN\", \"SV\", \"NI…\n$ vote         &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"y…\n\n\nImagine you want to analyze how many times each country voted in a specific way on each issue. In order to do that we need to join the issues and votes dataframes. All join functions in tidyr end with _join. They differ in what is kept or removed from the dataset when joining. In many situations not all records in two datasets will match (e.g. there is dropout between first and second wave of a longitudinal study so not every row in wave 1 will have a matching row in wave 2). You can deal with it in 3 ways: keep only the matching rows (inner_join() function), keep all rows from one dataset and remove non-matching rows from the other (right_join() and left_join() functions) or keep all the rows from both datasets (full_join() function). When performing a join you can also specify a by argument that controls which columns should be used for the join. If you don’t specify this argument R will automatically try to join on all columns that have the same names in both datasets.\nLets try and answer our question about how countries voted on specific issues:\n\nvotes %&gt;%\n  inner_join(issues, by = \"rcid\") %&gt;%\n  group_by(issue, country) %&gt;%\n  count(vote)\n\nWarning in inner_join(., issues, by = \"rcid\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 382 of `x` matches multiple rows in `y`.\nℹ Row 3009 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 3,469 × 4\n# Groups:   issue, country [1,195]\n   issue                        country     vote        n\n   &lt;chr&gt;                        &lt;chr&gt;       &lt;chr&gt;   &lt;int&gt;\n 1 Arms control and disarmament Afghanistan abstain    69\n 2 Arms control and disarmament Afghanistan no         19\n 3 Arms control and disarmament Afghanistan yes       939\n 4 Arms control and disarmament Albania     abstain   114\n 5 Arms control and disarmament Albania     no        168\n 6 Arms control and disarmament Albania     yes       468\n 7 Arms control and disarmament Algeria     abstain   109\n 8 Arms control and disarmament Algeria     no          2\n 9 Arms control and disarmament Algeria     yes       929\n10 Arms control and disarmament Andorra     abstain   112\n# ℹ 3,459 more rows\n\n\nNotice that we got a warning about a many-to-many relationship. That’s because in the votes dataset multiple rows have the same rcid value (multiple countries voted in the same roll calls) and in the issues datasets certain roll calls have more than one issue.\nApart from the classic joins there are also filtering joins. They don’t really join datasets but they filter the datasets to keep the rows that match (semi_join()) or don’t match (anti_join()). Using them we can e.g. find out for which roll calls we don’t have information on their issues:\n\nroll_calls %&gt;%\n  anti_join(issues, by = \"rcid\")\n\n# A tibble: 2,103 × 9\n    rcid session importantvote date       unres   amend  para short        descr\n   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;\n 1     3       1             0 1946-01-01 R/1/66      1     0 AMENDMENTS,… \"TO …\n 2     4       1             0 1946-01-02 R/1/79      0     0 SECURITY CO… \"TO …\n 3     5       1             0 1946-01-04 R/1/98      0     0 VOTING PROC… \"TO …\n 4     7       1             0 1946-01-02 R/1/295     1     0 GENERAL ASS… \"TO …\n 5     9       1             0 1946-02-05 R/1/329     0     0 POST-WAR RE… \"TO …\n 6    10       1             0 1946-02-05 R/1/361     1     1 U.N. MEMBER… \"TO …\n 7    12       1             0 1946-02-06 R/1/394     1     1 COUNCIL MEM… \"TO …\n 8    13       1             0 1946-02-01 R/1/434     1     1 PROPAGANDA,… \"TO …\n 9    14       1             0 1946-02-01 R/1/435     1     1 PERSONNEL, … \"TO …\n10    15       1             0 1946-02-01 R/1/435     1     1 QUISLINGS, … \"TO …\n# ℹ 2,093 more rows\n\n\nOne final note on joins: their behaviour might sometimes feel a bit unintuitive when dealing with duplicated values. Imagine two dataframe like below. What do you think will happen when we make an inner join on the id variable?\n\ntable1 &lt;- data.frame(id = c(1, 1),\n                     a = c(1, 2))\ntable2 &lt;- data.frame(id = c(1, 1),\n                     b = c(3, 4))\n\ntable1\n\n  id a\n1  1 1\n2  1 2\n\ntable2\n\n  id b\n1  1 3\n2  1 4\n\n\nWhat we get is a many-to-many mapping just like with the unvotes join we made:\n\ntable1 %&gt;%\n  inner_join(table2)\n\nJoining with `by = join_by(id)`\n\n\nWarning in inner_join(., table2): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n  id a b\n1  1 1 3\n2  1 1 4\n3  1 2 3\n4  1 2 4\n\n\nR automatically join every matching row from table 1 with every matching row from table 2. This might seem confusing but is actually a quite desirable behaviour. Remember the example with 2 tables on books: one with authors and one with books? When joining we want to match each author to every book they have written not just to the first or last one. If you want to have more control you can use the multiple or relationship arguments."
  },
  {
    "objectID": "08join_restructure.html#from-wide-to-long-format-and-back-again",
    "href": "08join_restructure.html#from-wide-to-long-format-and-back-again",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "The last thing we’ll cover here is changing the format of your data from wide to long or vice versa. For many datasets there are 2 ways in which you can store information.\nIn wide format you generally store 1 observation in one row (e.g. one participant across all the waves of a study), In long format one row is one measurement for one observation (e.g. 1 wave for 1 participant). Lets look at the votes data frame. if we are interested in roll calls its currently in a long format - each row is 1 vote in a given roll call for one country. In wide format we could have 1 row per roll call and 1 column per country.\nYou can reshape the dataset into a wide format using pivot_wider(). You need to specify 3 things: from which column to derive values for new columns, (values_from) from which columns to use values for new variable names (names_from) and a set of observations that uniquely identify each observation. (id_cols). The last argument is needed so that we end up with 1 value per cell. We can reshape the votes data frame like this:\n\nvotes_wide &lt;- votes %&gt;%\n  pivot_wider(id_cols = \"rcid\", names_from = \"country\", values_from = \"vote\")\n\nvotes_wide\n\n# A tibble: 6,202 × 201\n    rcid `United States` Canada Cuba  Haiti `Dominican Republic` Mexico\n   &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt; \n 1     3 yes             no     yes   yes   yes                  yes   \n 2     4 no              no     no    no    no                   no    \n 3     5 no              no     yes   no    no                   yes   \n 4     6 no              no     yes   &lt;NA&gt;  abstain              yes   \n 5     7 no              no     yes   yes   yes                  yes   \n 6     8 no              yes    yes   &lt;NA&gt;  yes                  yes   \n 7     9 yes             yes    yes   yes   yes                  yes   \n 8    10 yes             yes    yes   yes   yes                  yes   \n 9    11 yes             yes    yes   &lt;NA&gt;  yes                  &lt;NA&gt;  \n10    12 yes             yes    yes   &lt;NA&gt;  yes                  no    \n# ℹ 6,192 more rows\n# ℹ 194 more variables: Guatemala &lt;chr&gt;, Honduras &lt;chr&gt;, `El Salvador` &lt;chr&gt;,\n#   Nicaragua &lt;chr&gt;, `Costa Rica` &lt;chr&gt;, Panama &lt;chr&gt;, Colombia &lt;chr&gt;,\n#   Venezuela &lt;chr&gt;, Ecuador &lt;chr&gt;, Peru &lt;chr&gt;, Brazil &lt;chr&gt;, Bolivia &lt;chr&gt;,\n#   Paraguay &lt;chr&gt;, Chile &lt;chr&gt;, Argentina &lt;chr&gt;, Uruguay &lt;chr&gt;,\n#   `United Kingdom` &lt;chr&gt;, Netherlands &lt;chr&gt;, Belgium &lt;chr&gt;, Luxembourg &lt;chr&gt;,\n#   France &lt;chr&gt;, Poland &lt;chr&gt;, Czechoslovakia &lt;chr&gt;, Yugoslavia &lt;chr&gt;, …\n\n\nYou could reshape the data in a different way so that each country is in 1 row and each column is a separate roll call:\n\nvotes_wide2 &lt;- votes %&gt;%\n  pivot_wider(id_cols = \"country\", names_from = \"rcid\", values_from = \"vote\")\n\nvotes_wide2\n\n# A tibble: 200 × 6,203\n   country     `3`   `4`   `5`   `6`   `7`   `8`   `9`   `10`  `11`  `12`  `13` \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 United Sta… yes   no    no    no    no    no    yes   yes   yes   yes   no   \n 2 Canada      no    no    no    no    no    yes   yes   yes   yes   yes   no   \n 3 Cuba        yes   no    yes   yes   yes   yes   yes   yes   yes   yes   no   \n 4 Haiti       yes   no    no    &lt;NA&gt;  yes   &lt;NA&gt;  yes   yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 5 Dominican … yes   no    no    abst… yes   yes   yes   yes   yes   yes   no   \n 6 Mexico      yes   no    yes   yes   yes   yes   yes   yes   &lt;NA&gt;  no    &lt;NA&gt; \n 7 Guatemala   yes   no    no    no    &lt;NA&gt;  yes   &lt;NA&gt;  yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 8 Honduras    yes   no    yes   yes   yes   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  yes   yes   no   \n 9 El Salvador yes   no    yes   abst… &lt;NA&gt;  &lt;NA&gt;  yes   no    &lt;NA&gt;  &lt;NA&gt;  no   \n10 Nicaragua   yes   no    yes   yes   yes   &lt;NA&gt;  &lt;NA&gt;  no    &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 190 more rows\n# ℹ 6,191 more variables: `14` &lt;chr&gt;, `15` &lt;chr&gt;, `16` &lt;chr&gt;, `17` &lt;chr&gt;,\n#   `18` &lt;chr&gt;, `19` &lt;chr&gt;, `20` &lt;chr&gt;, `21` &lt;chr&gt;, `22` &lt;chr&gt;, `23` &lt;chr&gt;,\n#   `24` &lt;chr&gt;, `25` &lt;chr&gt;, `26` &lt;chr&gt;, `27` &lt;chr&gt;, `28` &lt;chr&gt;, `29` &lt;chr&gt;,\n#   `30` &lt;chr&gt;, `31` &lt;chr&gt;, `32` &lt;chr&gt;, `33` &lt;chr&gt;, `34` &lt;chr&gt;, `35` &lt;chr&gt;,\n#   `36` &lt;chr&gt;, `37` &lt;chr&gt;, `38` &lt;chr&gt;, `39` &lt;chr&gt;, `40` &lt;chr&gt;, `41` &lt;chr&gt;,\n#   `42` &lt;chr&gt;, `43` &lt;chr&gt;, `44` &lt;chr&gt;, `45` &lt;chr&gt;, `46` &lt;chr&gt;, `47` &lt;chr&gt;, …\n\n\nThere is no one correct way to shape your data. Instead the shape of your data should match the question you want to answer.\nYou can reshape the dataset into a long format using pivot_longer(). You nned to pass it a data frame, columns to reshape and how to name he new column with values and the new column with names. The latter will use variable names as its values. We can go back to the long format of votes:\n\nvotes_long &lt;- votes_wide %&gt;%\n  pivot_longer(cols = \"United States\":\"South Sudan\", names_to = \"country\", values_to = \"vote\")\n\nvotes_long\n\n# A tibble: 1,240,400 × 3\n    rcid country            vote \n   &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;\n 1     3 United States      yes  \n 2     3 Canada             no   \n 3     3 Cuba               yes  \n 4     3 Haiti              yes  \n 5     3 Dominican Republic yes  \n 6     3 Mexico             yes  \n 7     3 Guatemala          yes  \n 8     3 Honduras           yes  \n 9     3 El Salvador        yes  \n10     3 Nicaragua          yes  \n# ℹ 1,240,390 more rows"
  },
  {
    "objectID": "08join_restructure.html#exercises",
    "href": "08join_restructure.html#exercises",
    "title": "Restructuring and joining data",
    "section": "",
    "text": "Using the datasets on UN roll calls. You can get the data by installing unvotes package.\n\nFind out which country voted the most in 1946 regardless of what kind of vote it was.\nFind out how many amendments each issue had across all roll calls\nFind out on which issue there was most agreement (highest percentage of the same votes) between USA and Poland"
  },
  {
    "objectID": "09data_viz_1.html",
    "href": "09data_viz_1.html",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Visualization is an indispensible part of data analysis. If done properly it allows us to understand a lot more about our analysis/data and to understand it much faster than by wading through text. It also looks really nice! And good news is that R is absolutely great for plotting! In this class we’ll look at some basic R plotting functions first and then dive into the world of ggplot2, easily the best plotting package out there.\nIn this class we’ll use the midwest dataset from ggplot2 package. It stores a bunch of information about population of 5 midwestern states: Illinois, Indiana, Michigan, Ohio and Wisconsin. The data are at county level. Lets briefly look at our dataset for this class (all the variables are also described here):\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"midwest\")\nglimpse(midwest)\n\nRows: 437\nColumns: 28\n$ PID                  &lt;int&gt; 561, 562, 563, 564, 565, 566, 567, 568, 569, 570,…\n$ county               &lt;chr&gt; \"ADAMS\", \"ALEXANDER\", \"BOND\", \"BOONE\", \"BROWN\", \"…\n$ state                &lt;chr&gt; \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"…\n$ area                 &lt;dbl&gt; 0.052, 0.014, 0.022, 0.017, 0.018, 0.050, 0.017, …\n$ poptotal             &lt;int&gt; 66090, 10626, 14991, 30806, 5836, 35688, 5322, 16…\n$ popdensity           &lt;dbl&gt; 1270.9615, 759.0000, 681.4091, 1812.1176, 324.222…\n$ popwhite             &lt;int&gt; 63917, 7054, 14477, 29344, 5264, 35157, 5298, 165…\n$ popblack             &lt;int&gt; 1702, 3496, 429, 127, 547, 50, 1, 111, 16, 16559,…\n$ popamerindian        &lt;int&gt; 98, 19, 35, 46, 14, 65, 8, 30, 8, 331, 51, 26, 17…\n$ popasian             &lt;int&gt; 249, 48, 16, 150, 5, 195, 15, 61, 23, 8033, 89, 3…\n$ popother             &lt;int&gt; 124, 9, 34, 1139, 6, 221, 0, 84, 6, 1596, 20, 7, …\n$ percwhite            &lt;dbl&gt; 96.71206, 66.38434, 96.57128, 95.25417, 90.19877,…\n$ percblack            &lt;dbl&gt; 2.57527614, 32.90043290, 2.86171703, 0.41225735, …\n$ percamerindan        &lt;dbl&gt; 0.14828264, 0.17880670, 0.23347342, 0.14932156, 0…\n$ percasian            &lt;dbl&gt; 0.37675897, 0.45172219, 0.10673071, 0.48691813, 0…\n$ percother            &lt;dbl&gt; 0.18762294, 0.08469791, 0.22680275, 3.69733169, 0…\n$ popadults            &lt;int&gt; 43298, 6724, 9669, 19272, 3979, 23444, 3583, 1132…\n$ perchsd              &lt;dbl&gt; 75.10740, 59.72635, 69.33499, 75.47219, 68.86152,…\n$ percollege           &lt;dbl&gt; 19.63139, 11.24331, 17.03382, 17.27895, 14.47600,…\n$ percprof             &lt;dbl&gt; 4.355859, 2.870315, 4.488572, 4.197800, 3.367680,…\n$ poppovertyknown      &lt;int&gt; 63628, 10529, 14235, 30337, 4815, 35107, 5241, 16…\n$ percpovertyknown     &lt;dbl&gt; 96.27478, 99.08714, 94.95697, 98.47757, 82.50514,…\n$ percbelowpoverty     &lt;dbl&gt; 13.151443, 32.244278, 12.068844, 7.209019, 13.520…\n$ percchildbelowpovert &lt;dbl&gt; 18.011717, 45.826514, 14.036061, 11.179536, 13.02…\n$ percadultpoverty     &lt;dbl&gt; 11.009776, 27.385647, 10.852090, 5.536013, 11.143…\n$ percelderlypoverty   &lt;dbl&gt; 12.443812, 25.228976, 12.697410, 6.217047, 19.200…\n$ inmetro              &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0…\n$ category             &lt;chr&gt; \"AAR\", \"LHR\", \"AAR\", \"ALU\", \"AAR\", \"AAR\", \"LAR\", …\n\n\n\n\nBefore we move on to ggplot2 lets look at some built-in base R graphics. The graphics and stats packages have some function for plotting already available.\n\nThe most generic is the plot() function. It allows you to create simple plots in R. The first arguments are usually the variables you want to map onto the axes:\n\nplot(midwest$percadultpoverty, midwest$percollege)\n\n\n\n\nplot() function has a bunch of arguments you can use to customize the plot. For example we can change the color and thickness of the points and add a title to the plot and axes:\n\nplot(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2,\n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nUnfortunately the plot() function does not have great documentation and finding some of the arguments can be quite difficult. Some of these arguments also have very unintuitive names (e.g. argument named lty specifies line type, good luck memorizing that!). Base R has additional functions for more specific plots. Namely, lines() will create a line plot, points() will create a scatterplot and boxplot() will create a boxplot. These functions have better documentation than the general plot() though I still don’t consider it great. Lets see some of these in action. lines() and points() require that you first initialize the plots with the coordinates set to the variables of interest with the plot() function. You can specify what kind of plot (line plot, scatterplot etc) you want inside plot() by setting the type argument. E.g. setting it to l will create a line plot.\nLines: Lets say we want to make a line plot that will display the 10th, 50th and 90th quantile of percent of adults in poverty in all states. We’ll also introduce the axis() function which gives you some control over how the x and y axis should look like. Here we need it because lines() does not like categorical values at x axis so we need to add the state names (we add them at the top) with axis():\n\n#get the quantiles\nlibrary(dplyr)\n#calculate quantiles at of percadultpoverty for each state\nlist_q &lt;- tapply(midwest$percadultpoverty, midwest$state, quantile, c(.1,.5,.9))\n\n#convert the result into a dataframe with each column as one quantile and each row as one state\ndf_q &lt;- data.frame()\ndf_q &lt;- bind_rows(list_q[[1]], list_q[[2]], list_q[[3]], list_q[[4]], list_q[[5]])\ncolnames(df_q) &lt;- c(\"q_10\", \"q_50\", \"q_90\")\n\n#plot them\nplot(1:5, df_q$q_10, ylim = c(0,20), type = \"l\", col = 2, lwd = 2, lty = 2)\nlines(1:5, df_q$q_50, col = 4, lwd = 2, lty = 1)\nlines(1:5, df_q$q_90, col = 2, lwd = 2, lty = 2)\naxis(side = 3, at = 1:5, labels = unique(midwest$state))\n\n\n\n\nOne important thing here: notice how each element is added to the plot on a new line. These functions are not chained together in any way and we are not saving any intermediate objects. This is pretty unusual in R (although perfectly normal in other programming languages). It’s just how base R plotting works.\nPoints: We can recreate the plot that we made with the generic plot() function but using points():\n\nplot(midwest$percadultpoverty, midwest$percollege)\npoints(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2, \n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nBoxplots are useful for showing differences in distributions of some continuous variable between levels of some factor. For example lets say we want to\n\nboxplot(midwest$percadultpoverty ~ midwest$state)\n\n\n\n\n\nThere are also built-in functions for plotting distributions: hist() for histograms, plot(density()) for density functions and plot.ecdf() for cumulative distribution plots. Lets look at all 3 plots for percent of population in college:\n\nhist(midwest$percollege)\n\n\n\n\n\nplot(density(midwest$percollege))\n\n\n\n\n\nplot.ecdf(midwest$percollege)\n\n\n\n\nOne problem with base R plots is that they are not intuitive. Lots of arguments have weird names and doing some things is really not so easy. Making more complicated plots (e.g. adding text annotations on the plot) is also generally hard to do. That’s why we’ll focus on ggplot2 which is much more intuitive and versatile.\n\n\n\nggplot2 is a package in th tidyverse designed for making data visualizations. One of the great things about it is that it breaks down each plot into a number of layers that can be changed (more or less) independently. This idea is encapsulated in what is called the grammar of graphics.\n\n\n\nThe name comes from a book by Leland Wilkinson under the same title. Basically making a plot with the grammar of graphics is like making a building with lego blocks with different colors. You can mix the colors of the blocks to get exactly the building you want. Similalry in ggplot2 you can use different layers to make the plot that you want (e.g. mix different datasets or add a line to a scatterplot). Ok, but what are those layers? In ggplot2 there are following layers:\n\nData: the datasets (usually data frames) you want to plot\nAesthetics: mapping between data and the plot, e.g. what is to be mapped to x and y axis\nGeometry: shapes used to represent data\nStatistics: any statistics like means or confidence intervals that you want to add\nCoordinates: coordinates of the plot (axes limits, cartesian vs polar coordinates etc.)\nFacets: this layer is used for making subplots (e.g. separate plot for each level of a categorical variable)\nTheme: All non-data stuff like fonts, titles\n\nYou always start the plot with ggplot() function. All the lego blocks that you want to add are chained together with +. Lets see what happens when we pass the first layer, data, to our plot:\n\nmidwest %&gt;%\n  ggplot()\n\n\n\n\nHmm, we get en empty plot. Why is that? Well, all we supplied so far is the data we want to plot but we did not include any additional information so R doesn’t know yet what exactly should be displayed on the plot. We need to add the next layer: aesthetics.\n\n\n\nAesthetics map what variables should be displayed on the plot in what way. In this layer you declare e.g. what should be on the x and y axis. The basic aesthetics (the list is not exhaustive) are:\n\nx axis\ny axis\ncolour: colour of points and lines (that includes borders of e.g. rectangles)\nfill: colour of filling\nalpha: transparency. 0 means completely transparent, 1 means not transparency\nsize: point size\nlinewidth: width of lines\nlinetype: type of line (e.g. dashed)\nshape: shape of points (circles, rectangles, etc.)\nlabels: text\n\nAesthetics are declared within aes(). It can be declared within ggplot() function, separately or inside geoms (more on those in a second). Lets see what happens when we add aesthetics to our plot:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege)\n\n\n\n\nNow we got our axes! Notice there is no data on the plot though. That’s because so far we have declared which dataset to plot and which variables to map onto axes but we did not specify how to represent the data. This is declared in the next layer: geometry.\n\n\n\nAfter providing a dataset and aesthetics we have the variables and their mapping to axes on the plot. However, we still don’t have any shapes to actually represent the data. Do we want a scatter plot? Or maybe a bar plot? Or a line plot? The shapes used to represent the data are defined in the geometry layer. Generally all geometries start with geom_ so for example geom_point() will make a scatter plot while geom_bar() will make a bar plot.\nGeometries differ in what aesthetics they accept. You can look up what these are by looking up help for a given geometry. Each geometry has all the aesthetics it accepts in its documentation. They also differ in what kinds of variables they expect (any combination of categorical vs continuous variables).\nLets expand the initial plot with geom_point() to make a scatterplot!\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege) +\n  geom_point()\n\n\n\n\nWe got a plot very similar to the one we made with base R! We can customize it further if we want to. Lets say we want to represent each state with a different color. We just need to add a color aesthetic. R will add a legend automatically:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege, color = state) +\n  geom_point()\n\n\n\n\nThere is one more thing about geometries and aesthetics. Remember you can declare aesthetics in different places? You can set global aesthetics inside the ggplot() function. If you do so these aesthetics will be used by default by all geometries in that plot. You can also set aesthetics inside a given geometry (in fact you can even set a different dataset for a given geometry; this is what we meant by independence of layers) but then they will be used only for this particular geometry and won’t be inherited by other ones.\nCompare the to plots below. They produce the same result:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point()\n\n\n\n\nAnd the second plot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege))\n\n\n\n\nSome common geometries you can encounter are as follows:\n\ngeom_histogram() and geom_density() for displaying distributions of continuous variables\ngeom_bar() for displaying counts of categorical variables (you can change counts to other summary statistic but more on that later)\ngeom_col() for displaying differences in some continuous variable between levels of a factor\ngeom_point(): your good old scatterplot\ngeom_boxplot()\ngeom_violin() a bit like boxplot but displays a distribution of a continuous variable for each level of a factor\ngeom_smooth() for displaying lines of best fit (e.g. from a linear model)\n\nYou can check them out if you want to to see how they look like.\nLets look at 2 thing in a bit more details with regard to geometries. First, lets look at combining geoms. We can add a line of best fit to our scatterplot if we want to by adding an additional geometry. geom_smooth() will use a GAM or LOESS by default (depending on how many unique values x variable has) but we can set it to a linear model by adding method = \"lm\". A linear model probably won’t do well here but it’s just for demonstration:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point() +\n  geom_smooth(method  =\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee? Just like building with lego blocks.\nLast thing before we move on: a few words on geom_bar() and geom_col(). They often get mixed up at the beginning because they both show bar plots. geom_bar() takes a single aesthetic and is generally used to display counts of some factor variable. For example if we wanted to see how many counties are there in each state we could use geom_bar() (later we’ll see how to display e.g. means of variables across levels of a factor):\n\nmidwest %&gt;%\n  ggplot(aes(x=state)) +\n  geom_bar()\n\n\n\n\ngeom_col() takes at least 2 aesthetics: x and y. It needs one categorical and one continuous variable. By default it is going to sum all values in a given category. e.g. lets look at the sum of area of all counties in each of the states:\n\nmidwest %&gt;%\n  ggplot(aes(x=state, y = area)) +\n  geom_col()\n\n\n\n\nWhat if we wanted to look at the average county area in each state? We can e.g. first summarise the dataset and then pipe it to plot:\n\nmidwest %&gt;%\n  summarise(mean_area = mean(area), .by = state) %&gt;%\n  ggplot(aes(x = state, y = mean_area)) +\n  geom_col()\n\n\n\n\nSince ggplot2 is part of tidyverse it’s really easy to pipe a set of dplyr functions into a plot in a single call!\n\n\n\nThere are situations in which you don’t want to set some feature to be represented by a given variable but to set them to a fixed value for the entire plot/geometry. For example you might want to set the color or size of all points in a scatter plot. That’s when you set attributes. They are declared inside geometries but outside of aesthetics. Lets say we want to take our scatterplot and change the transparency and color of the points. We can do it by defining them inside geometry. The names for the arguments are the same as for aesthetics. Just remember to use them outside of aes()!\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3)\n\n\n\n\nBy the way, you can define colors either with hex values as RGB but you can also use one of the built-in colors in R. You can check all the names of those built-in colors with colors() function. Another neat thing is that in newer version of RStudio you can see the preview of the colors when you type them in a script.\nOne potential problem is when aesthetic and scale come into conflict. E.g. what will happen if we map color to state variable and then set it as attribute? Lets see:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), color = \"red\", alpha = .3)\n\n\n\n\nThe attribute overrides the aesthetic. It’s something to remember about.\n\n\n\nOne more thing are functions for working with scales: they allow you to have more control over how each scale is represented (e.g. what the breaks and values are, should the scale be transformed). For example notice that R automatically chose some limits for the scales and displayed breaks (the values on the axes). You can have more control over that with the scale_ family of functions. It’s a family of functions because you need to declare: 1) which aesthetic you want to change and 2) what kind of scale you are working with (continuous, discrete or binned). So for example scale_x_continuous() allows you to customize a continuous x axis. The basic things that you can change inside the scale_ function are: breaks, labels, limits and the expand argument (the last one controls additional space - notice that e.g. on the x axis there is a little space on the left of 0 and there is similarly a little space to the right of the point with the highest percadultpoverty value).\nLets say now we want to work a little with our scales. Lets change the breaks to be every 5%, change the labels to be actually in percentage format (there is a very neat function in scales package called label_percent() that does that. We just need to set scale argument in it to 1 because in our dataset e.g. 10% is represented as 10 and not 0.1):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,40,5),labels = scales::label_percent(scale = 1)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\n\n\n\nA cautionary tale about using limits in scale_ functions: These limits work by filtering the data to be plotted. This can be a serious problem because you basically lose data when plotting and only get a warning about it. This can be especially problematic if you are trying to display bars or ranges because if e.g. one side of an interval falls out of the limits, the entire range will not be displayed. Lets see what happens if we set limits on one of our axes:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,25,5),labels = scales::label_percent(scale = 1), limits = c(0, 25)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice the warning about removed data. This is especially problematic in bar plots because by default they start from 0 (which can be a good thing because it reduces misinterpretation of visual differences between bars but can be undesired e.g. if displaying results of a 1-5 Likert scale which doesn’t have a 0). Another problematic situation is when plotting some summaries e.g. means and limiting axes. Limits will work before calculating the summaries so you might get nonsensical plots because of this like below where we try to plot mean percadultpoverty by each state and limit the y axis to 10:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percadultpoverty)) +\n  geom_bar(stat = \"summary\", fun = mean) +\n  scale_y_continuous(limits = c(0,10))\n\nWarning: Removed 220 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nThe warning now says that 220 rows were removed! What’s even weirded we still got our plot but the means were calculated based on trimmed variable.\n\n\n\nScale functions also allow you to control the color and fill aesthetics. You can change which color palette you want to use. There are many packages available with predefined color palettes (e.g. viridis or MetBrewer). We’ll focus on the built-in Brewer palettes and on making manual palettes. Brewer palettes can be invoked with scale_color_brewer() and scale_fill_brewer(). You can choose color palette by setting the palette argument. I can never remember the names of all the palettes but you can easily google them (just type something like “R brewer palettes). Lets change our palette on the scatterplot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also set your own colors and create your own palette. In order to do that use scale_color_manual() and scale_fill_manual(). You can set the colors as hex or built-in colors in values argument. You can provide it a simple text vector or named vector to explicitly map each categorical variable (if you have categorical variables mapped to color or fill):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_manual(values = c(\"IL\" = \"#5f0f40\", \"IN\" = \"#9a031e\", \"MI\" = \"#fb8b24\", \"OH\" = \"#e36414\", \"WI\" = \"#0f4c5c\"))\n\n\n\n\nfor contiunous variables matched to color aesthetic this works a little different. You need scale_color_gradient() function. scale_color_gradient2() allows you to specify a midpoint and thus make a divergent palette. Lets code county area with color this time and specify our own palette with midpoint at the mean county area:\n\nmidpoint &lt;- mean(midwest$area)\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = area)) +\n  scale_color_gradient2(low = \"#e63946\", mid = \"#8d99ae\", high = \"#1d3557\", midpoint = midpoint)\n\n\n\n\nWhen using colors on a plot you need to be mindful of a number of things. Color is used to convey information so it should be visible. Try to avoid very small contrasts if you want something to stand out (e.g. using light grey on white background). Also remember that not everyone perceives color the same way so it is worth checking if your palette is suitable for everyone. Finally, color shouldn’t be used just because “it looks cool”. Generally everything that is in a plot should be there for a reason. Remember that data visualizations should convey information. That’s their primary purpose. Using color can help with that but it can also make things more difficult to understand. E.g. using a lot of flashy colors just for the sake of using colors might make a plot much more difficult to understand for viewers. Finally, colors can be used to convey different kinds of information, from continuity (e.g. a palette going from light to dark blue to code a continuous variable liek percentage of adults in college), divergence (a palette going from dark blue through light blue and red to dark red to display polarization of opinions) or contrast (e.g. contrasting colors in a palette to display different US states).\n\n\n\nOne issue that pops up quite often, especially when adding color to aesthetics, is how to deal with overlapping values. On the plots above there were some overlapping points on the scatterplot but we could easily deal with it by adjusting transparency. There are situations when this is not so easy or even not desirable. For example you can’t just as easily deal with overlapping bars. ggplot2 has a number of position adjustments that help us deal with this problem. THey allow us to stack, normalize or nudge shapes so that they won’t overlap. The basic position adjustments are:\n\ndodge: move shapes to the side (how much is controlled with the width argument)\nstack: stack shapes on top of each other\nfill: stack shapes on top of each other and normalize height (good for displaying proportions)\njitter: add some random noise (you can adjust how much with height and width arguments). It’s good for cluttered scatterplots\nnudge: slightly move shapes, good for nudging text\n\nOk, lets see some of them in action. We’ll start with jitter. In some situation you might want to display point with a categorical x axis. This can be useful when showing a distribution, e.g. with geom_violin() and adding all the data points on top of it with geom_point(). IF we don’t use any position adjustments we’ll get something like this (we want to look at distribution of percollege in each state):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point()\n\n\n\n\nOk, lets add some jitter but constrain it to be only in width (adding jitter in height might change how we interpret some values!):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point(position = position_jitter(height = 0, width = .2))\n\n\n\n\nThis makes it much easier to see all the points!\nNow we’ll move on to position adjustments in barplots. Lets say we want to look at number of counties in each state that are or are not in metro area(inmetro variable). We can do it by adding a fill aesthetic to the geom_bar():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar()\n\n\n\n\nBy default geom_bar() uses the stack position adjustment. Lets experiment with it a little and see what happens if we add a fill adjustment:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar(position = \"fill\")\n\n\n\n\nNow we have proportions rather than counts. Remember that order here matters (proportions sum to 1 within each state. If we wanted to see the share of each state in metro vs non-metro counties in the midwest we would need to switch x and fill aesthetics).\nTo see position dodge we will look at geom_col(). Lets say we want to plot the average percentage of adults in poverty in metro and nonmetro counties in each state:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(state, inmetro)) %&gt;%\n  ggplot(aes(x = inmetro, y = mean_perc, fill = state)) +\n  geom_col()\n\n\n\n\nThis looks pretty bad right? geom_col() also uses stack adjustment by default. To make it more readable and compare the percentages we can use the dodge adjustment:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(inmetro, state)) %&gt;%\n  ggplot(aes(x = as.factor(inmetro), y = mean_perc, fill = state)) +\n  geom_col(position = position_dodge(width = .9))\n\n\n\n\nMuch better! Now we can compare the percentages across state and metro vs non-metro counties!"
  },
  {
    "objectID": "09data_viz_1.html#some-base-r-graphics",
    "href": "09data_viz_1.html#some-base-r-graphics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Before we move on to ggplot2 lets look at some built-in base R graphics. The graphics and stats packages have some function for plotting already available.\n\nThe most generic is the plot() function. It allows you to create simple plots in R. The first arguments are usually the variables you want to map onto the axes:\n\nplot(midwest$percadultpoverty, midwest$percollege)\n\n\n\n\nplot() function has a bunch of arguments you can use to customize the plot. For example we can change the color and thickness of the points and add a title to the plot and axes:\n\nplot(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2,\n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nUnfortunately the plot() function does not have great documentation and finding some of the arguments can be quite difficult. Some of these arguments also have very unintuitive names (e.g. argument named lty specifies line type, good luck memorizing that!). Base R has additional functions for more specific plots. Namely, lines() will create a line plot, points() will create a scatterplot and boxplot() will create a boxplot. These functions have better documentation than the general plot() though I still don’t consider it great. Lets see some of these in action. lines() and points() require that you first initialize the plots with the coordinates set to the variables of interest with the plot() function. You can specify what kind of plot (line plot, scatterplot etc) you want inside plot() by setting the type argument. E.g. setting it to l will create a line plot.\nLines: Lets say we want to make a line plot that will display the 10th, 50th and 90th quantile of percent of adults in poverty in all states. We’ll also introduce the axis() function which gives you some control over how the x and y axis should look like. Here we need it because lines() does not like categorical values at x axis so we need to add the state names (we add them at the top) with axis():\n\n#get the quantiles\nlibrary(dplyr)\n#calculate quantiles at of percadultpoverty for each state\nlist_q &lt;- tapply(midwest$percadultpoverty, midwest$state, quantile, c(.1,.5,.9))\n\n#convert the result into a dataframe with each column as one quantile and each row as one state\ndf_q &lt;- data.frame()\ndf_q &lt;- bind_rows(list_q[[1]], list_q[[2]], list_q[[3]], list_q[[4]], list_q[[5]])\ncolnames(df_q) &lt;- c(\"q_10\", \"q_50\", \"q_90\")\n\n#plot them\nplot(1:5, df_q$q_10, ylim = c(0,20), type = \"l\", col = 2, lwd = 2, lty = 2)\nlines(1:5, df_q$q_50, col = 4, lwd = 2, lty = 1)\nlines(1:5, df_q$q_90, col = 2, lwd = 2, lty = 2)\naxis(side = 3, at = 1:5, labels = unique(midwest$state))\n\n\n\n\nOne important thing here: notice how each element is added to the plot on a new line. These functions are not chained together in any way and we are not saving any intermediate objects. This is pretty unusual in R (although perfectly normal in other programming languages). It’s just how base R plotting works.\nPoints: We can recreate the plot that we made with the generic plot() function but using points():\n\nplot(midwest$percadultpoverty, midwest$percollege)\npoints(midwest$percadultpoverty, midwest$percollege, col = 2, lwd = 2, \n     main = \"Percent in college vs percent adults in poverty\",\n     xlab = \"Percent adults in poverty\",\n     ylab = \"percent in college\")\n\n\n\n\nBoxplots are useful for showing differences in distributions of some continuous variable between levels of some factor. For example lets say we want to\n\nboxplot(midwest$percadultpoverty ~ midwest$state)\n\n\n\n\n\nThere are also built-in functions for plotting distributions: hist() for histograms, plot(density()) for density functions and plot.ecdf() for cumulative distribution plots. Lets look at all 3 plots for percent of population in college:\n\nhist(midwest$percollege)\n\n\n\n\n\nplot(density(midwest$percollege))\n\n\n\n\n\nplot.ecdf(midwest$percollege)\n\n\n\n\nOne problem with base R plots is that they are not intuitive. Lots of arguments have weird names and doing some things is really not so easy. Making more complicated plots (e.g. adding text annotations on the plot) is also generally hard to do. That’s why we’ll focus on ggplot2 which is much more intuitive and versatile."
  },
  {
    "objectID": "09data_viz_1.html#enter-ggplot2",
    "href": "09data_viz_1.html#enter-ggplot2",
    "title": "Data visualization part 1",
    "section": "",
    "text": "ggplot2 is a package in th tidyverse designed for making data visualizations. One of the great things about it is that it breaks down each plot into a number of layers that can be changed (more or less) independently. This idea is encapsulated in what is called the grammar of graphics."
  },
  {
    "objectID": "09data_viz_1.html#grammar-of-graphics",
    "href": "09data_viz_1.html#grammar-of-graphics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "The name comes from a book by Leland Wilkinson under the same title. Basically making a plot with the grammar of graphics is like making a building with lego blocks with different colors. You can mix the colors of the blocks to get exactly the building you want. Similalry in ggplot2 you can use different layers to make the plot that you want (e.g. mix different datasets or add a line to a scatterplot). Ok, but what are those layers? In ggplot2 there are following layers:\n\nData: the datasets (usually data frames) you want to plot\nAesthetics: mapping between data and the plot, e.g. what is to be mapped to x and y axis\nGeometry: shapes used to represent data\nStatistics: any statistics like means or confidence intervals that you want to add\nCoordinates: coordinates of the plot (axes limits, cartesian vs polar coordinates etc.)\nFacets: this layer is used for making subplots (e.g. separate plot for each level of a categorical variable)\nTheme: All non-data stuff like fonts, titles\n\nYou always start the plot with ggplot() function. All the lego blocks that you want to add are chained together with +. Lets see what happens when we pass the first layer, data, to our plot:\n\nmidwest %&gt;%\n  ggplot()\n\n\n\n\nHmm, we get en empty plot. Why is that? Well, all we supplied so far is the data we want to plot but we did not include any additional information so R doesn’t know yet what exactly should be displayed on the plot. We need to add the next layer: aesthetics."
  },
  {
    "objectID": "09data_viz_1.html#aesthetics",
    "href": "09data_viz_1.html#aesthetics",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Aesthetics map what variables should be displayed on the plot in what way. In this layer you declare e.g. what should be on the x and y axis. The basic aesthetics (the list is not exhaustive) are:\n\nx axis\ny axis\ncolour: colour of points and lines (that includes borders of e.g. rectangles)\nfill: colour of filling\nalpha: transparency. 0 means completely transparent, 1 means not transparency\nsize: point size\nlinewidth: width of lines\nlinetype: type of line (e.g. dashed)\nshape: shape of points (circles, rectangles, etc.)\nlabels: text\n\nAesthetics are declared within aes(). It can be declared within ggplot() function, separately or inside geoms (more on those in a second). Lets see what happens when we add aesthetics to our plot:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege)\n\n\n\n\nNow we got our axes! Notice there is no data on the plot though. That’s because so far we have declared which dataset to plot and which variables to map onto axes but we did not specify how to represent the data. This is declared in the next layer: geometry."
  },
  {
    "objectID": "09data_viz_1.html#geometries",
    "href": "09data_viz_1.html#geometries",
    "title": "Data visualization part 1",
    "section": "",
    "text": "After providing a dataset and aesthetics we have the variables and their mapping to axes on the plot. However, we still don’t have any shapes to actually represent the data. Do we want a scatter plot? Or maybe a bar plot? Or a line plot? The shapes used to represent the data are defined in the geometry layer. Generally all geometries start with geom_ so for example geom_point() will make a scatter plot while geom_bar() will make a bar plot.\nGeometries differ in what aesthetics they accept. You can look up what these are by looking up help for a given geometry. Each geometry has all the aesthetics it accepts in its documentation. They also differ in what kinds of variables they expect (any combination of categorical vs continuous variables).\nLets expand the initial plot with geom_point() to make a scatterplot!\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege) +\n  geom_point()\n\n\n\n\nWe got a plot very similar to the one we made with base R! We can customize it further if we want to. Lets say we want to represent each state with a different color. We just need to add a color aesthetic. R will add a legend automatically:\n\nmidwest %&gt;%\n  ggplot() +\n  aes(x = percadultpoverty, y = percollege, color = state) +\n  geom_point()\n\n\n\n\nThere is one more thing about geometries and aesthetics. Remember you can declare aesthetics in different places? You can set global aesthetics inside the ggplot() function. If you do so these aesthetics will be used by default by all geometries in that plot. You can also set aesthetics inside a given geometry (in fact you can even set a different dataset for a given geometry; this is what we meant by independence of layers) but then they will be used only for this particular geometry and won’t be inherited by other ones.\nCompare the to plots below. They produce the same result:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point()\n\n\n\n\nAnd the second plot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege))\n\n\n\n\nSome common geometries you can encounter are as follows:\n\ngeom_histogram() and geom_density() for displaying distributions of continuous variables\ngeom_bar() for displaying counts of categorical variables (you can change counts to other summary statistic but more on that later)\ngeom_col() for displaying differences in some continuous variable between levels of a factor\ngeom_point(): your good old scatterplot\ngeom_boxplot()\ngeom_violin() a bit like boxplot but displays a distribution of a continuous variable for each level of a factor\ngeom_smooth() for displaying lines of best fit (e.g. from a linear model)\n\nYou can check them out if you want to to see how they look like.\nLets look at 2 thing in a bit more details with regard to geometries. First, lets look at combining geoms. We can add a line of best fit to our scatterplot if we want to by adding an additional geometry. geom_smooth() will use a GAM or LOESS by default (depending on how many unique values x variable has) but we can set it to a linear model by adding method = \"lm\". A linear model probably won’t do well here but it’s just for demonstration:\n\nmidwest %&gt;%\n  ggplot(aes(x = percadultpoverty, y = percollege)) +\n  geom_point() +\n  geom_smooth(method  =\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee? Just like building with lego blocks.\nLast thing before we move on: a few words on geom_bar() and geom_col(). They often get mixed up at the beginning because they both show bar plots. geom_bar() takes a single aesthetic and is generally used to display counts of some factor variable. For example if we wanted to see how many counties are there in each state we could use geom_bar() (later we’ll see how to display e.g. means of variables across levels of a factor):\n\nmidwest %&gt;%\n  ggplot(aes(x=state)) +\n  geom_bar()\n\n\n\n\ngeom_col() takes at least 2 aesthetics: x and y. It needs one categorical and one continuous variable. By default it is going to sum all values in a given category. e.g. lets look at the sum of area of all counties in each of the states:\n\nmidwest %&gt;%\n  ggplot(aes(x=state, y = area)) +\n  geom_col()\n\n\n\n\nWhat if we wanted to look at the average county area in each state? We can e.g. first summarise the dataset and then pipe it to plot:\n\nmidwest %&gt;%\n  summarise(mean_area = mean(area), .by = state) %&gt;%\n  ggplot(aes(x = state, y = mean_area)) +\n  geom_col()\n\n\n\n\nSince ggplot2 is part of tidyverse it’s really easy to pipe a set of dplyr functions into a plot in a single call!"
  },
  {
    "objectID": "09data_viz_1.html#attributes",
    "href": "09data_viz_1.html#attributes",
    "title": "Data visualization part 1",
    "section": "",
    "text": "There are situations in which you don’t want to set some feature to be represented by a given variable but to set them to a fixed value for the entire plot/geometry. For example you might want to set the color or size of all points in a scatter plot. That’s when you set attributes. They are declared inside geometries but outside of aesthetics. Lets say we want to take our scatterplot and change the transparency and color of the points. We can do it by defining them inside geometry. The names for the arguments are the same as for aesthetics. Just remember to use them outside of aes()!\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3)\n\n\n\n\nBy the way, you can define colors either with hex values as RGB but you can also use one of the built-in colors in R. You can check all the names of those built-in colors with colors() function. Another neat thing is that in newer version of RStudio you can see the preview of the colors when you type them in a script.\nOne potential problem is when aesthetic and scale come into conflict. E.g. what will happen if we map color to state variable and then set it as attribute? Lets see:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), color = \"red\", alpha = .3)\n\n\n\n\nThe attribute overrides the aesthetic. It’s something to remember about."
  },
  {
    "objectID": "09data_viz_1.html#scales",
    "href": "09data_viz_1.html#scales",
    "title": "Data visualization part 1",
    "section": "",
    "text": "One more thing are functions for working with scales: they allow you to have more control over how each scale is represented (e.g. what the breaks and values are, should the scale be transformed). For example notice that R automatically chose some limits for the scales and displayed breaks (the values on the axes). You can have more control over that with the scale_ family of functions. It’s a family of functions because you need to declare: 1) which aesthetic you want to change and 2) what kind of scale you are working with (continuous, discrete or binned). So for example scale_x_continuous() allows you to customize a continuous x axis. The basic things that you can change inside the scale_ function are: breaks, labels, limits and the expand argument (the last one controls additional space - notice that e.g. on the x axis there is a little space on the left of 0 and there is similarly a little space to the right of the point with the highest percadultpoverty value).\nLets say now we want to work a little with our scales. Lets change the breaks to be every 5%, change the labels to be actually in percentage format (there is a very neat function in scales package called label_percent() that does that. We just need to set scale argument in it to 1 because in our dataset e.g. 10% is represented as 10 and not 0.1):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,40,5),labels = scales::label_percent(scale = 1)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\n\n\n\nA cautionary tale about using limits in scale_ functions: These limits work by filtering the data to be plotted. This can be a serious problem because you basically lose data when plotting and only get a warning about it. This can be especially problematic if you are trying to display bars or ranges because if e.g. one side of an interval falls out of the limits, the entire range will not be displayed. Lets see what happens if we set limits on one of our axes:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege), color = \"red\", alpha = .3) + scale_x_continuous(breaks = seq(0,25,5),labels = scales::label_percent(scale = 1), limits = c(0, 25)) +\n  scale_y_continuous(breaks = seq(0,50,5),labels = scales::label_percent(scale = 1))\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice the warning about removed data. This is especially problematic in bar plots because by default they start from 0 (which can be a good thing because it reduces misinterpretation of visual differences between bars but can be undesired e.g. if displaying results of a 1-5 Likert scale which doesn’t have a 0). Another problematic situation is when plotting some summaries e.g. means and limiting axes. Limits will work before calculating the summaries so you might get nonsensical plots because of this like below where we try to plot mean percadultpoverty by each state and limit the y axis to 10:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percadultpoverty)) +\n  geom_bar(stat = \"summary\", fun = mean) +\n  scale_y_continuous(limits = c(0,10))\n\nWarning: Removed 220 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nThe warning now says that 220 rows were removed! What’s even weirded we still got our plot but the means were calculated based on trimmed variable."
  },
  {
    "objectID": "09data_viz_1.html#colors",
    "href": "09data_viz_1.html#colors",
    "title": "Data visualization part 1",
    "section": "",
    "text": "Scale functions also allow you to control the color and fill aesthetics. You can change which color palette you want to use. There are many packages available with predefined color palettes (e.g. viridis or MetBrewer). We’ll focus on the built-in Brewer palettes and on making manual palettes. Brewer palettes can be invoked with scale_color_brewer() and scale_fill_brewer(). You can choose color palette by setting the palette argument. I can never remember the names of all the palettes but you can easily google them (just type something like “R brewer palettes). Lets change our palette on the scatterplot:\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\nYou can also set your own colors and create your own palette. In order to do that use scale_color_manual() and scale_fill_manual(). You can set the colors as hex or built-in colors in values argument. You can provide it a simple text vector or named vector to explicitly map each categorical variable (if you have categorical variables mapped to color or fill):\n\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = state), alpha = .8) +\n  scale_color_manual(values = c(\"IL\" = \"#5f0f40\", \"IN\" = \"#9a031e\", \"MI\" = \"#fb8b24\", \"OH\" = \"#e36414\", \"WI\" = \"#0f4c5c\"))\n\n\n\n\nfor contiunous variables matched to color aesthetic this works a little different. You need scale_color_gradient() function. scale_color_gradient2() allows you to specify a midpoint and thus make a divergent palette. Lets code county area with color this time and specify our own palette with midpoint at the mean county area:\n\nmidpoint &lt;- mean(midwest$area)\nmidwest %&gt;%\n  ggplot() +\n  geom_point(aes(x = percadultpoverty, y = percollege, color = area)) +\n  scale_color_gradient2(low = \"#e63946\", mid = \"#8d99ae\", high = \"#1d3557\", midpoint = midpoint)\n\n\n\n\nWhen using colors on a plot you need to be mindful of a number of things. Color is used to convey information so it should be visible. Try to avoid very small contrasts if you want something to stand out (e.g. using light grey on white background). Also remember that not everyone perceives color the same way so it is worth checking if your palette is suitable for everyone. Finally, color shouldn’t be used just because “it looks cool”. Generally everything that is in a plot should be there for a reason. Remember that data visualizations should convey information. That’s their primary purpose. Using color can help with that but it can also make things more difficult to understand. E.g. using a lot of flashy colors just for the sake of using colors might make a plot much more difficult to understand for viewers. Finally, colors can be used to convey different kinds of information, from continuity (e.g. a palette going from light to dark blue to code a continuous variable liek percentage of adults in college), divergence (a palette going from dark blue through light blue and red to dark red to display polarization of opinions) or contrast (e.g. contrasting colors in a palette to display different US states)."
  },
  {
    "objectID": "09data_viz_1.html#positions",
    "href": "09data_viz_1.html#positions",
    "title": "Data visualization part 1",
    "section": "",
    "text": "One issue that pops up quite often, especially when adding color to aesthetics, is how to deal with overlapping values. On the plots above there were some overlapping points on the scatterplot but we could easily deal with it by adjusting transparency. There are situations when this is not so easy or even not desirable. For example you can’t just as easily deal with overlapping bars. ggplot2 has a number of position adjustments that help us deal with this problem. THey allow us to stack, normalize or nudge shapes so that they won’t overlap. The basic position adjustments are:\n\ndodge: move shapes to the side (how much is controlled with the width argument)\nstack: stack shapes on top of each other\nfill: stack shapes on top of each other and normalize height (good for displaying proportions)\njitter: add some random noise (you can adjust how much with height and width arguments). It’s good for cluttered scatterplots\nnudge: slightly move shapes, good for nudging text\n\nOk, lets see some of them in action. We’ll start with jitter. In some situation you might want to display point with a categorical x axis. This can be useful when showing a distribution, e.g. with geom_violin() and adding all the data points on top of it with geom_point(). IF we don’t use any position adjustments we’ll get something like this (we want to look at distribution of percollege in each state):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point()\n\n\n\n\nOk, lets add some jitter but constrain it to be only in width (adding jitter in height might change how we interpret some values!):\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege)) +\n  geom_violin(alpha = .4) +\n  geom_point(position = position_jitter(height = 0, width = .2))\n\n\n\n\nThis makes it much easier to see all the points!\nNow we’ll move on to position adjustments in barplots. Lets say we want to look at number of counties in each state that are or are not in metro area(inmetro variable). We can do it by adding a fill aesthetic to the geom_bar():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar()\n\n\n\n\nBy default geom_bar() uses the stack position adjustment. Lets experiment with it a little and see what happens if we add a fill adjustment:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = as.factor(inmetro))) +\n  geom_bar(position = \"fill\")\n\n\n\n\nNow we have proportions rather than counts. Remember that order here matters (proportions sum to 1 within each state. If we wanted to see the share of each state in metro vs non-metro counties in the midwest we would need to switch x and fill aesthetics).\nTo see position dodge we will look at geom_col(). Lets say we want to plot the average percentage of adults in poverty in metro and nonmetro counties in each state:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(state, inmetro)) %&gt;%\n  ggplot(aes(x = inmetro, y = mean_perc, fill = state)) +\n  geom_col()\n\n\n\n\nThis looks pretty bad right? geom_col() also uses stack adjustment by default. To make it more readable and compare the percentages we can use the dodge adjustment:\n\nmidwest %&gt;%\n  summarise(mean_perc = mean(percadultpoverty), .by = c(inmetro, state)) %&gt;%\n  ggplot(aes(x = as.factor(inmetro), y = mean_perc, fill = state)) +\n  geom_col(position = position_dodge(width = .9))\n\n\n\n\nMuch better! Now we can compare the percentages across state and metro vs non-metro counties!"
  },
  {
    "objectID": "10data_viz_2.html",
    "href": "10data_viz_2.html",
    "title": "Data visualization part 2",
    "section": "",
    "text": "We know how to make various plots to display the data and how to control the attributes and scales. Now we’ll move on to additional layers in ggplot2. We’ll continue working with the midwest dataset from the previous lesson.\n\n\nWe’ll start with faceting as it is pretty straightforward. Plotting often involves including a lot of information in a single plot. Sometimes the amount of information might be too much to easily comprehend. One of the ways in which we can try to deal with this is by faceting. It allows us to create a separate subplot for each level of a factor variable. The two basic functions for faceting in ggplot2 are facet_wrap() and facet_grid(). The main difference is in their syntax and how they display facets. The first one takes a formula-like syntax: facet_wrap(~variable). It automatically tries to fit rows and columns and adjust the number of rows and colums (you can customize that if you want). The second one needs you to specify the rows and columns manually: facet_grid(rows = vars(variable1), cols = vars(variable2).\nFor example lets say we are interested in relation between percentage of children below poverty and percentage of people with college degree in all 5 state with a split between metro and non-metro areas. That’s a lot to cram into one plot so we can use faceting to make the plot more readable. Using facet_wrap() is super easy here:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, y = percchildbelowpovert, color = as.factor(inmetro))) +\n  geom_point() +\n  facet_wrap(~state)\n\n\n\n\nIf we wanted to display the same information but with facet_grid() we could do it like this (lets put inmetro as one of facets instead of color as well):\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, y = percchildbelowpovert)) +\n  geom_point() +\n  facet_grid(rows = vars(state), cols = vars(inmetro))\n\n\n\n\nBy default R will fix x and y axis across all facets (which is generally good!). If you want to free any axis by setting the scales argument to free_x or free_y or free to free both axes.\n\n\n\nThis layer allows us to add various statistics like means and confidence intervals to our plots. Functions in this layer start with stat_ although sometimes they can be called within geoms as well.\nOne obvious way in which we can include summary statistics on our plot is to calculate them first and then pipe them to the plot. Two additional geoms that might come in handy here are geom_errorbar() and geom_pointrange() which allow you to display ranges (the second one with a mid point as well). We can make a very classic plot often called a dynamite plot to show means and a range of 1 standard error like this:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_col(aes(x = state, y = mean_pct_college)) +\n  geom_errorbar(aes(x = state, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college), width = .2)\n\n\n\n\nYou can see how they resemble dynamites! Recall that geom_col() by default will start y axis from 0. If we wanted to use geom_pointrange() we could do everything in a single geom:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_pointrange(aes(x = state, y = mean_pct_college, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college))\n\n\n\n\nSo we could go even further and add distributions to our plot to give us more information:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_violin(data = midwest, aes(x = state, y = percollege, color = state, fill = state), alpha = .6) +\n  geom_pointrange(aes(x = state, y = mean_pct_college, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college))\n\n\n\n\nWe get much more information though when we show the entire distribution the ranges for standard errors become tiny. Also notice how geom_violin() specifies a new dataset, midwest rather than work on summaries. This again shows how ggplot2 is like building lego - you can mix blocks and layers to build whatever you want.\nGenerally, dynamite plots are discouraged now. One problem with them is that they limit the number of information. A given mean can come from any number of underlying distributions and what those distributions are matters! An interpretation of a mean will differ depending on whether the distribution is nicely symmetrical or heavily skewed. What’s more, bar plots aren’t really the best way to display things like means. When we look at a bar it usually implies some count or proportion rather than a point estimate. A point estimate is, well… a point so maybe using points is better here.\nWe don’t need to calculate all the statistics we want to display before we make the plot. For the most common things we can easily do it inside the plot by using stat_summary(). This function needs you to specify what function you want to plot (e.g. mean for average value or mean_se for average value and standard error range around it) and what geometry you want to show it. There are two arguments for specifying functions in stat_summary() and they differ in what they output. The first one, fun.data always returns a ymin, y and ymax values so it’s perfect for displaying ranges around some value (like a mean and confidence interval). The second one, fun will return a single value y. You can add your own fun.min and fun.max if you want to. Lets remake the same plot as above but this time using stat_summary():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege, color = state, fill = state)) +\n  geom_violin(alpha = .6) +\n  stat_summary(fun.data = \"mean_se\", color = \"black\")\n\n\n\n\nYay, we got an identical plot!\nAnother function called stat_function() allows you to apply a custom statistical function and plot it. For example you can use it to plot cumulative density plot. The ecdf() function creates a function to generate the cumulative density function from a variable. Important thing about it is that it creates a function and not a vector of values or a dataframe. We can then use it inside stat_function() and add a line geom:\n\ntest &lt;- ecdf(midwest$percollege)\nmidwest %&gt;%\n  ggplot(aes(x = percollege)) +\n  stat_function(fun = test, geom = \"line\")\n\n\n\n\n\n\n\nThis layer controls how the coordinates of the plot should be handled. Most likely you are used to plots with the cartesian space: an x and y axis that are perpendicular. However you can change that e.g. by fising the ratio of x to y axis, zooming in on a particualr part of the plot or even bending the axis altogether. All of this is handled in this layer. Its functions start with coord_. The basic one is coord_cartesian():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege)) +\n  geom_point() +\n  coord_cartesian(xlim = c(0, 25))\n\n\n\n\nNotice that this time we did not get any warnings about missing data. This way we can e.g. adjust the axis limits of a bar plot without losing the bars.\nNow for slightly weirder stuff. What if we bended the coordinates so that they are no longer simple cartesian but polar instead (imagine taking the x axis and bending it into a circle)? We get something like this:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = state)) +\n  geom_bar(width = 1) +\n  coord_polar()\n\n\n\n\nThis isn’t really a common thing to do in plots. Some situations in which it might be justified is with some circular forms, e.g. when displaying hours\n\n\n\nThe final layer, called theme, controls all the non-data part of the plot: setting fonts, typeface, text size, controlling the background color, plot legend and the grid. It’s generally controlled within the theme() function. And there’s a lot to control! Theme layer is huge. If you look at the documentation of theme() function you can see just how many things can be customized in it. Apart from a few exceptions setting anything inside theme() is done by setting a specific argument to an appropriate type of object on the plot. There are 3 general types: element_line(), element_text() and element_rect(). Inside each of these 3 functions you specify everything you want to customize. If you want to turn off an element you can use element_blank(). The only exception is the legend which for some reason is turned off by using legend.position = \"none\" (by the way I think this is one of the most googled things by R users). You can see all of them used below:\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  labs(title = \"Relation between percent below poverty and\\npercent with college education in each state\", x = \"Percent below poverty\", y = \"Percent with college degree\") +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color = \"grey60\"),\n        title = element_text(face = \"bold\", colour = \"#5f0f40\"),\n        panel.background = element_rect(fill = \"#AAAAAA\"),\n        plot.background = element_rect(fill = \"#AAAAAA\"),\n        legend.background = element_rect(fill = \"#AAAAAA\"),\n        legend.key = element_rect(fill = \"#AAAAAA\"),\n        legend.position = \"top\")\n\n\n\n\nApart from the theme() function there is a number of pre-specified themes that you can use. They allow you to quickly get a specific theme without needing to change everything manually. They can also be a good start for further customization. Lets see some examples:\n\ntheme_minimal():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\ntheme_bw():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\ntheme_classic():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\nYou can also create your own theme! You just need to create your own function:\n\ntheme_own &lt;- function() {\n  theme_minimal() +\n    theme(text = element_text(face = \"bold\", color = \"#9a031e\"),\n          panel.background = element_rect(fill = \"grey60\", color = \"grey60\"),\n          plot.background = element_rect(fill = \"grey60\", color = \"grey60\"),\n          legend.background = element_rect(fill = \"grey60\"),\n          legend.key = element_rect(fill = \"grey60\"),\n          panel.grid = element_line(linetype = \"dashed\", color = \"grey40\"))\n}\n\nNow we can use it in our plots!\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_own()\n\n\n\n\n\n\nA note on working with fonts: if you want to work with more fonts than the built-in ones you will have to load them into R. This can sometimes prove quite problematic. The showtext package makes it much easier to work with e.g. Google fonts. Fonts are declared inside theme() in family argument.\n\nlibrary(showtext)\n\nŁadowanie wymaganego pakietu: sysfonts\n\n\nŁadowanie wymaganego pakietu: showtextdb\n\n#load the Merriweather font\nfont_add_google(\"Merriweather\")\n#set the fonts to automatically appear\nshowtext_auto()\n\n#set the font inside our plot\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  labs(title = \"Relation between percent below poverty and\\npercent with college education in each state\") +\n  theme(text = element_text(family = \"Merriweather\"))"
  },
  {
    "objectID": "10data_viz_2.html#faceting",
    "href": "10data_viz_2.html#faceting",
    "title": "Data visualization part 2",
    "section": "",
    "text": "We’ll start with faceting as it is pretty straightforward. Plotting often involves including a lot of information in a single plot. Sometimes the amount of information might be too much to easily comprehend. One of the ways in which we can try to deal with this is by faceting. It allows us to create a separate subplot for each level of a factor variable. The two basic functions for faceting in ggplot2 are facet_wrap() and facet_grid(). The main difference is in their syntax and how they display facets. The first one takes a formula-like syntax: facet_wrap(~variable). It automatically tries to fit rows and columns and adjust the number of rows and colums (you can customize that if you want). The second one needs you to specify the rows and columns manually: facet_grid(rows = vars(variable1), cols = vars(variable2).\nFor example lets say we are interested in relation between percentage of children below poverty and percentage of people with college degree in all 5 state with a split between metro and non-metro areas. That’s a lot to cram into one plot so we can use faceting to make the plot more readable. Using facet_wrap() is super easy here:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, y = percchildbelowpovert, color = as.factor(inmetro))) +\n  geom_point() +\n  facet_wrap(~state)\n\n\n\n\nIf we wanted to display the same information but with facet_grid() we could do it like this (lets put inmetro as one of facets instead of color as well):\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, y = percchildbelowpovert)) +\n  geom_point() +\n  facet_grid(rows = vars(state), cols = vars(inmetro))\n\n\n\n\nBy default R will fix x and y axis across all facets (which is generally good!). If you want to free any axis by setting the scales argument to free_x or free_y or free to free both axes."
  },
  {
    "objectID": "10data_viz_2.html#statistics",
    "href": "10data_viz_2.html#statistics",
    "title": "Data visualization part 2",
    "section": "",
    "text": "This layer allows us to add various statistics like means and confidence intervals to our plots. Functions in this layer start with stat_ although sometimes they can be called within geoms as well.\nOne obvious way in which we can include summary statistics on our plot is to calculate them first and then pipe them to the plot. Two additional geoms that might come in handy here are geom_errorbar() and geom_pointrange() which allow you to display ranges (the second one with a mid point as well). We can make a very classic plot often called a dynamite plot to show means and a range of 1 standard error like this:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_col(aes(x = state, y = mean_pct_college)) +\n  geom_errorbar(aes(x = state, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college), width = .2)\n\n\n\n\nYou can see how they resemble dynamites! Recall that geom_col() by default will start y axis from 0. If we wanted to use geom_pointrange() we could do everything in a single geom:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_pointrange(aes(x = state, y = mean_pct_college, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college))\n\n\n\n\nSo we could go even further and add distributions to our plot to give us more information:\n\nmidwest %&gt;%\n  group_by(state) %&gt;%\n  summarise(mean_pct_college = mean(percollege),\n            sd_pct_college = sd(percollege),\n            se_pct_college = sd_pct_college/sqrt(n())) %&gt;%\n  ggplot() +\n  geom_violin(data = midwest, aes(x = state, y = percollege, color = state, fill = state), alpha = .6) +\n  geom_pointrange(aes(x = state, y = mean_pct_college, ymin = mean_pct_college - se_pct_college, ymax = mean_pct_college + se_pct_college))\n\n\n\n\nWe get much more information though when we show the entire distribution the ranges for standard errors become tiny. Also notice how geom_violin() specifies a new dataset, midwest rather than work on summaries. This again shows how ggplot2 is like building lego - you can mix blocks and layers to build whatever you want.\nGenerally, dynamite plots are discouraged now. One problem with them is that they limit the number of information. A given mean can come from any number of underlying distributions and what those distributions are matters! An interpretation of a mean will differ depending on whether the distribution is nicely symmetrical or heavily skewed. What’s more, bar plots aren’t really the best way to display things like means. When we look at a bar it usually implies some count or proportion rather than a point estimate. A point estimate is, well… a point so maybe using points is better here.\nWe don’t need to calculate all the statistics we want to display before we make the plot. For the most common things we can easily do it inside the plot by using stat_summary(). This function needs you to specify what function you want to plot (e.g. mean for average value or mean_se for average value and standard error range around it) and what geometry you want to show it. There are two arguments for specifying functions in stat_summary() and they differ in what they output. The first one, fun.data always returns a ymin, y and ymax values so it’s perfect for displaying ranges around some value (like a mean and confidence interval). The second one, fun will return a single value y. You can add your own fun.min and fun.max if you want to. Lets remake the same plot as above but this time using stat_summary():\n\nmidwest %&gt;%\n  ggplot(aes(x = state, y = percollege, color = state, fill = state)) +\n  geom_violin(alpha = .6) +\n  stat_summary(fun.data = \"mean_se\", color = \"black\")\n\n\n\n\nYay, we got an identical plot!\nAnother function called stat_function() allows you to apply a custom statistical function and plot it. For example you can use it to plot cumulative density plot. The ecdf() function creates a function to generate the cumulative density function from a variable. Important thing about it is that it creates a function and not a vector of values or a dataframe. We can then use it inside stat_function() and add a line geom:\n\ntest &lt;- ecdf(midwest$percollege)\nmidwest %&gt;%\n  ggplot(aes(x = percollege)) +\n  stat_function(fun = test, geom = \"line\")"
  },
  {
    "objectID": "10data_viz_2.html#coordinates",
    "href": "10data_viz_2.html#coordinates",
    "title": "Data visualization part 2",
    "section": "",
    "text": "This layer controls how the coordinates of the plot should be handled. Most likely you are used to plots with the cartesian space: an x and y axis that are perpendicular. However you can change that e.g. by fising the ratio of x to y axis, zooming in on a particualr part of the plot or even bending the axis altogether. All of this is handled in this layer. Its functions start with coord_. The basic one is coord_cartesian():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege)) +\n  geom_point() +\n  coord_cartesian(xlim = c(0, 25))\n\n\n\n\nNotice that this time we did not get any warnings about missing data. This way we can e.g. adjust the axis limits of a bar plot without losing the bars.\nNow for slightly weirder stuff. What if we bended the coordinates so that they are no longer simple cartesian but polar instead (imagine taking the x axis and bending it into a circle)? We get something like this:\n\nmidwest %&gt;%\n  ggplot(aes(x = state, fill = state)) +\n  geom_bar(width = 1) +\n  coord_polar()\n\n\n\n\nThis isn’t really a common thing to do in plots. Some situations in which it might be justified is with some circular forms, e.g. when displaying hours"
  },
  {
    "objectID": "10data_viz_2.html#theme",
    "href": "10data_viz_2.html#theme",
    "title": "Data visualization part 2",
    "section": "",
    "text": "The final layer, called theme, controls all the non-data part of the plot: setting fonts, typeface, text size, controlling the background color, plot legend and the grid. It’s generally controlled within the theme() function. And there’s a lot to control! Theme layer is huge. If you look at the documentation of theme() function you can see just how many things can be customized in it. Apart from a few exceptions setting anything inside theme() is done by setting a specific argument to an appropriate type of object on the plot. There are 3 general types: element_line(), element_text() and element_rect(). Inside each of these 3 functions you specify everything you want to customize. If you want to turn off an element you can use element_blank(). The only exception is the legend which for some reason is turned off by using legend.position = \"none\" (by the way I think this is one of the most googled things by R users). You can see all of them used below:\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  labs(title = \"Relation between percent below poverty and\\npercent with college education in each state\", x = \"Percent below poverty\", y = \"Percent with college degree\") +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(color = \"grey60\"),\n        title = element_text(face = \"bold\", colour = \"#5f0f40\"),\n        panel.background = element_rect(fill = \"#AAAAAA\"),\n        plot.background = element_rect(fill = \"#AAAAAA\"),\n        legend.background = element_rect(fill = \"#AAAAAA\"),\n        legend.key = element_rect(fill = \"#AAAAAA\"),\n        legend.position = \"top\")\n\n\n\n\nApart from the theme() function there is a number of pre-specified themes that you can use. They allow you to quickly get a specific theme without needing to change everything manually. They can also be a good start for further customization. Lets see some examples:\n\ntheme_minimal():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\ntheme_bw():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\ntheme_classic():\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\nYou can also create your own theme! You just need to create your own function:\n\ntheme_own &lt;- function() {\n  theme_minimal() +\n    theme(text = element_text(face = \"bold\", color = \"#9a031e\"),\n          panel.background = element_rect(fill = \"grey60\", color = \"grey60\"),\n          plot.background = element_rect(fill = \"grey60\", color = \"grey60\"),\n          legend.background = element_rect(fill = \"grey60\"),\n          legend.key = element_rect(fill = \"grey60\"),\n          panel.grid = element_line(linetype = \"dashed\", color = \"grey40\"))\n}\n\nNow we can use it in our plots!\n\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  theme_own()\n\n\n\n\n\n\nA note on working with fonts: if you want to work with more fonts than the built-in ones you will have to load them into R. This can sometimes prove quite problematic. The showtext package makes it much easier to work with e.g. Google fonts. Fonts are declared inside theme() in family argument.\n\nlibrary(showtext)\n\nŁadowanie wymaganego pakietu: sysfonts\n\n\nŁadowanie wymaganego pakietu: showtextdb\n\n#load the Merriweather font\nfont_add_google(\"Merriweather\")\n#set the fonts to automatically appear\nshowtext_auto()\n\n#set the font inside our plot\nmidwest %&gt;%\n  ggplot(aes(x = percbelowpoverty, y = percollege, color = state)) +\n  geom_point() +\n  labs(title = \"Relation between percent below poverty and\\npercent with college education in each state\") +\n  theme(text = element_text(family = \"Merriweather\"))"
  },
  {
    "objectID": "11organizingwork.html",
    "href": "11organizingwork.html",
    "title": "Organizing your work",
    "section": "",
    "text": "Organizing your work:"
  },
  {
    "objectID": "11organizingwork.html#working-directories",
    "href": "11organizingwork.html#working-directories",
    "title": "Organizing your work",
    "section": "Working directories",
    "text": "Working directories"
  },
  {
    "objectID": "11organizingwork.html#environments",
    "href": "11organizingwork.html#environments",
    "title": "Organizing your work",
    "section": "Environments",
    "text": "Environments"
  },
  {
    "objectID": "11organizingwork.html#projects",
    "href": "11organizingwork.html#projects",
    "title": "Organizing your work",
    "section": "Projects",
    "text": "Projects\n-what is a project\n-why is a project useful"
  },
  {
    "objectID": "11organizingwork.html#renv",
    "href": "11organizingwork.html#renv",
    "title": "Organizing your work",
    "section": "Renv",
    "text": "Renv\n-managing package versions\n-groundhog package"
  },
  {
    "objectID": "11organizingwork.html#rprofile",
    "href": "11organizingwork.html#rprofile",
    "title": "Organizing your work",
    "section": ".Rprofile",
    "text": ".Rprofile\n-what these are and why they might be useful"
  },
  {
    "objectID": "12eda.html",
    "href": "12eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Usually once you get the data for your analysis the first thing you want to do is get to know it and briefly browse through it to know e.g. what variables are in the dataset, how many observations you have etc. You might feel the urge to jump right into answering your key research questions. After all, usually data collection is a laborious and tiring process and you are so curious to check what the results are! But it’s best to set aside the main analyses for a moment and first take a closer look at the data. Why? It is a necessary steps because there is a near-infinite number of things that can go wrong when preparing the data from errors in how data was coded to errors in preprocessing or loading the files. Lots of things can also happen that can make analysis or drawing conclusions difficult. These can range from issues with your measures (e.g. reliability) through unexpected behaviour of participants during the study (e.g. non-compliance, reactance, purposufely giving ridiculous answers) all the way to errors in the data (e.g. items not recoded or errors in coding, values out of range etc.)."
  },
  {
    "objectID": "12eda.html#getting-the-basic-information-about-a-dataset",
    "href": "12eda.html#getting-the-basic-information-about-a-dataset",
    "title": "Exploratory Data Analysis",
    "section": "getting the basic information about a dataset",
    "text": "getting the basic information about a dataset\nHow to best get to know a new dataset? You can work with datasets from various sources. If it’s your study that you’re analyzing, then you probably know everything about the variables that should be in it, how many particiapnts took part in the study, how variables are coded etc. However, you might also be working with datasets provided by other people (e.g. by a colleague who ran the study, by a company, or you got the data from an open repository). In these situation it is crucial that you can get to know your data. Many of such datasets will have a codebook available which should describe it in detail. Unfortunately that is not always the case (codebook is like documentation - super important but nobody wants to do it). If a codebook is available, read it (this still does not allow you to skip checking for potential errors/problems with the data). If you don’t have a codebook you need to check everything yourself. Fortunately there is a lot of functions in R that make it a lot easier.\nLets starts with just general first look at the data. We might want to see how many observations we have, how many columns there are, and the names and types of columns. dplyr has a very nice function for it called glimpse(). It’s more concise than str() but gives you more information than e.g. head(). Lets load our dataset and have a firsty look at it!\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#what dataset do I want for this? Some dataset that I tinkered with?\n#' Should I load a dataset I have?\n#' Can I easily do it? Which dataset could I use for that purpose?\n\nThe psych package also has a pretty useful function that can give you multiple summaries very quickly. It’s called describe(). It takes a data frame or a matrix as an argument and can provide a lot of summary statistics for numeric variables such as the mean, median, standard deviation, standard error, skew, kurtosis and quantiles. You can specify which of these you want by setting additional arguments.d\n\nlibrary(psych)\n\n\nDołączanie pakietu: 'psych'\n\n\nNastępujące obiekty zostały zakryte z 'package:ggplot2':\n\n    %+%, alpha\n\n\nSometimes you want summary statistics grouped by some categorical variable (e.g. experimental condition). You can do it with describe.by() from psych package. It works the same way as describe() you just need to provide the group argument which tells R what variable to split the dataset by.\n\nYou might also want to get some summary statistics of categorical variables. The simplest way to do it is with table() which allows you to get counts or cross tabulations of categorical variables."
  },
  {
    "objectID": "12eda.html#exploring-via-plots",
    "href": "12eda.html#exploring-via-plots",
    "title": "Exploratory Data Analysis",
    "section": "Exploring via plots",
    "text": "Exploring via plots\nWhy plotting data is always important\n\nA cautionary tale the boring way\nOne of the reasons why plotting is very important and useful is that all sorts of data can provide you the same point estimate (like a mean or a regression slope). The most common example is the Anscombe quartet. It’s a set of 4 datasets with x and y variable each. Each of these have exactly the same correlation coefficient between x and y so you might be tempted to say they are pretty much the same, right? But if you plot them you’ll see this:\n\n\n\n\n\nEven though the regression line is the same on each plot you can immediately see that it makes sense only in the first one (top left). The top right one shows a quadratic and not a linear relation. The bottom left one clearly has an outlier that drives the regression line to be steeper. And the bottom right one shows pretty much no relation because entire variation in x is driven by a single data point. If we didn’t plot the data we wouldn’t realize just how different these datasets are. You could discover this in some numerical way (e.g. looking at regression diagnostics) but plotting makes it much faster and allows you to immediately spot the problem.\n\n\nA cautionary tale the fun way\nThere is a more fun way to see the same point that was made by Anscombe. We’ll look at an experiment that was conducted to test how displaying information in the media affects attitude towards migrants. The researchers showed participants either a negative article or a neutral one and measured attitudes towards migrants. They also measured general exposure to the media (ie how much media does one consume) because they were interested in the relation between media consumption and attitudes towards migrants. They also recorded gender and age of participants. Lets load the data:\n\n\nRows: 1,700\nColumns: 5\n$ gender         &lt;chr&gt; \"female\", \"male\", \"male\", \"female\", \"female\", \"female\",…\n$ age            &lt;dbl&gt; 35, 35, 38, 18, 44, 50, 58, 53, 45, 35, 20, 58, 34, 35,…\n$ condition      &lt;chr&gt; \"control\", \"control\", \"control\", \"experimental\", \"contr…\n$ media_exposure &lt;dbl&gt; 9.82, 9.54, 5.00, 9.22, 8.54, 6.60, 5.04, 4.54, 5.92, 1…\n$ att_migrants   &lt;dbl&gt; 6.12, -0.62, -2.12, 5.68, 6.00, 0.54, 5.70, 7.04, 6.12,…\n\n\nLets look at some summary statistics first:\n\nexperiment %&gt;%\n  group_by(condition) %&gt;%\n  summarise(mean_att = mean(att_migrants),\n            se_att = sd(att_migrants)/sqrt(n())) %&gt;%\n  knitr::kable()\n\n\n\n\ncondition\nmean_att\nse_att\n\n\n\n\ncontrol\n3.172235\n0.1107740\n\n\nexperimental\n3.149035\n0.1085498\n\n\n\n\n\nWell, it does not look like there are any meaningful differences between the conditions. We can also plot these differences:\n\nexperiment %&gt;%\n  ggplot(aes(x = condition, y = att_migrants)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\") +\n  coord_cartesian(ylim = c(0,5))\n\n\n\n\nThis is a bit disappointing. We could say that we did not reject the null hypothesis and there is no support in the data that reading a negative article affect attitudes towards migrants relative to reading a neutral article. Lets at least look at the correlation between media exposure and attitude towards migrants. Maybe there is something interesting there? We can use the cor() function for it:\n\ncor(experiment$media_exposure, experiment$att_migrants)\n\n[1] -0.2702737\n\n\nWell the correlation seems negative and it definitely is not small. Maybe we are finally on to something! So lets finally plot the data to see how it looks like:\n\nexperiment %&gt;%\n  ggplot(aes(x = media_exposure, y = att_migrants)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhoops! This obviously makes no sense. We get a plot of a gorilla. This dataset is fabricated! This example is taken fully from a paper: Yanai, I, Lercher, M. (2020). A Hypothesis is a liability. Genome Biology, 21, 231. What they did was provide students with a very similar dataset as above and randomly assign their students into one of two condition: half the students were asked to test a specific hypothesis and half to just analyze the dataset. The researchers were interested how many students would find the gorilla. They found (although the sample was small) that students who were asked to test specific hypothesis were less likely to find the gorilla in the data than the students that were not assigned a specific hypothesis to test. This of course dos not mean we should throw hypotheses away. However it shows that being too focused on testing very specific things with our data we can miss some really big errors and unless we explore the data properly we might make terrible mistakes.\nThe key takeaway is that a lot of things that are wrong or at least problematic can be immediately spotted when you plot the data. e.g. in when comparing two conditions of an experiment you might spot that the whole effect is driven but just a few outliers. Or that some values are out of range. There are of course limits to what a plot can provide (if you want to quantify then you need more than a plot) but they often work very well for initial screening.\nAnother important takeaway is that good exploratory data analysis makes detecting both errors and fraud much easier."
  },
  {
    "objectID": "12eda.html#reliabilities",
    "href": "12eda.html#reliabilities",
    "title": "Exploratory Data Analysis",
    "section": "reliabilities",
    "text": "reliabilities\n-getting reliability analysis with alpha()"
  },
  {
    "objectID": "13t-tests.html",
    "href": "13t-tests.html",
    "title": "t tests",
    "section": "",
    "text": "-The whole idea behind the t-tests.\nThe logic behind a t test."
  },
  {
    "objectID": "14anova.html",
    "href": "14anova.html",
    "title": "Anova",
    "section": "",
    "text": "-many groups, many means\n\n\n-how to make a one-way anova.\n\n\n-What these are and how to make them\n\n\n\n-How these are different from post hocs\n-How to make them\n\n\n\n\n\n\n\n-how to interpret the results\n-plotting results\n-effect size and power"
  },
  {
    "objectID": "14anova.html#simple-one-way-anova",
    "href": "14anova.html#simple-one-way-anova",
    "title": "Anova",
    "section": "",
    "text": "-how to make a one-way anova.\n\n\n-What these are and how to make them\n\n\n\n-How these are different from post hocs\n-How to make them"
  },
  {
    "objectID": "14anova.html#looking-at-the-results",
    "href": "14anova.html#looking-at-the-results",
    "title": "Anova",
    "section": "",
    "text": "-how to interpret the results\n-plotting results\n-effect size and power"
  },
  {
    "objectID": "15correlations.html",
    "href": "15correlations.html",
    "title": "correlations",
    "section": "",
    "text": "-what are those"
  },
  {
    "objectID": "16regression.html",
    "href": "16regression.html",
    "title": "Regression",
    "section": "",
    "text": "What type of problems are we talking about?\n-What is a regression analysis?\n\n\nRegression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest.\nHousing market example: imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on.\n\n\n\nThe very idea of making a regression analysis is ultimately an optimization problem. We want to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones.\nMaking a guess:\nSo, we know we need to optimize our prediction - we want to make the best guess possible. But what does it mean ‘the best guess possible’? We need some rule on what it would mean to make a best guess. Since we are predicting a continuous variable we can calculate how much we miss for every prediction (subtract the actual value from the predicted value) and then choose the one that has the smallest error. This is already a start but we can miss in two ways: we can predict too little or too much. The first one is going to produce a negative error and the other one a positive one. Negative numbers are smaller than positive ones so our rule so far will always favor predicting too little! Fortunately there is a very easy way to deal with this - we can square the errors (there are reasons why squaring is preferred to taking absolute values but they are beyond the scope of this course). Now, we can sum all the squared errors for each observation in our dataset. The prediction that gets the smallest sum of squared errors wins! This is pretty much how the Ordinary Least Squares (OLS) regression works.\nLets start with the simplest example possible. Lets say we have no other information except our variable that we want to predict. Which value will minimize the sum of squared errors? There’s actually a fancy formula for this because OLS regression has a closed form but perhaps a better way to learn this is to actually make a bunch of guesses and see what happens. This is what simulation is perfect for.\nLets first simulate a bunch of observations from a normal distribution: our dependent variable. We will keep mean = 0 and standard deviation = 1.\n\ny &lt;- rnorm(1000)\n\nNext, we’ll make a bunch of guesses and see how each of them performs (what is their sum of squared errors)\n\nsse &lt;- c() #initialize vector to store sum of squared errors\npred &lt;- seq(-1,1,length.out = 10) #we'll make 10 guesses from -1 to 1, equally spaced\nfor (i in pred) {\n  res &lt;- sum((i - y)^2) # for each guess calculate sum of squared errors\n  sse &lt;- c(sse, res) #append the sum to the vector\n}\npred_sse &lt;-data.frame(pred,sse) #put these \n\nYou can go ahead and inspect the sse and pred. Can you see for which guess we get the smallest error? You can also look at the animation below:\n\n\n\npredictions and sums of squared errors\n\n\nYou can see that the points fall on a really nice parabola. For a guess of mean - 1 standard deviation we get a really big sum of squared errors, they gradually get smaller and smaller and then start to get bigger up to mean + 1 standard deviation. The lowest point of the parabola is at the mean and that is in fact our best guess. When making regression analysis we will be working with means all the time. We’ll just be adding more information to the model (e.g. if we add belonging to experimental vs control group into the model, then our best guess will be the mean in each of those groups and we get a two sample t-test).\nA more general way which we can use to think about linear models is that we are modelling Y as following a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). The mean is then determined by the variables we put into the model. If we don’t add anything \\(\\mu\\) is going to be the actual mean in our sample. If we start adding variables into the model then \\(\\mu_i\\) will be determined by them (that’s why we have the i).\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma)\n\\\\\n\\mu_i =\\alpha\n\\]\n\n\n\nRunning a regression in R is actually super simple. You can use the lm() function to do it.\n-how to look at the results\n\n\n\n\nWhat are categorical predictors\nHow R codes categorical predictors\nt-tests, one-way anovas and regressions - it’s all the same!"
  },
  {
    "objectID": "16regression.html#prediction-and-inference",
    "href": "16regression.html#prediction-and-inference",
    "title": "Regression",
    "section": "",
    "text": "Regression analysis is often described in two ways. One of them talks about how to predict a value of variable of interest given a set of other variables. The other context focuses on inference: which variables are in fact related to a variable of interest.\nHousing market example: imagine you work in a real estate agency selling houses. You track information on a number of characteristics of each house: their price, size, number of rooms, distance from city center and various facilities etc. You might be interested in predicting the price of a house as accurately as possible given all the characteristics of a house. You might also be interested in how various characteristics of a house relate to its price so as to know what to focus on."
  },
  {
    "objectID": "16regression.html#regression-engine",
    "href": "16regression.html#regression-engine",
    "title": "Regression",
    "section": "",
    "text": "The very idea of making a regression analysis is ultimately an optimization problem. We want to reexpress the relations between variables so as to be able to express one of them as a combination of the other ones.\nMaking a guess:\nSo, we know we need to optimize our prediction - we want to make the best guess possible. But what does it mean ‘the best guess possible’? We need some rule on what it would mean to make a best guess. Since we are predicting a continuous variable we can calculate how much we miss for every prediction (subtract the actual value from the predicted value) and then choose the one that has the smallest error. This is already a start but we can miss in two ways: we can predict too little or too much. The first one is going to produce a negative error and the other one a positive one. Negative numbers are smaller than positive ones so our rule so far will always favor predicting too little! Fortunately there is a very easy way to deal with this - we can square the errors (there are reasons why squaring is preferred to taking absolute values but they are beyond the scope of this course). Now, we can sum all the squared errors for each observation in our dataset. The prediction that gets the smallest sum of squared errors wins! This is pretty much how the Ordinary Least Squares (OLS) regression works.\nLets start with the simplest example possible. Lets say we have no other information except our variable that we want to predict. Which value will minimize the sum of squared errors? There’s actually a fancy formula for this because OLS regression has a closed form but perhaps a better way to learn this is to actually make a bunch of guesses and see what happens. This is what simulation is perfect for.\nLets first simulate a bunch of observations from a normal distribution: our dependent variable. We will keep mean = 0 and standard deviation = 1.\n\ny &lt;- rnorm(1000)\n\nNext, we’ll make a bunch of guesses and see how each of them performs (what is their sum of squared errors)\n\nsse &lt;- c() #initialize vector to store sum of squared errors\npred &lt;- seq(-1,1,length.out = 10) #we'll make 10 guesses from -1 to 1, equally spaced\nfor (i in pred) {\n  res &lt;- sum((i - y)^2) # for each guess calculate sum of squared errors\n  sse &lt;- c(sse, res) #append the sum to the vector\n}\npred_sse &lt;-data.frame(pred,sse) #put these \n\nYou can go ahead and inspect the sse and pred. Can you see for which guess we get the smallest error? You can also look at the animation below:\n\n\n\npredictions and sums of squared errors\n\n\nYou can see that the points fall on a really nice parabola. For a guess of mean - 1 standard deviation we get a really big sum of squared errors, they gradually get smaller and smaller and then start to get bigger up to mean + 1 standard deviation. The lowest point of the parabola is at the mean and that is in fact our best guess. When making regression analysis we will be working with means all the time. We’ll just be adding more information to the model (e.g. if we add belonging to experimental vs control group into the model, then our best guess will be the mean in each of those groups and we get a two sample t-test).\nA more general way which we can use to think about linear models is that we are modelling Y as following a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). The mean is then determined by the variables we put into the model. If we don’t add anything \\(\\mu\\) is going to be the actual mean in our sample. If we start adding variables into the model then \\(\\mu_i\\) will be determined by them (that’s why we have the i).\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma)\n\\\\\n\\mu_i =\\alpha\n\\]"
  },
  {
    "objectID": "16regression.html#making-a-simple-regression",
    "href": "16regression.html#making-a-simple-regression",
    "title": "Regression",
    "section": "",
    "text": "Running a regression in R is actually super simple. You can use the lm() function to do it.\n-how to look at the results"
  },
  {
    "objectID": "16regression.html#multiple-regression",
    "href": "16regression.html#multiple-regression",
    "title": "Regression",
    "section": "",
    "text": "-how to make it\n-Why you should always think first: putting things into regression"
  },
  {
    "objectID": "17linear_models.html",
    "href": "17linear_models.html",
    "title": "Linear Models",
    "section": "",
    "text": "-what is a linear model?\n-How to think about linear models\n-Basics of specifying a linear model\n-T-test, between-anova and regression are the same thing - a linear model\n-Buiding a simple linear model\n-Adding predictors\n-Categorical predictors"
  },
  {
    "objectID": "17linear_models.html#linear-models",
    "href": "17linear_models.html#linear-models",
    "title": "Linear Models",
    "section": "",
    "text": "-what is a linear model?\n-How to think about linear models\n-Basics of specifying a linear model\n-T-test, between-anova and regression are the same thing - a linear model\n-Buiding a simple linear model\n-Adding predictors\n-Categorical predictors"
  },
  {
    "objectID": "18statistical_control.html",
    "href": "18statistical_control.html",
    "title": "Statistical control",
    "section": "",
    "text": "So far we have been working with only 1 independent variable at a time."
  },
  {
    "objectID": "19postprocessing.html",
    "href": "19postprocessing.html",
    "title": "Postprocessing results",
    "section": "",
    "text": "In the final class we will look at how to post process your models to get exactly what you want from them. In older tutorials or textbooks you could usually read to make models directly interpretable. That meant for example such coding of categorical variables so that the regression coefficient were exactly the contrast you wanted.\nWith newer and more versatile software you don’t really need to do it this way. You can run a model and then process the results to get exactly what matches your hypothesis e.g. if you want to test a specific contrast from an experiment. Or if you want to compare whether the effect of one variable is significantly stronger than the effect of another variable (remember that if one effect is statistically significant and another is not it does not mean they are significantly different). This gets especially important once you start working with more complex models (like logistic or ordinal regression) where coefficients themselves aren’t easily interpretable."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a website accompanying the Introduction to R course. The course covers basics of working in R, from the basic operations to fundamental statistics. No knowledge of R is assumed but basic knowledge of statistics is required to follow the course (especially for the latter topics).\nPlease note that this website is not a substitution for the course. It supplements and extends it but cannot replace it.\nThis website is still work in progress so some things might change (exercises added, some classes modified etc.)"
  },
  {
    "objectID": "index.html#what-this-website-is-about",
    "href": "index.html#what-this-website-is-about",
    "title": "Introduction",
    "section": "",
    "text": "This is a website accompanying the Introduction to R course. The course covers basics of working in R, from the basic operations to fundamental statistics. No knowledge of R is assumed but basic knowledge of statistics is required to follow the course (especially for the latter topics).\nPlease note that this website is not a substitution for the course. It supplements and extends it but cannot replace it.\nThis website is still work in progress so some things might change (exercises added, some classes modified etc.)"
  },
  {
    "objectID": "03data_types.html#exercises",
    "href": "03data_types.html#exercises",
    "title": "Types of data",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose you ran a survey among your friends to what extent they agree with a statement “I like pineapples” and got 6 answers: “agree”, “disagree”, “agree”, “somewhat agree”, “somewhat disagree”, “disagree”. Choose the most appropriate type of object to store this information and save it as answers.\n\n\nexercise 1\n#' We need a data structure that can store a categorical variable with various levels.\n#'  These levels also have some order to them. The best structure to store the answers is\n#'   an ordered factor. We can create it by turning a vector into a factor,\n#'    setting ordered argument to TRUE and specifying the levels of values.\n\n\npinapple_factor &lt;- factor(c(\"agree\", \"disagree\", \"agree\", \"somewhat agree\", \"somewhat disagree\", \"disagree\"), ordered = TRUE, levels = c(\"disagree\", \"somewhat disagree\", \"somewhat agree\", \"agree\"))\npinapple_factor\n\n\nCreate v1 vector as 500 random numbers from normal distribution with mean of 3 and standard deviation of 1 and a v2 vector as 500 random numbers from normal distribution with mean of 2.5 and standard deviation of 1.5. Append v2 to v1 to create one vector with 1000 values and save it as v4. Create a v3 vector by repeating c(\"a\", \"b\") 500 times. Put together v4 and v3 as a dataframe. Then calculate which rows: a or b has a higher mean of v4?\n\n\nexercise 2\n#' We need to create 2 vectors, put them into a data frame and then calculate means for\n#' subsetted data frame. We can create v4 by using rnorm() with appropriate arguments twice\n#' and putting the resulting vectors together into v4.\n#' v3 can be created with rep() function to repeat vector c(\"a\", \"b\") 500 times.\n#' Then we need to put v3 and v4 into a dataframe with data.frame and save it as e.g. df.\n#' Finally we need to calculate 2 means on subsetted dataframes. We can use which() inside\n#' df[] to get the rows that match our condition (v1 == \"a\" or v1 == \"b\") and get the mean\n#' of v4 from the subsetted data frames.\n\nv1 &lt;- rnorm(500, 3, 1)\nv2 &lt;- rnorm(500, 2.5, 1.5)\n\nv4 &lt;- c(v1, v2)\n\nv3 &lt;- rep(c(\"a\", \"b\"), 500)\n\ndf &lt;- data.frame(v3, v4)\n\nmean(df[which(df$v3 == \"a\"), \"v4\"])\nmean(df[which(df$v3 == \"b\"), \"v4\"])\n\n\nSuppose you are interested in assessing how effective a given drug is. You have the following data: Out of 900 people who took the drug 657 got better. Out of 1000 people who did not take the drug, 540 got better. Represent this information as a matrix with columns coding those who took a drug or didn’t take it and rows representing those who got better or not. Next calculate the percentage of people who got better in the drug and no drug conditions.\n\n\nexercise 3\n# we have all the information that we just need to put into a matrix.\n# Remember that columns are supposed to code whether someone took the drug or not \n# (so 1st column should sum to 900 and second to 1000) and rows code whether someone got better or not.\n# We can get all the necesary values into the vector: we know how many people took the drug or\n#not and how many got better in each condition. We can substract the appropriate numbers \n# to get how many people took (or didn't take) the drug and did not get better\n# we then need to create a column with 2 rows and carefully set the byrow argument\n# so that it will match our input vector.\n\n# Finally to get the percentages we need to divide values from the first row and appropriate column by the sum of appropriate column.\n\n\n\ntrial_matrix &lt;- matrix(c(657, 900-657, 540, 1000 - 540), nrow = 2, byrow = F)\n\ngot_better_drug_percent &lt;- trial_matrix[1,1]/sum(trial_matrix[,1])\ngot_better_nodrug_percent &lt;- trial_matrix[1,2]/sum(trial_matrix[,2])\n\ngot_better_drug_percent\ngot_better_nodrug_percent"
  },
  {
    "objectID": "18statistical_control.html#what-is-statistical-control",
    "href": "18statistical_control.html#what-is-statistical-control",
    "title": "Statistical control",
    "section": "What is statistical control?",
    "text": "What is statistical control?\n\nWhat does it mean to control for a variable?\nHow do we do it in R?"
  },
  {
    "objectID": "18statistical_control.html#to-control-or-not-to-control",
    "href": "18statistical_control.html#to-control-or-not-to-control",
    "title": "Statistical control",
    "section": "To control or not to control?",
    "text": "To control or not to control?\n\nWhat are confounders\nBasic types of confounds: fork, mediator, collider, descendant. A really good paper on this topic is here.\nWhen should you control?\nInterpreting coefficients from covariates - should you?\nFor example you might be tempted to interpret the effect of gender which was included in a model as a covariate. But ask yourself - is it the total or direct effect?"
  },
  {
    "objectID": "02basic_data.html#exercises",
    "href": "02basic_data.html#exercises",
    "title": "Basic objects",
    "section": "",
    "text": "Test if the expression below is a tautology (true no matter what the truth value of p and q):\n!(p & q) | !(p | !q)\nCreate 5 objects: a, b, c, d, e and assign them values 1, 15, 3, 4.5, 6. Calculate the mean of all these elements and then calculate the sum of squared differences of each value from the mean.\nWhat is the result of comparison TRUE == \"TRUE\"? Why?"
  },
  {
    "objectID": "16regression.html#categorical-predictors",
    "href": "16regression.html#categorical-predictors",
    "title": "Regression",
    "section": "",
    "text": "What are categorical predictors\nHow R codes categorical predictors\nt-tests, one-way anovas and regressions - it’s all the same!"
  },
  {
    "objectID": "19postprocessing.html#general-logic",
    "href": "19postprocessing.html#general-logic",
    "title": "Postprocessing results",
    "section": "General logic",
    "text": "General logic\nThe dataset we will be working with today is\nThe packages we will be working with today is broom, performance and (amazing!) marginaleffects. The last package also has absolutely incredible documentation so I highly recommend checking out its website."
  },
  {
    "objectID": "19postprocessing.html#diagnostics",
    "href": "19postprocessing.html#diagnostics",
    "title": "Postprocessing results",
    "section": "Diagnostics",
    "text": "Diagnostics\nHere we’ll look at some basic diagnostics for regression models.\n\nLooking at outliers\nLooking at multicollinearity"
  },
  {
    "objectID": "19postprocessing.html#making-predictions",
    "href": "19postprocessing.html#making-predictions",
    "title": "Postprocessing results",
    "section": "Making predictions",
    "text": "Making predictions\nSometimes you might want to assess the predictions of your model - how well does it predict?\n\nsplitting the sample\ncommon metrics like rmse, r2"
  },
  {
    "objectID": "19postprocessing.html#marginal-effects",
    "href": "19postprocessing.html#marginal-effects",
    "title": "Postprocessing results",
    "section": "Marginal effects",
    "text": "Marginal effects\nAn extremely good and clear introduction to marginal effects with explanation of differences between 2 main packages used for them in R can be found on Andrew Heiss’ blog. If you want to dive deeper into this topic I highly recommend you read that blog post.\n\nWhat is a marginal effect\ncalculating marginal effect\nconditional effects"
  },
  {
    "objectID": "19postprocessing.html#contrasts",
    "href": "19postprocessing.html#contrasts",
    "title": "Postprocessing results",
    "section": "Contrasts",
    "text": "Contrasts\nIn many situations when you work with categorical variables you are interested in"
  }
]