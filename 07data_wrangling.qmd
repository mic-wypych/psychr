---
title: "data wrangling"
author: "MichaÅ‚ Wypych"
---

# Data wrangling

Now that we have our dataset loaded we can finally get to work with it! We'll start with the basics of data wrangling: subsetting datasets, sorting variables, changing variables and getting basic summaries. These are the standard things that you might want to do before any statistical modelling.

## Tidyverse

For data wrangling we'll be working within `tidyverse` throughout this course. `Tidyverse` is a set of packages designed for working with data in a clean, readable way. A huge advantage (apart from readability) is that all packages in `tidyverse` are designed to be compatible with each other and share common "grammar" and way of doing things. This way it is easy to combine them to do a lot of different things with your data. We've already met one package from this collection: `readr`. Other packages include:

-   `dplyr`: package for data wrangling. We'll focus on it in this class

-   `tidyr`: package for tidying data and reshaping it. We'll look at it in the next class

-   `ggplot2`: the go to package for data visualization in R. Absolutely the best of the best when it comes to plotting.

-   `forcats`: package for working with factors

-   `strings`: package for working with text data

-   `purrr`: functional programming stuff in R like easier iteration within tidyverse

-   `tibble`: package that introduces slightly altered data frames

`Tidyverse` is not the only way in R for data wrangling (other package often used is `data.table`, a new alternative is `polars`). If you don't want additional packages you don't even need them, you can do almost everything in base R if you want to. So why choose `tidyverse`? First of all it's extremely intuitive. Writing and reading code in `tidyverse` feels almost like writing plain text of what you want to do with your data. Thanks to pipes it also made code much more readable (more on that in a moment although there are now pipe alternatives). One downside of `tidyverse` is that it is significantly slower than other packages. If speed is paramount you might want to consider switching to `data.table` or `polars`.

## The pipe

So far if we wanted to use multiple functions in a single call we had to wrap one function inside another e.g. if we wanted to take a list of vectors, calculate the mean of each vector and then find out the highest mean we could do something like this:

```{r}
max(sapply(list(c(1,2,3), c(4,5,6), c(6,7,8)), mean))

```

It's not the easiest code to read, right? When combining functions this way you need to read them inside out. This is not how people read. It would e much easier if we could read code more linearly e.g. from left to right and top to bottom. Enter the pipe! The pipe operator allows you to chain together functions in a readable way. The basic idea (there's more to pipes though) is to take what is on the left hand side of the pipe and pass it as the first argument of whatever is on the right hand side of the pipe. This changes the inside-out into left-to-right code. There are 2 pipes in R. The first one comes from the `magrittr` package and this is the one used in `tidyvese`. This pipe looks like this: `%>%`. If we wanted to rewrite the code above using this pipe it would look like this:

```{r}
library(magrittr)

list(c(1,2,3), c(4,5,6), c(6,7,8)) %>%
  sapply(mean) %>%
  max()
  
```

It's much easier to understand what this code does right? An alternative introduced in R 4.1 is the native pipe: `|>`. The basic functionality is pretty much the same as in the `magrittr` pipe but you don't need to load any packages to use it (you might need to enable native pipe in the global options in Tools bar in RStudio). The same code as above but with native pipe looks like this:

```{r}
list(c(1,2,3), c(4,5,6), c(6,7,8)) |>
  sapply(mean) |>
  max()
```

You might wonder why have two kinds of pipes one of which needs loading a new package? The first reason is very simple: `magrittr` pipe is older. There are however a few differences. You can read about the details [here](https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/). Remember that pipe automatically passes what is on the left as the first argument to whatever is on the right of the pipe? What if you need to pass it not as the first but second or third argument? Both pipe operators have a placeholder argument that can be used in such situations. `%>%` has the `.` operator and `|>` has `_`. The difference between them is that `_` can only be used once and has to be used with named arguments. Here's an example of how placeholder argument can work: `append()` allows you to join two vectors together. The vector passed as the second argument is appended to the one passed as the first argument:

```{r}
x <- c(1,2,3)
y <- c(4,5,6)
x %>%
  append(y, .)
  
```

Generally, the differences boil down to simplicity: native pipe was deliberately created to be a simpler operator with less functionality. Most of the time you won't notice much difference (maybe except for how the placeholder argument works).

## The basic dplyr function

Now we can get to the basics of data wrangling in `dplyr` package. We'll look at the storms dataset in the `dplyr` package. It stores information on date, place, status and some other things about storms from 1975 to 2021. The dataset stores multiple observations from each storm because measurements were made every few hours. Before we move one to working with data lets introduce one function: `glimpse()`. It's a bit like `str()` but is a bit more readable for dataframes. This function can give you a concise look at what variables you have in your dataset. Lets load `tidyverse`, our dataset and look at it:

```{r}
library(tidyverse)
data("storms")

glimpse(storms)
```

The dataset has 19066 rows and 13 columns. We can also see that we have various types of variables: many numeric, one factor and one character.

### Subsetting

You can subset a dataframe either by columns or by rows. If you want to extract a subset of rows based on some logical conditions you can use `filter()`. Lets say we want only storms from 2020:

```{r}
storms %>%
  filter(year == 2020)
```

We can also filter based on multiple conditions. This works exactly like all the logical operations we've seen previously. One difference is that you can use `,` instead of `&`. Lets say we want to get all storms from June 2020:

```{r}
storms %>%
  filter(year == 2020, month == 6)
```

There were 57 storms recorded in June 2020.

If you want to select only certain columns from a dataset you can use `select()`. E.g. if we want only the name, latitude and longitude of the storm we can do it like this:

```{r}
storms %>%
  select(name, lat, long)
```

If you want to select a range of columns you can use `:`

```{r}
storms %>%
  select(year:hour)
```

A particular situation in which you might want to subset a dataset is to get the rows with highest/lowest values of a variable, get the first/last rows or draw a random sample from the dataset. All of these can be achieved by different versions of `slice()`. `slice_sample()` will draw a random sample from the dataset (either by number or proportion). You can also specify if you want to draw with replacements:

```{r}
storms %>%
  slice_sample(n = 100)
```

`slice_min()` and `slice_max()` allow you to get rows with highest values on a variable.

```{r}
storms %>%
  slice_max(wind, n = 10)
```

Finally `slice_head()` and `slice_tail()` allow you to get n first or last rows from a dataset.

```{r}
storms %>%
  slice_head(n = 5)
```

One last thing about filtering. Sometimes you want to filter all unique values of a variable. In order to do it you can use `distinct()`. It will extract all unique values of a variable. By default it will return only the column with distinct values and drop all the other columns from the dataframe. If you want to keep all the other variables (though remember that it will probably keep only the first entry for each unique value!) you can set the `.keep_all` argument to `TRUE`.

```{r}
storms %>%
  distinct(category)
```

### Sorting

Sorting datasets based on variables is super simple. You can use the `arrange()` function and if you need to sort in descending order use `desc()` inside it. Lets say we want to find the storm with the strongest wind:

```{r}
storms %>%
  arrange(desc(wind))
```

Looks like Allen from 1980 was the strongest storm.

### Counting

Counting values in variables is very simple, just use `count()`. One thing to remember is that `count()` will returned a different dataframe. Unless you specify anything additional it will return a dataframe with 2 columns: one will contain all unique values of the variable you counted and the other one, named `n` will contain their counts (you can specify the `name` variable to change that to something else). Setting `sort` argument to `TRUE` will sort the counts in descending order. E.g. if we want to find out which year had the most measurements of storms (and not the number of storms! Remember that each row is 1 measurement of 1 storm) we can do it with:

```{r}
storms %>%
  count(year, sort = TRUE)
  
```

### Changing variables

A common task when wrangling data is creating new variables in an already existing dataset. You can do it by using `mutate()`. Lets say we want to create a new variable that stores information on whether the storm was during summer (June, July, August) or not:

```{r}
storms %>%
  mutate(summer = ifelse(month == 6 | month == 7 | month == 8, TRUE, FALSE))
```

Remember that if you want to keep the variable you need to assign the new dataset to an object.

### Summaries

Another extremely common task is to get summaries about our dataset. We can do it with `summarise()` function. e.g. what if we want to see the mean and standard deviation of strength of wind (for now across all measurements):

```{r}
storms %>%
  summarise(mean_wind = mean(wind, na.rm = T),
            sd_wind = sd(wind, na.rm = T))
```

Notice that the shape of the dataset has changed now. The columns are now the summaries and not the original variables.

### Grouping rows

So far we have been calculating things on entire datasets. In many situations you want to calculate something separately for each level of a categorical variable (much like `tapply()` earlier). To group a dataset we can use `group_by()`. You can also group by multiple variables at once by separating them by a coma. R will group by the first variable and then by the second etc. This is especially useful for creating summaries. E.g. if we want to get average wind speed for each storm we can easily do it:

```{r}
storms %>%
  group_by(name) %>%
  summarise(mean_wind = mean(wind, na.rm = T))
```

Similarly if we want to calculate average wind speed from all measurements for each year and month:

```{r}
storms %>%
  group_by(year, month) %>%
  summarise(mean_wind = mean(wind, na.rm = T))
```

Actually the `summarise()` function has its own argument for calculating grouped summaries. You can specify `.by` argument inside the `summarise()` function. If you use `group_by()` then `summarise()` will by default drop the last level of grouping.

One more important thing about the `group_by()` function is that it works by adding an attribute to the dataframe. This means that after a `group_by()` all subsequent operations will be conducted on the grouped dataframe. E.g. if you sort after grouping then sorting will be conducted within each group separately. You can drop the grouping with `ungroup()`. There is one last special kind of grouping you might want to use - sometimes you want to perform some operation separately on each row (e.g. calculate the average of a multi item scale for each participant). You can do it with `rowwise()`.

### calculating across columns

There are situations when you want to perform the same operation on multiple columns (e.g. calculate the mean and standard deviation of multiple variables). You can do it by hand but this can be tedious. To simplify it you can use `across()` inside `mutate()` or `summarise()`. the syntax of `across()` is as follows: the first argument, `.cols` specifies which columns to perform operations on. The second argument `.fns` specifies which functions to apply. You have to provide it a list of functions (preferably named list) or a formula. Finally the `.names` argument specifies how to automatically assign new variable names. E.g. "{.col}\_{.fn}" will create variables with column name, underscore and function name (that's why named list is useful here). If we want to get the mean and standard deviation of wind speed and pressure in each category of storms we could do it in a few lines of code (by the way, notice how `group_by()` by default includes `NA` as a separate category):

```{r}
storms %>%
  group_by(category) %>%
  summarise(across(.cols = c("wind", "pressure"), .fns = list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))
```

### Helper verbs

One more useful thing for data wrangling is a set of functions making it easier to select multiple columns based on some condition. There are a few helper functions you can use to do that. They do basically what their names suggest. These verbs are as follows: `starts_with()`, `ends_with()` and `contains()`. This way you don't have to manually type all the names if they have something in common (e.g. they are items from the same scale so they are named \`\`scale_1, scale_2 etc.). E.g. lets say we want to get all the columns that end with "diameter":

```{r}
storms %>%
  select(ends_with("diameter"))
```

Nice! These verbs also work nicely inside `across()`. One thing to be aware of: if no column matches what you ask for you won't get an error but a dataframe with all the rows but 0 columns:

```{r}
storms %>%
  select(contains("some_weird_name"))
```

## Combining functions together

Where `tidyverse` really shines is in combining multiple functions together with pipes. Through different orders of the functions described above we can get a ton of things out of our dataset. This already gives us the ability to anwser a number of questions that might be very interesting for analysis.

-   Example 1: Lets say we want to find out the name of storm from each category that had the highest average pressure in 1989.

    ```{r}
    storms %>%
      filter(year == 1989) %>%
      group_by(category, name) %>%
      summarise(mean_pressure = mean(pressure, na.rm = T)) %>%
      slice_max(mean_pressure, n = 1)
    ```

-   Example 2: Lets say we want to find the average wind speed at each hour of the day but we want that in kilometers per hours rather than knots (as is in the database). 1 knot is around 1.852 km/h.

    ```{r}
    storms %>%
      mutate(wind_km = wind*1.852) %>%
      group_by(hour) %>%
      summarise(mean_wind_km = mean(wind, na.rm = T))
    ```

## Exercises

Throughout the exercises you'll still work with the `storms` dataset

-   Find out which year had the most storms (remember that each row is 1 measurement of 1 storm. You have to find the year with the most storms and not the most measurements!)

-   Find the average wind speed for each storm in 2018 and then sort from the highest to the lowest.

-   Calculate the mean and standard deviation of measurement pressure for each month of the year
