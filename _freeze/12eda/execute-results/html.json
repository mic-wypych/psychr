{
  "hash": "dd7ab0c7568f7f457de58650f67b689f",
  "result": {
    "markdown": "---\ntitle: \"Exploratory Data Analysis\"\nformat:\n  html:\n    fig-align: center\n---\n\n\nUsually once you get the data for your analysis the first thing you want to do is get to know it and briefly browse through it to know e.g. what variables are in the dataset, how many observations you have etc. You might feel the urge to jump right into answering your key research questions. After all, usually data collection is a laborious and tiring process and you are so curious to check what the results are! But it's best to set aside the main analyses for a moment and first take a closer look at the data. Why? It is a necessary steps because there is a near-infinite number of things that can go wrong when preparing the data from errors in how data was coded to errors in preprocessing or loading the files. Lots of things can also happen that can make analysis or drawing conclusions difficult. These can range from issues with your measures (e.g. reliability) through unexpected behaviour of participants during the study (e.g. non-compliance, reactance, purposufely giving ridiculous answers) all the way to errors in the data (e.g. items not recoded or errors in coding, values out of range etc.). Generally exploring the data is not so much about writing/using proper functions but about thinking what could possibly be off with the data to prepare yourself before further analyses.\n\n## getting the basic information about a dataset\n\nHow to best get to know a new dataset? You can work with datasets from various sources. If it's your study that you're analyzing, then you probably know everything about the variables that should be in it, how many particiapnts took part in the study, how variables are coded etc. However, you might also be working with datasets provided by other people (e.g. by a colleague who ran the study, by a company, or you got the data from an open repository). In these situation it is crucial that you can get to know your data. Many of such datasets will have a codebook available which should describe it in detail. Unfortunately that is not always the case (codebook is like documentation - super important but nobody wants to do it). If a codebook is available, read it (this still does not allow you to skip checking for potential errors/problems with the data). If you don't have a codebook you need to check everything yourself. Fortunately there is a lot of functions in R that make it a lot easier.\n\nLets starts with just general first look at the data. We might want to see how many observations we have, how many columns there are, and the names and types of columns. `dplyr` has a very nice function for it called `glimpse()`. It's more concise than `str()` but gives you more information than e.g. `head()`. Lets load our dataset and have a firsty look at it! We will work with a dataset on wine reviews (the original dataset and its documentation can be found [here](https://www.kaggle.com/datasets/zynicide/wine-reviews)). The dataset has information on country and region of the vineyard, points awarded by reviewers and the price of the wine, as well as variety of the wine.\n\nLets load the dataset, set some base theme for the plots and look at the variables in the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\ntheme_set(theme_minimal(base_size = 15))\n\nwine <- read_csv(\"data/wine.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 150930 Columns: 11\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): country, description, designation, province, region_1, region_2, va... dbl\n(3): ...1, points, price\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n:::\n\n```{.r .cell-code}\nglimpse(wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 150,930\nColumns: 11\n$ ...1        <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ country     <chr> \"US\", \"Spain\", \"US\", \"US\", \"France\", \"Spain\", \"Spain\", \"Sp…\n$ description <chr> \"This tremendous 100% varietal wine hails from Oakville an…\n$ designation <chr> \"Martha's Vineyard\", \"Carodorum Selección Especial Reserva…\n$ points      <dbl> 96, 96, 96, 96, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95…\n$ price       <dbl> 235, 110, 90, 65, 66, 73, 65, 110, 65, 60, 80, 48, 48, 90,…\n$ province    <chr> \"California\", \"Northern Spain\", \"California\", \"Oregon\", \"P…\n$ region_1    <chr> \"Napa Valley\", \"Toro\", \"Knights Valley\", \"Willamette Valle…\n$ region_2    <chr> \"Napa\", NA, \"Sonoma\", \"Willamette Valley\", NA, NA, NA, NA,…\n$ variety     <chr> \"Cabernet Sauvignon\", \"Tinta de Toro\", \"Sauvignon Blanc\", …\n$ winery      <chr> \"Heitz\", \"Bodega Carmen Rodríguez\", \"Macauley\", \"Ponzi\", \"…\n```\n:::\n:::\n\n\nThere's plenty of variables but we will focus on 3: country (where the vineyard is located), points (score awarded by reviewer) and price (price of the wine). There is a number of things we might want to look at to get to know the data better. Lets start with the numeric columns. A good point to start is to look at some summary statistics like means, standard deviations, minimum and maximum values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- wine %>%\n  select(-`...1`)\nwine %>%\n  summarize(across(is.numeric, .fns = list(mean = ~mean(.x, na.rm = T), \n                                           sd = ~sd(.x, na.rm = T),\n                                           min = ~min(.x, na.rm = T), \n                                           max = ~max(.x, na.rm = T)))) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n| points_mean| points_sd| points_min| points_max| price_mean| price_sd| price_min| price_max|\n|-----------:|---------:|----------:|----------:|----------:|--------:|---------:|---------:|\n|    87.88842|  3.222392|         80|        100|   33.13148| 36.32254|         4|      2300|\n:::\n:::\n\n\nIt seems that no values are out out range as the documentation says that `points` should be between 1 and 100 and the price seems reasonable as well - from 4 to 2300. Perhaps surprisingly the lowest rating is 80 - this is fairly high for a 1 to 100 scale!\n\nWe can also look at other variables that aren't numeric - lets look at the country variable as an example. Lets see which countries are available and for which countries we have the most reviews. We can do it by combining `count()` with `arrange()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  count(country) %>%\n  arrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 49 × 2\n   country         n\n   <chr>       <int>\n 1 US          62397\n 2 Italy       23478\n 3 France      21098\n 4 Spain        8268\n 5 Chile        5816\n 6 Argentina    5631\n 7 Portugal     5322\n 8 Australia    4957\n 9 New Zealand  3320\n10 Austria      3057\n# ℹ 39 more rows\n```\n:::\n:::\n\n\nIf we just want to get information on which countries are in the data we can use `unique()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(wine$country)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"US\"                     \"Spain\"                  \"France\"                \n [4] \"Italy\"                  \"New Zealand\"            \"Bulgaria\"              \n [7] \"Argentina\"              \"Australia\"              \"Portugal\"              \n[10] \"Israel\"                 \"South Africa\"           \"Greece\"                \n[13] \"Chile\"                  \"Morocco\"                \"Romania\"               \n[16] \"Germany\"                \"Canada\"                 \"Moldova\"               \n[19] \"Hungary\"                \"Austria\"                \"Croatia\"               \n[22] \"Slovenia\"               NA                       \"India\"                 \n[25] \"Turkey\"                 \"Macedonia\"              \"Lebanon\"               \n[28] \"Serbia\"                 \"Uruguay\"                \"Switzerland\"           \n[31] \"Albania\"                \"Bosnia and Herzegovina\" \"Brazil\"                \n[34] \"Cyprus\"                 \"Lithuania\"              \"Japan\"                 \n[37] \"China\"                  \"South Korea\"            \"Ukraine\"               \n[40] \"England\"                \"Mexico\"                 \"Georgia\"               \n[43] \"Montenegro\"             \"Luxembourg\"             \"Slovakia\"              \n[46] \"Czech Republic\"         \"Egypt\"                  \"Tunisia\"               \n[49] \"US-France\"             \n```\n:::\n:::\n\n\nThe `psych` package also has a pretty useful function that can give you multiple summaries very quickly. It's called `describe()`. It takes a data frame or a matrix as an argument and can provide a lot of summary statistics for numeric variables such as the mean, median, standard deviation, standard error, skew, kurtosis and quantiles. You can specify which of these you want by setting additional arguments.d\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nDołączanie pakietu: 'psych'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNastępujące obiekty zostały zakryte z 'package:ggplot2':\n\n    %+%, alpha\n```\n:::\n\n```{.r .cell-code}\nwine %>%\n  select(where(is.numeric)) %>%\n  describe() %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|       | vars|      n|     mean|        sd| median|  trimmed|     mad| min|  max| range|       skew|    kurtosis|        se|\n|:------|----:|------:|--------:|---------:|------:|--------:|-------:|---:|----:|-----:|----------:|-----------:|---------:|\n|points |    1| 150930| 87.88842|  3.222392|     88| 87.84567|  2.9652|  80|  100|    20|  0.1428298|  -0.2864823| 0.0082945|\n|price  |    2| 137235| 33.13148| 36.322536|     24| 27.33739| 14.8260|   4| 2300|  2296| 11.8198417| 372.8051184| 0.0980491|\n:::\n:::\n\n\nSometimes you want summary statistics grouped by some categorical variable (e.g. experimental condition). You can do it with `describe.by()` from `psych` package. It works the same way as `describe()` you just need to provide the `group` argument which tells R what variable to split the dataset by. Lets look at a subset of countries (since there are so many!): USA, Italy and France:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescribeBy(price + points ~ country, data = wine[which(wine$country %in% c(\"US\", \"Italy\", \"France\")),], mat=TRUE)%>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|        |item |group1 | vars|     n|     mean|        sd| median|  trimmed|     mad| min|  max| range|       skew|    kurtosis|        se|\n|:-------|:----|:------|----:|-----:|--------:|---------:|------:|--------:|-------:|---:|----:|-----:|----------:|-----------:|---------:|\n|price1  |1    |France |    1| 14785| 45.61988| 69.697060|     25| 32.55541| 16.3086|   5| 2300|  2295|  9.3637885| 174.0621606| 0.5731968|\n|price2  |2    |Italy  |    1| 18784| 37.54791| 37.067869|     25| 31.23556| 16.3086|   5|  900|   895|  5.7314418|  64.9905166| 0.2704603|\n|price3  |3    |US     |    1| 62139| 33.65381| 24.891343|     28| 30.06270| 14.8260|   4| 2013|  2009| 11.3983381| 674.5951508| 0.0998542|\n|points1 |4    |France |    2| 21098| 88.92587|  3.199695|     89| 88.88436|  2.9652|  80|  100|    20|  0.1288524|  -0.3625780| 0.0220287|\n|points2 |5    |Italy  |    2| 23478| 88.41366|  2.728914|     88| 88.31820|  2.9652|  80|  100|    20|  0.3800496|   0.1094096| 0.0178098|\n|points3 |6    |US     |    2| 62397| 87.81879|  3.410294|     88| 87.77596|  2.9652|  80|  100|    20|  0.1404701|  -0.5299118| 0.0136524|\n:::\n:::\n\n\n## Exploring via plots\n\nDoing numeric exploration is always useful and can give you plenty of information about a dataset but in many situations a plot makes exploration much easier. It allows you to immediately spot certain problems like implausible outliers, weird (e.g. censored) distributions or types of relations (is it linear? quadratic? Does it make sense at all?). Lets look at some quick summaries of our variables with plots. We can start by simple histograms of numeric variables and a scatterplot to show the relation between\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  select(price, points) %>%\n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~var, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13695 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can see that `points` is a nicely normal distribution while the `price` variable is heavily skewed (which is to be expected). Lets look at the scatterplot of the two:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  ggplot(aes(x = points, y = price)) +\n  geom_point(alpha = .4) +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13695 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13695 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAnother good idea might be looktng at price and scores e.g. in different countries. Lets focus again on a subset of countries: US, Italy and France.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  filter(country %in% c(\"US\", \"Italy\", \"France\")) %>%\n  ggplot(aes(x = country, y = price)) +\n  geom_violin() +\n  geom_boxplot(width = .2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 11265 rows containing non-finite values (`stat_ydensity()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 11265 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  filter(country %in% c(\"US\", \"Italy\", \"France\")) %>%\n  ggplot(aes(x = country, y = points)) +\n  geom_violin() +\n  geom_boxplot(width = .2)\n```\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAnd the relation between points and price in each of the three countries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>%\n  filter(country %in% c(\"US\", \"Italy\", \"France\")) %>%\n  ggplot(aes(x = points, y = price)) +\n  geom_point(alpha = .4) +\n  facet_wrap(~country)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 11265 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nLooks like the rise in price for the highest scored wines is driven especially by French wines.\n\n### A cautionary tale the boring way\n\nOne of the reasons why plotting is very important and useful is that all sorts of data can provide you the same point estimate (like a mean or a regression slope). The most common example is the Anscombe quartet. It's a set of 4 datasets with x and y variable each. Each of these have exactly the same correlation coefficient between x and y so you might be tempted to say they are pretty much the same, right? But if you plot them you'll see this:\n\n![](images/paste-E216A64B.png){fig-align=\"center\"}\n\nEven though the regression line is the same on each plot you can immediately see that it makes sense only in the first one (top left). The top right one shows a quadratic and not a linear relation. The bottom left one clearly has an outlier that drives the regression line to be steeper. And the bottom right one shows pretty much no relation because entire variation in x is driven by a single data point. If we didn't plot the data we wouldn't realize just how different these datasets are. You could discover this in some numerical way (e.g. looking at regression diagnostics) but plotting makes it much faster and allows you to immediately spot the problem.\n\n### A cautionary tale the fun way\n\nThere is a more fun way to see the same point that was made by Anscombe. We'll look at an experiment that was conducted to test how displaying information in the media affects attitude towards migrants. The researchers showed participants either a negative article or a neutral one and measured attitudes towards migrants. They also measured general exposure to the media (ie how much media does one consume) because they were interested in the relation between media consumption and attitudes towards migrants. They also recorded gender and age of participants. Lets load the data:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,700\nColumns: 5\n$ gender         <chr> \"female\", \"male\", \"male\", \"female\", \"female\", \"female\",…\n$ age            <dbl> 35, 35, 38, 18, 44, 50, 58, 53, 45, 35, 20, 58, 34, 35,…\n$ condition      <chr> \"control\", \"control\", \"control\", \"experimental\", \"contr…\n$ media_exposure <dbl> 9.82, 9.54, 5.00, 9.22, 8.54, 6.60, 5.04, 4.54, 5.92, 1…\n$ att_migrants   <dbl> 6.12, -0.62, -2.12, 5.68, 6.00, 0.54, 5.70, 7.04, 6.12,…\n```\n:::\n:::\n\n\nLets look at some summary statistics first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment %>%\n  group_by(condition) %>%\n  summarise(mean_att = mean(att_migrants),\n            se_att = sd(att_migrants)/sqrt(n())) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|condition    | mean_att|    se_att|\n|:------------|--------:|---------:|\n|control      | 3.172235| 0.1107740|\n|experimental | 3.149035| 0.1085498|\n:::\n:::\n\n\nWell, it does not look like there are any meaningful differences between the conditions. We can also plot these differences:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment %>%\n  ggplot(aes(x = condition, y = att_migrants)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\") +\n  coord_cartesian(ylim = c(0,5))\n```\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThis is a bit disappointing. We could say that we did not reject the null hypothesis and there is no support in the data that reading a negative article affect attitudes towards migrants relative to reading a neutral article. Lets at least look at the correlation between media exposure and attitude towards migrants. Maybe there is something interesting there? We can use the `cor()` function for it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(experiment$media_exposure, experiment$att_migrants)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.2702737\n```\n:::\n:::\n\n\nWell the correlation seems negative and it definitely is not small. Maybe we are finally on to something! So lets finally plot the data to see how it looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment %>%\n  ggplot(aes(x = media_exposure, y = att_migrants)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](12eda_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nWhoops! This obviously makes no sense. We get a plot of a gorilla. This dataset is fabricated! This example is taken fully from a paper: Yanai, I, Lercher, M. (2020). A Hypothesis is a liability. *Genome Biology*, 21, 231. What they did was provide students with a very similar dataset as above and randomly assign their students into one of two condition: half the students were asked to test a specific hypothesis and half to just analyze the dataset. The researchers were interested how many students would find the gorilla. They found (although the sample was small) that students who were asked to test specific hypothesis were less likely to find the gorilla in the data than the students that were not assigned a specific hypothesis to test. This of course dos not mean we should throw hypotheses away. However it shows that being too focused on testing very specific things with our data we can miss some really big errors and unless we explore the data properly we might make terrible mistakes.\n\nThe key takeaway is that a lot of things that are wrong or at least problematic can be immediately spotted when you plot the data. e.g. in when comparing two conditions of an experiment you might spot that the whole effect is driven but just a few outliers. Or that some values are out of range. There are of course limits to what a plot can provide (if you want to quantify then you need more than a plot) but they often work very well for initial screening.\n\n## Exercises\n\n1.  Look at the dataset and its codebook. Try to validate it and make sure all variables are coded correctly\n2.  Explore the data with plots - does the data look ok?\n",
    "supporting": [
      "12eda_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}